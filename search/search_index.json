{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to use Torch-RecHub","text":"Torch-RecHub <p>An easy-to-use, scalable, and high-performance recommendation system framework based on PyTorch</p>            Quick Start                     View GitHub"},{"location":"contributing/","title":"Contribution Guide","text":"<p>Thank you very much for your interest in the Torch-RecHub project and for considering contributing! Your help is crucial for the development of the project. This guide will detail how to participate in contributions.</p>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":"<p>You can contribute in the following ways:</p> <ol> <li>Report Bugs: If you find any errors or unexpected behavior while using Torch-RecHub, please submit an Issue detailing the problem, steps to reproduce it, and your environment information.</li> <li>Suggest Enhancements: If you have ideas for improving existing features or adding new ones, please create an Issue first to discuss.</li> <li>Submit Code Changes: Fix bugs, implement new features, or improve code quality.</li> <li>Improve Documentation: Enhance or correct documentation, write tutorials, or provide example use cases.</li> </ol>"},{"location":"contributing/#finding-contribution-points","title":"Finding Contribution Points","text":"<p>You can start from the following aspects:</p> <ol> <li>Check Issues: Browse the project Issues list, look for issues marked with <code>help wanted</code>.</li> <li>Improve Existing Features: If you find areas for optimization during use, feel free to propose suggestions or submit improvements directly.</li> <li>Implement New Features: If you have new ideas, it is recommended to create an Issue for discussion first to ensure it aligns with the project direction.</li> </ol>"},{"location":"contributing/#contribution-process-code-and-documentation","title":"Contribution Process (Code and Documentation)","text":"<p>We use the standard GitHub Fork &amp; Pull Request workflow to accept code and documentation contributions. (You can also perform the following operations on the GitHub website).</p> <ol> <li> <p>Fork the Repository     Visit the Torch-RecHub GitHub repository page, click the \"Fork\" button in the top right corner to copy the project to your own GitHub account.</p> </li> <li> <p>Clone Your Fork     Clone your forked repository locally:     <pre><code>git clone https://github.com/YOUR_USERNAME/torch-rechub.git\ncd torch-rechub\n</code></pre>     Please replace <code>YOUR_USERNAME</code> with your GitHub username.</p> </li> <li> <p>Set Upstream Remote (Optional but Recommended)     Add the original project repository as an upstream remote for easy synchronization of updates:     <pre><code>git remote add upstream https://github.com/datawhalechina/torch-rechub.git\n# Sync updates from upstream main branch if needed\n# git fetch upstream\n# git checkout main\n# git merge upstream/main\n</code></pre></p> </li> <li> <p>Make Changes     Write code, modify documentation, or make other improvements directly on the <code>main</code> branch.</p> </li> <li> <p>Ensure Code Quality</p> <ul> <li>Code Style: Please adhere to the project's existing code style.</li> <li>Documentation: For user-visible changes (like new features, API changes), please update relevant documentation (README, files under <code>docs/</code>, etc.).</li> </ul> </li> <li> <p>Commit Changes     Commit your changes with clear and meaningful commit messages. We recommend following the Conventional Commits specification.     <pre><code>git add .\ngit commit -m \"feat: Add support for XXX model\"\n# Or\ngit commit -m \"fix: Correct typo in README\"\n# Or\ngit commit -m \"docs: Update contribution guide\"\n</code></pre></p> </li> <li> <p>Push Branch     Push your local <code>main</code> branch to your forked GitHub repository:     <pre><code>git push origin main\n</code></pre></p> </li> <li> <p>Create a Pull Request (PR)     Return to your forked repository page on GitHub. You should see a prompt suggesting you create a Pull Request based on the recent pushes to <code>main</code>. Click that prompt or manually navigate to the \"Pull requests\" tab and click \"New pull request\".</p> <ul> <li>Choose Branches: Ensure the Base repository is <code>datawhalechina/torch-rechub</code>'s <code>main</code> branch, and the Head repository is your fork's <code>main</code> branch.</li> <li>Fill in PR Information:<ul> <li>Title: Concisely describe the purpose of the PR, often based on the commit message.</li> <li>Description: Detail the changes you've made, the problem solved (you can link related Issues, e.g., <code>Closes #123</code>), and any points reviewers should note. If the PR includes UI changes, please attach screenshots or screen recordings.</li> </ul> </li> <li>Allow Maintainer Edits (Optional): Checking \"Allow edits by maintainers\" often helps maintainers quickly fix minor issues.</li> <li>Submit PR: Click \"Create pull request\".</li> </ul> </li> </ol>"},{"location":"contributing/#pull-request-review","title":"Pull Request Review","text":"<ul> <li>After submitting the PR, the project's CI/CD workflow will automatically run tests and checks.</li> <li>Project maintainers will review your code and documentation and may suggest modifications.</li> <li>Please respond promptly to review comments and make necessary changes. Maintainers might make minor edits directly on your branch (if you allow it), or you might need to update the code yourself and push again.</li> <li>Once the PR is approved and passes all checks, maintainers will merge it into the main branch.</li> </ul> <p>Thank you again for your contribution to Torch-RecHub!</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>Common questions and troubleshooting guide for Torch-RecHub.</p> <ul> <li> <p>Will there be a TensorFlow version?</p> <ul> <li> <p>Not currently planned</p> </li> <li> <p>This project's core positioning is to provide easy-to-use model implementations for beginners, referencing industry-used models. PyTorch has a wider audience base.</p> </li> </ul> </li> <li> <p>Why is the AUC=0 when running the example?</p> <ul> <li>The example contains 100 sample data entries, which are meant to demonstrate data format and feature types, ensuring the code runs smoothly. It does not guarantee performance metrics.</li> <li>If you need to test performance, you can download the dataset using the links described in the readme, then refer to the parameter configuration file in the example for model training and evaluation.</li> </ul> </li> <li> <p>Installing annoy</p> <ul> <li> <p>Installing annoy on Windows</p> <ul> <li>Online installation</li> </ul> <pre><code>pip install annoy\n</code></pre> <p>If you don't have C++ related compilation environment on Windows, you might see the following error:</p> <pre><code>error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n---------------------------------------------------------------------\n</code></pre> <p>Error screenshot: </p> <p>In this case, you can use offline installation</p> <ul> <li>Offline installation</li> </ul> <p>Download annoy library from: https://www.lfd.uci.edu/~gohlke/pythonlibs/#_annoy</p> <pre><code>pip install annoy\u20111.17.0\u2011cp39\u2011cp39\u2011win_amd64.whl\n</code></pre> </li> <li> <p>Installing annoy on Linux/Mac OS</p> <ul> <li>Online installation</li> </ul> <pre><code>pip install annoy\n</code></pre> <p>Normally Mac can install successfully online. If online installation fails with nose-related errors at the bottom, proceed with offline compilation installation</p> <ul> <li> <p>Offline installation</p> </li> <li> <p>Download nose</p> <p>Download from: https://www.lfd.uci.edu/~gohlke/pythonlibs/#_annoy</p> <pre><code>pip install nose\u20111.3.7\u2011py3\u2011none\u2011any.whl\n</code></pre> </li> <li> <p>Download annoy</p> <p>Download from: https://files.pythonhosted.org/packages/a1/5b/1c22129f608b3f438713b91cd880dc681d747a860afe3e8e0af86e921942/annoy-1.17.0.tar.gz</p> <pre><code>tar -zxvf annoy-1.17.0.tar.gz\ncd annoy-1.17.0\npython setup.py install\n</code></pre> </li> </ul> </li> </ul> <p>After installing annoy, you can proceed to install torch-rechub</p> <pre><code>pip install --upgrade torch-rechub\n</code></pre> </li> </ul>"},{"location":"getting-started/","title":"Getting Started Tutorial","text":"<p>First, install Torch-RecHub:</p> <pre><code>pip install torch-rechub\n</code></pre> <p>Then use the following code to train recommender system models:</p>"},{"location":"getting-started/#ranking-ctr-prediction","title":"Ranking (CTR Prediction)","text":"<pre><code>from torch_rechub.models.ranking import DeepFM\nfrom torch_rechub.trainers import CTRTrainer\nfrom torch_rechub.utils.data import DataGenerator\n\ndg = DataGenerator(x, y)\ntrain_dl, val_dl, test_dl = dg.generate_dataloader(split_ratio=[0.7, 0.1], batch_size=256)\n\nmodel = DeepFM(deep_features=deep_features, fm_features=fm_features, \n               mlp_params={\"dims\": [256, 128], \"dropout\": 0.2, \"activation\": \"relu\"})\n\nctr_trainer = CTRTrainer(model)\nctr_trainer.fit(train_dl, val_dl)\nauc = ctr_trainer.evaluate(test_dl)\n</code></pre>"},{"location":"getting-started/#multi-task-learning","title":"Multi-Task Learning","text":"<pre><code>from torch_rechub.models.multi_task import SharedBottom, ESMM, MMOE, PLE, AITM\nfrom torch_rechub.trainers import MTLTrainer\n\ntask_types = [\"classification\", \"classification\"]\nmodel = MMOE(features, task_types, 8, \n            expert_params={\"dims\": [32,16]}, \n            tower_params_list=[{\"dims\": [32, 16]}, {\"dims\": [32, 16]}])\n\nmtl_trainer = MTLTrainer(model)\nmtl_trainer.fit(train_dl, val_dl)\n</code></pre>"},{"location":"getting-started/#matching-models","title":"Matching Models","text":"<pre><code>from torch_rechub.models.matching import DSSM\nfrom torch_rechub.trainers import MatchTrainer\nfrom torch_rechub.utils.data import MatchDataGenerator\n\ndg = MatchDataGenerator(x, y)\ntrain_dl, test_dl, item_dl = dg.generate_dataloader(test_user, all_item, batch_size=256)\n\nmodel = DSSM(user_features, item_features, temperature=0.02,\n             user_params={\"dims\": [256, 128, 64], \"activation\": 'prelu'},\n             item_params={\"dims\": [256, 128, 64], \"activation\": 'prelu'})\n\nmatch_trainer = MatchTrainer(model)\nmatch_trainer.fit(train_dl)\n</code></pre>"},{"location":"getting-started/#model-zoo","title":"Model Zoo","text":""},{"location":"getting-started/#model-list","title":"Model List","text":"Title Tag Development Status Developers Institution Meeting Year URL pdf DIN Rank,Sequence Completed \u8d56\u654f\u6750 Alibaba KDD 2018 https://arxiv.org/abs/1706.06978 1706.06978.pdf ESMM Rank Completed \u8d56\u654f\u6750 Alibaba SIGIR 2018 https://arxiv.org/abs/1804.07931 1804.07931.pdf Youtube-SBC Match Completed \u8d56\u654f\u6750 Google RecSys 2019 https://research.google/pubs/pub48840/ 6c8a86c981a62b0126a11896b7f6ae0dae4c3566.pdf DSSM Match Completed \u8d56\u654f\u6750 \u5fae\u8f6f CIKM 2013 https://posenhuang.github.io/papers/cikm2013_DSSM_fullversion.pdf cikm2013_DSSM_fullversion.pdf MetaBalance \u5176\u4ed6 Completed Facebook www 2022 https://arxiv.org/pdf/2203.06801v1.pdf 2203.06801v1-3.pdf Wide &amp; Deep Rank Completed \u8d56\u654f\u6750 Google DLRS 2016 https://arxiv.org/pdf/1606.07792.pdf 1606.07792.pdf DSSM-Facebook Match Completed \u8d56\u654f\u6750 Facebook KDD 2020 https://arxiv.org/abs/2006.11632 2006.11632.pdf DeepFM Rank Completed \u8d56\u654f\u6750 Huawei IJCAI 2017 https://arxiv.org/abs/1703.04247 1703.04247.pdf SasRec Match \u8fdb\u884c\u4e2d \u738b\u5b87\u5bb8 PLE Rank Completed \u8d56\u654f\u6750 Tencent RecSys 2020 https://dl.acm.org/doi/abs/10.1145/3383313.3412236?casa_token=4g_ErWbxWf8AAAAA%3APhbcdBa6b-SXHlpFtKh1Lybjtv48sYV2l1GsPeL43N5Lpih_GwarAwV5hzxOYUVZoWd8dimltm4czmI 2020 (Tencent) (Recsys) [PLE] Progressive Layered Extraction (PLE) - A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations.pdf AITM Rank Completed \u8d56\u654f\u6750 Meituan KDD 2021 https://arxiv.org/abs/2105.08489 2105.08489-2.pdf Shared-Bottom Rank Completed \u8d56\u654f\u6750 CMU ML 1997 https://link.springer.com/content/pdf/10.1023/A:1007379606734.pdf Caruana1997_Article_MultitaskLearning.pdf DCN Rank Completed \u8d56\u654f\u6750 Google,\u65af\u5766\u798f AKDD 2017 https://arxiv.org/abs/1708.05123 1708.05123.pdf Youtube-DNN Match Completed \u8d56\u654f\u6750 Google RecSys 2016 https://dl.acm.org/doi/10.1145/2959100.2959190 2959100.2959190.pdf MMOE Rank Completed \u8d56\u654f\u6750 Google KDD 2018 https://dl.acm.org/doi/pdf/10.1145/3219819.3220007 3219819.3220007.pdf GRU4Rec Match,Sequence Completed \u738b\u51ef Tencent KDD 2022 SASRec Match,Sequence Completed \u738b\u5b87\u5bb8 UC ICDM 2018 https://arxiv.org/pdf/1808.09781.pdf 1808.09781-3.pdf SINE Match Completed \u5eb7\u535a Alibaba WSDM 2021 https://arxiv.org/pdf/2102.09267.pdf 2102.09267.pdf (FAT-)DeepFFM Rank Completed \u5eb7\u535a Sina arXiv 2019 https://arxiv.org/pdf/1905.06336.pdf 1905.06336.pdf STAMP Match,Sequence Completed \u5eb7\u535a \u7535\u5b50\u79d1\u5927 KDD 2018 https://dl.acm.org/doi/10.1145/3219819.3219950 3219819.3219950.pdf NARM Match,Sequence Completed \u5eb7\u535a \u4eac\u4e1c,\u5c71\u4e1c\u5927\u5b66 CIKM 2017 https://arxiv.org/pdf/1711.04725.pdf 1711.00165.pdf DCN_v2 Rank Completed \u53f6\u5fd7\u96c4 Google www 2021 https://arxiv.org/abs/2008.13535 DCN V2 Improved Deep &amp; Cross Network and Practical Lessons.pdf EDCN Rank Completed \u53f6\u5fd7\u96c4 Huawei KDD 2021 https://dlp-kdd.github.io/assets/pdf/DLP-KDD_2021_paper_12.pdf FiBiNet Rank Completed \u53f6\u5fd7\u96c4 Sina RecSys 2019 https://dl.acm.org/doi/abs/10.1145/3298689.3347043 DIEN Rank,Sequence Completed \u8303\u6d9b Alibaba AAAI 2019 https://ojs.aaai.org/index.php/AAAI/article/view/4545 4545-Article Text-7584-1-10-20190706.pdf BST Rank,Sequence Completed \u8303\u6d9b Alibaba arXiv 2019 Behavior Sequence Transformer for E-commerce Recommendation in Alibaba pdf"},{"location":"installation/","title":"Installation Guide","text":"<p>This document provides detailed installation instructions for Torch-RecHub, including both stable and development versions.</p>"},{"location":"installation/#stable-release","title":"Stable Release","text":"<pre><code>pip install torch-rechub\n</code></pre>"},{"location":"installation/#latest-version-recommended","title":"Latest Version (Recommended)","text":"<pre><code>git clone https://github.com/datawhalechina/torch-rechub.git\ncd torch-rechub\npython setup.py install\n</code></pre>"},{"location":"introduction/","title":"Project Introduction","text":""},{"location":"introduction/#project-overview","title":"Project Overview","text":"<p>Torch-RecHub is a flexible and easily extensible recommendation system framework built using PyTorch. It aims to simplify the research and application of recommendation algorithms, providing common model implementations, data processing tools, and evaluation metrics.</p>"},{"location":"introduction/#features","title":"Features","text":"<ul> <li>Modular Design: Easy to add new models, datasets, and evaluation metrics.</li> <li>Based on PyTorch: Leverage PyTorch's dynamic graph and GPU acceleration capabilities.</li> <li>Rich Model Library: Includes various classic and cutting - edge recommendation algorithms (listed below).</li> <li>Standardized Process: Provide unified data loading, training, and evaluation processes.</li> <li>Easy to Configure: Easily adjust experimental settings through configuration files or command - line parameters.</li> <li>Reproducibility: Aims to ensure the reproducibility of experimental results.</li> <li>Easy to Extend: Decouple model training from model definition, without the concept of a base model.</li> <li>Native Functions: Use PyTorch's native classes and functions as much as possible without excessive customization.</li> <li>Concise Model Code: Facilitate beginners' learning while adhering to the ideas of academic papers.</li> <li>Other Features: For example, support negative sampling, multi - task learning, etc.</li> </ul>"},{"location":"introduction/#overall-architecture","title":"Overall Architecture","text":""},{"location":"introduction/#data-layer-design","title":"Data Layer Design","text":""},{"location":"introduction/#feature-classes","title":"Feature Classes","text":"<p>Numerical Features</p> <ul> <li>Such as age, salary, daily click - through rate, etc.</li> </ul> <p>Categorical Features</p> <ul> <li>Such as city, education level, gender, etc.</li> <li>Encode with LabelEncoder to obtain Embedding vectors.</li> </ul> <p>Sequence Features</p> <ul> <li>Ordered interest sequences: such as the item list clicked in the last week.</li> <li>Unordered tag features: such as movie genres (action | suspense | crime).</li> <li>Encode with LabelEncoder to obtain sequence Embedding vectors.</li> <li>Perform pooling to reduce dimensions.</li> <li>Preserve the sequence for model operations such as attention with other features.</li> <li>Share the Embedding Table with Sparse features.</li> </ul>"},{"location":"introduction/#data-classes","title":"Data Classes","text":"<ul> <li>Dataset</li> <li>Dataloader</li> </ul>"},{"location":"introduction/#tool-classes","title":"Tool Classes","text":"<ul> <li>Sequence feature generation</li> <li>Sample construction</li> <li>Negative sampling</li> <li>Vectorized retrieval</li> </ul>"},{"location":"introduction/#model-layer-design","title":"Model Layer Design","text":""},{"location":"introduction/#model-classes","title":"Model Classes","text":"<p>General Layers Shallow Feature Modeling</p> <ul> <li>LR: Logistic Regression</li> <li>MLP: Multi - Layer Perceptron, parameters such as dims can be set through a dictionary.</li> <li>EmbeddingLayer: A general Embedding layer that handles three types of features, maintains an EmbeddingTable in dictionary format, and outputs the input embeddings required by the model.</li> </ul> <p>Deep Feature Modeling</p> <ul> <li>FM, FFM, CIN</li> <li>self - attention, target - attention, transformer</li> </ul> <p>Custom Layers</p>"},{"location":"api-reference/basic/","title":"Basic Components API Reference","text":"<p>This document provides detailed documentation for basic components in Torch-RecHub, including feature processing, data transformation, and other fundamental functionalities.</p>"},{"location":"api-reference/basic/#feature-processing","title":"Feature Processing","text":""},{"location":"api-reference/basic/#feature-columns","title":"Feature Columns","text":""},{"location":"api-reference/basic/#densefeature","title":"DenseFeature","text":"<ul> <li>Introduction: Process continuous numerical features.</li> <li>Parameters:</li> <li><code>name</code> (str): Feature name</li> <li><code>dimension</code> (int): Feature dimension</li> <li><code>dtype</code> (str): Data type, default 'float32'</li> </ul>"},{"location":"api-reference/basic/#sparsefeature","title":"SparseFeature","text":"<ul> <li>Introduction: Process discrete categorical features.</li> <li>Parameters:</li> <li><code>name</code> (str): Feature name</li> <li><code>vocabulary_size</code> (int): Size of category vocabulary</li> <li><code>embedding_dim</code> (int): Embedding vector dimension</li> <li><code>dtype</code> (str): Data type, default 'int32'</li> <li><code>embedding_name</code> (str): Embedding layer name, default None</li> </ul>"},{"location":"api-reference/basic/#varlensparsefeature","title":"VarLenSparseFeature","text":"<ul> <li>Introduction: Process variable-length discrete features.</li> <li>Parameters:</li> <li><code>name</code> (str): Feature name</li> <li><code>vocabulary_size</code> (int): Size of category vocabulary</li> <li><code>embedding_dim</code> (int): Embedding vector dimension</li> <li><code>maxlen</code> (int): Maximum sequence length</li> <li><code>dtype</code> (str): Data type, default 'int32'</li> <li><code>embedding_name</code> (str): Embedding layer name, default None</li> <li><code>combiner</code> (str): Sequence pooling method, options: 'sum', 'mean', 'max', default 'mean'</li> </ul>"},{"location":"api-reference/basic/#data-transformation","title":"Data Transformation","text":""},{"location":"api-reference/basic/#data-preprocessing","title":"Data Preprocessing","text":""},{"location":"api-reference/basic/#minmaxscaler","title":"MinMaxScaler","text":"<ul> <li>Introduction: Normalize numerical features.</li> <li>Parameters:</li> <li><code>feature_range</code> (tuple): Normalization range, default (0, 1)</li> </ul>"},{"location":"api-reference/basic/#standardscaler","title":"StandardScaler","text":"<ul> <li>Introduction: Standardize numerical features.</li> <li>Parameters:</li> <li><code>with_mean</code> (bool): Whether to remove mean, default True</li> <li><code>with_std</code> (bool): Whether to scale by standard deviation, default True</li> </ul>"},{"location":"api-reference/basic/#labelencoder","title":"LabelEncoder","text":"<ul> <li>Introduction: Encode categorical features.</li> <li>Methods:</li> <li><code>fit(values)</code>: Fit the encoder</li> <li><code>transform(values)</code>: Transform data</li> <li><code>fit_transform(values)</code>: Fit and transform</li> </ul>"},{"location":"api-reference/basic/#data-format-conversion","title":"Data Format Conversion","text":""},{"location":"api-reference/basic/#pandas_to_torch","title":"pandas_to_torch","text":"<ul> <li>Introduction: Convert Pandas data to PyTorch tensors.</li> <li>Parameters:</li> <li><code>df</code> (pd.DataFrame): Input DataFrame</li> <li><code>dense_cols</code> (list): List of continuous feature column names</li> <li><code>sparse_cols</code> (list): List of discrete feature column names</li> <li><code>device</code> (str): Device type, 'cpu' or 'cuda'</li> </ul>"},{"location":"api-reference/basic/#numpy_to_torch","title":"numpy_to_torch","text":"<ul> <li>Introduction: Convert NumPy arrays to PyTorch tensors.</li> <li>Parameters:</li> <li><code>arrays</code> (list): List of NumPy arrays</li> <li><code>device</code> (str): Device type, 'cpu' or 'cuda'</li> </ul>"},{"location":"api-reference/basic/#model-components","title":"Model Components","text":""},{"location":"api-reference/basic/#activation-functions","title":"Activation Functions","text":""},{"location":"api-reference/basic/#dice","title":"Dice","text":"<ul> <li>Introduction: Dice activation function, proposed in Deep Interest Network (DIN).</li> <li>Parameters:</li> <li><code>epsilon</code> (float): Smoothing parameter, default 1e-3</li> <li><code>device</code> (str): Device type, default 'cpu'</li> </ul>"},{"location":"api-reference/basic/#attention-mechanisms","title":"Attention Mechanisms","text":""},{"location":"api-reference/basic/#scaleddotproductattention","title":"ScaledDotProductAttention","text":"<ul> <li>Introduction: Scaled dot-product attention mechanism.</li> <li>Parameters:</li> <li><code>temperature</code> (float): Temperature parameter for scaling</li> <li><code>attn_dropout</code> (float): Attention dropout rate</li> </ul>"},{"location":"api-reference/basic/#multiheadattention","title":"MultiHeadAttention","text":"<ul> <li>Introduction: Multi-head attention mechanism.</li> <li>Parameters:</li> <li><code>d_model</code> (int): Model dimension</li> <li><code>n_heads</code> (int): Number of attention heads</li> <li><code>d_k</code> (int): Key vector dimension</li> <li><code>d_v</code> (int): Value vector dimension</li> <li><code>dropout</code> (float): Dropout rate </li> </ul>"},{"location":"api-reference/models/","title":"Models API Reference","text":"<p>This section provides detailed API documentation for all models in Torch-RecHub.</p>"},{"location":"api-reference/models/#recall-models","title":"Recall Models","text":"<p>Recall models are primarily used in the recall stage for quick retrieval of relevant items from massive candidate sets. They typically adopt two-tower or sequential model structures to meet the efficiency requirements of the recall stage.</p>"},{"location":"api-reference/models/#two-tower-model-series","title":"Two-Tower Model Series","text":""},{"location":"api-reference/models/#dssm-deep-structured-semantic-model","title":"DSSM (Deep Structured Semantic Model)","text":"<ul> <li>Introduction: Originally proposed by Microsoft for semantic matching and later widely applied in recommender systems. Adopts classic two-tower structure that separately represents users and items, computing similarity through inner product. This structure allows pre-computation of item vectors during online serving, greatly improving service efficiency. The key lies in learning effective user and item representations.</li> <li>Parameters:</li> <li><code>user_features</code> (list): List of user features</li> <li><code>item_features</code> (list): List of item features</li> <li><code>hidden_units</code> (list): List of hidden layer units</li> <li><code>dropout_rates</code> (list): List of dropout rates</li> <li><code>embedding_dim</code> (int): Final representation vector dimension</li> </ul>"},{"location":"api-reference/models/#facebook-dssm","title":"Facebook DSSM","text":"<ul> <li>Introduction: Facebook's improved version of DSSM that incorporates multi-task learning framework. Besides the main recall task, it adds auxiliary tasks to help learn better feature representations. The model can simultaneously optimize multiple related objectives like clicks, favorites, purchases, etc., learning richer user and item representations.</li> <li>Parameters:</li> <li><code>user_features</code> (list): List of user features</li> <li><code>item_features</code> (list): List of item features</li> <li><code>hidden_units</code> (list): List of hidden layer units</li> <li><code>num_tasks</code> (int): Number of tasks</li> <li><code>task_types</code> (list): List of task types</li> </ul>"},{"location":"api-reference/models/#youtube-dnn","title":"YouTube DNN","text":"<ul> <li>Introduction: A deep recall model proposed by YouTube, designed for large-scale video recommendation scenarios. The model aggregates user viewing history through average pooling and combines it with other user features. Innovatively introduces negative sampling techniques and multi-task learning framework to improve training efficiency and effectiveness.</li> <li>Parameters:</li> <li><code>user_features</code> (list): List of user features</li> <li><code>item_features</code> (list): List of item features</li> <li><code>hidden_units</code> (list): List of hidden layer units</li> <li><code>embedding_dim</code> (int): Embedding dimension</li> <li><code>max_seq_len</code> (int): Maximum sequence length</li> </ul>"},{"location":"api-reference/models/#sequential-recommendation-series","title":"Sequential Recommendation Series","text":""},{"location":"api-reference/models/#gru4rec","title":"GRU4Rec","text":"<ul> <li>Introduction: A pioneering work that first applied GRU networks to session-based sequential recommendation. Through GRU network, it captures temporal dependencies in user behavior sequences, with hidden states at each time step containing information about historical behaviors. The model also introduces special mini-batch construction methods and loss function designs to adapt to the characteristics of sequential recommendation.</li> <li>Parameters:</li> <li><code>item_num</code> (int): Total number of items</li> <li><code>hidden_size</code> (int): Size of GRU hidden layer</li> <li><code>num_layers</code> (int): Number of GRU layers</li> <li><code>dropout_rate</code> (float): Dropout rate</li> <li><code>embedding_dim</code> (int): Item embedding dimension</li> </ul>"},{"location":"api-reference/models/#narm-neural-attentive-recommendation-machine","title":"NARM (Neural Attentive Recommendation Machine)","text":"<ul> <li>Introduction: A sequential recommendation model that introduces attention mechanism on top of GRU4Rec. Through attention mechanism, the model can dynamically focus on relevant behaviors in the sequence based on the current prediction target. It maintains both global and local sequence representations, comprehensively capturing user's short-term interests. This design enables better handling of user interest diversity and dynamics.</li> <li>Parameters:</li> <li><code>item_num</code> (int): Total number of items</li> <li><code>hidden_size</code> (int): Size of hidden layer</li> <li><code>attention_size</code> (int): Size of attention layer</li> <li><code>dropout_rate</code> (float): Dropout rate</li> <li><code>embedding_dim</code> (int): Item embedding dimension</li> </ul>"},{"location":"api-reference/models/#sasrec-self-attentive-sequential-recommendation","title":"SASRec (Self-Attentive Sequential Recommendation)","text":"<ul> <li>Introduction: A representative work that applies Transformer structure to sequential recommendation. Through self-attention mechanism, the model can directly compute and learn relationships between any two behaviors in the sequence, unrestricted by RNN's inherent sequential dependencies. Position encoding helps preserve temporal information of behaviors, while multi-layer structure allows the model to extract increasingly abstract behavior patterns layer by layer. Compared to RNN-based models, it offers better parallelism and scalability.</li> <li>Parameters:</li> <li><code>item_num</code> (int): Total number of items</li> <li><code>max_len</code> (int): Maximum sequence length</li> <li><code>num_heads</code> (int): Number of attention heads</li> <li><code>num_layers</code> (int): Number of Transformer layers</li> <li><code>hidden_size</code> (int): Hidden layer dimension</li> <li><code>dropout_rate</code> (float): Dropout rate</li> </ul>"},{"location":"api-reference/models/#mind-multi-interest-network-with-dynamic-routing","title":"MIND (Multi-Interest Network with Dynamic routing)","text":"<ul> <li>Introduction: A recall model designed for user's diverse interests. Through capsule network and dynamic routing mechanism, it extracts multiple interest vectors from user's behavior sequence. Each interest vector represents user preferences in different aspects, providing a more comprehensive characterization of user interest distribution.</li> <li>Parameters:</li> <li><code>item_num</code> (int): Total number of items</li> <li><code>num_interests</code> (int): Number of interest vectors</li> <li><code>routing_iterations</code> (int): Number of dynamic routing iterations</li> <li><code>hidden_size</code> (int): Hidden layer dimension</li> <li><code>embedding_dim</code> (int): Item embedding dimension</li> </ul>"},{"location":"api-reference/models/#ranking-models","title":"Ranking Models","text":"<p>Ranking models are primarily used in the fine-ranking stage to precisely rank candidate items. They learn complex interactions between users and items through deep learning methods to generate final ranking scores.</p>"},{"location":"api-reference/models/#wide-deep-series","title":"Wide &amp; Deep Series","text":""},{"location":"api-reference/models/#widedeep","title":"WideDeep","text":"<ul> <li>Introduction: A classic model proposed by Google in 2016 that combines the advantages of linear models and deep neural networks. The Wide part performs memorization through feature crosses, suitable for modeling direct, explicit feature correlations; the Deep part performs generalization through deep networks, capable of learning implicit, high-order feature relationships. This combination allows the model to both memorize historical patterns and generalize to new patterns.</li> <li>Parameters:</li> <li><code>wide_features</code> (list): List of features for the wide part, used in linear layer</li> <li><code>deep_features</code> (list): List of features for the deep part, used in deep network</li> <li><code>hidden_units</code> (list): List of hidden layer units for the deep network, e.g., [256, 128, 64]</li> <li><code>dropout_rates</code> (list): Dropout rates for each layer, used for preventing overfitting</li> </ul>"},{"location":"api-reference/models/#deepfm","title":"DeepFM","text":"<ul> <li>Introduction: A model that combines Factorization Machines (FM) feature interactions with deep learning models. The FM part efficiently models second-order feature interactions, while the Deep part learns high-order feature relationships. Compared to Wide&amp;Deep, DeepFM doesn't require manual feature engineering and can automatically learn feature crosses. The model consists of three parts: first-order features, FM's second-order interactions, and deep network's high-order interactions.</li> <li>Parameters:</li> <li><code>features</code> (list): List of features</li> <li><code>hidden_units</code> (list): Hidden layer units for DNN part</li> <li><code>dropout_rates</code> (list): List of dropout rates</li> <li><code>embedding_dim</code> (int): Feature embedding dimension</li> </ul>"},{"location":"api-reference/models/#dcn-dcn-v2","title":"DCN / DCN-V2","text":"<ul> <li>Introduction: Learns feature interactions explicitly through specially designed Cross Network layers. Each cross layer performs interactions between feature vectors and their original form, increasing the degree of feature crossing as the depth increases. DCN-V2 improves the cross network parameterization, offering both \"vector\" and \"matrix\" options, maintaining model expressiveness while improving efficiency.</li> <li>Parameters:</li> <li><code>features</code> (list): List of features</li> <li><code>cross_num</code> (int): Number of cross layers</li> <li><code>hidden_units</code> (list): Hidden layer units for DNN part</li> <li><code>cross_parameterization</code> (str, DCN-V2): Cross parameterization method, \"vector\" or \"matrix\"</li> </ul>"},{"location":"api-reference/models/#afm-attentional-factorization-machine","title":"AFM (Attentional Factorization Machine)","text":"<ul> <li>Introduction: Introduces attention mechanism to FM, assigning different importance weights to different feature interactions. Through the attention network, it adaptively learns the importance of feature interactions, identifying feature combinations that are more relevant to the prediction target.</li> <li>Parameters:</li> <li><code>features</code> (list): List of features</li> <li><code>attention_units</code> (list): Hidden layer units for attention network</li> <li><code>embedding_dim</code> (int): Feature embedding dimension</li> <li><code>dropout_rate</code> (float): Dropout rate for attention network</li> </ul>"},{"location":"api-reference/models/#fibinet-feature-importance-and-bilinear-feature-interaction-network","title":"FiBiNET (Feature Importance and Bilinear feature Interaction Network)","text":"<ul> <li>Introduction: Dynamically learns feature importance through SENET mechanism and uses bilinear layers for feature interaction. The SENET module helps identify important features, while bilinear interaction provides richer feature interaction methods than inner products.</li> <li>Parameters:</li> <li><code>features</code> (list): List of features</li> <li><code>bilinear_type</code> (str): Bilinear layer type, options: \"field_all\"/\"field_each\"/\"field_interaction\"</li> <li><code>hidden_units</code> (list): Hidden layer units for DNN part</li> <li><code>reduction_ratio</code> (int): Reduction ratio for SENET module</li> </ul>"},{"location":"api-reference/models/#attention-based-series","title":"Attention-based Series","text":""},{"location":"api-reference/models/#din-deep-interest-network","title":"DIN (Deep Interest Network)","text":"<ul> <li>Introduction: A model designed for user interest diversity, using attention mechanism for adaptive learning of user historical behaviors. The model dynamically calculates relevance weights of user historical behaviors based on the current candidate ad, thereby activating relevant user interests and capturing diverse user preferences. It innovatively introduced attention mechanism to recommender systems, pioneering a new paradigm for behavior sequence modeling.</li> <li>Parameters:</li> <li><code>features</code> (list): List of base features</li> <li><code>behavior_features</code> (list): List of behavior features for attention calculation</li> <li><code>attention_units</code> (list): Hidden layer units for attention network</li> <li><code>hidden_units</code> (list): Hidden layer units for DNN part</li> <li><code>activation</code> (str): Activation function type</li> </ul>"},{"location":"api-reference/models/#dien-deep-interest-evolution-network","title":"DIEN (Deep Interest Evolution Network)","text":"<ul> <li>Introduction: An advanced version of DIN that models the dynamic evolution of user interests through interest evolution layer. It uses GRU structure to capture interest evolution and innovatively designs AUGRU (GRU with Attentional Update Gate) to make the interest evolution process aware of target items. It also includes auxiliary loss to supervise the training of interest extraction layer. This design not only captures the dynamic changes of user interests but also models the temporal dependencies of interests.</li> <li>Parameters:</li> <li><code>features</code> (list): List of base features</li> <li><code>behavior_features</code> (list): List of behavior features</li> <li><code>interest_units</code> (list): Units for interest extraction layer</li> <li><code>gru_type</code> (str): GRU type, \"AUGRU\" or \"AIGRU\"</li> <li><code>hidden_units</code> (list): Hidden layer units for DNN part</li> </ul>"},{"location":"api-reference/models/#bst-behavior-sequence-transformer","title":"BST (Behavior Sequence Transformer)","text":"<ul> <li>Introduction: A pioneering work that introduces Transformer architecture to recommender systems for modeling user behavior sequences. Through self-attention mechanism, the model can directly compute relationships between any two behaviors in the sequence, overcoming the limitations of RNN models in processing long sequences. Position embedding helps the model perceive temporal information of behaviors, while multi-head attention allows the model to understand user behavior patterns from multiple perspectives.</li> <li>Parameters:</li> <li><code>features</code> (list): List of base features</li> <li><code>behavior_features</code> (list): List of behavior features</li> <li><code>num_heads</code> (int): Number of attention heads</li> <li><code>num_layers</code> (int): Number of Transformer layers</li> <li><code>hidden_size</code> (int): Hidden layer dimension</li> <li><code>dropout_rate</code> (float): Dropout rate</li> </ul>"},{"location":"api-reference/models/#edcn-enhancing-explicit-and-implicit-feature-interactions","title":"EDCN (Enhancing Explicit and Implicit Feature Interactions)","text":"<ul> <li>Introduction: A deep cross network that enhances both explicit and implicit feature interactions. Through a newly designed cross network structure, it considers both explicit and implicit feature interactions. Introduces gating mechanism to regulate the importance of different orders of feature interactions and uses residual connections to alleviate training issues in deep networks.</li> <li>Parameters:</li> <li><code>features</code> (list): List of features</li> <li><code>cross_num</code> (int): Number of cross layers</li> <li><code>hidden_units</code> (list): Hidden layer units for DNN part</li> <li><code>gate_type</code> (str): Gate type, \"FGU\" or \"BGU\"</li> </ul>"},{"location":"api-reference/models/#multi-task-models","title":"Multi-task Models","text":"<p>Multi-task models learn multiple related tasks jointly to achieve knowledge sharing and transfer, improving overall model performance.</p>"},{"location":"api-reference/models/#sharedbottom","title":"SharedBottom","text":"<ul> <li>Introduction: The most basic multi-task learning model that shares parameters in bottom network for extracting common feature representations. The shared layers learn common features across tasks, while task-specific layers learn individualized features for each task. This simple yet effective structure laid the foundation for multi-task learning.</li> <li>Parameters:</li> <li><code>features</code> (list): List of features</li> <li><code>hidden_units</code> (list): Hidden layer units for shared network</li> <li><code>task_hidden_units</code> (list): Hidden layer units for task-specific networks</li> <li><code>num_tasks</code> (int): Number of tasks</li> <li><code>task_types</code> (list): List of task types</li> </ul>"},{"location":"api-reference/models/#esmm-entire-space-multi-task-model","title":"ESMM (Entire Space Multi-Task Model)","text":"<ul> <li>Introduction: An innovative multi-task model proposed by Alibaba specifically designed to address sample selection bias in recommender systems. Through joint modeling of CVR and CTR tasks, it performs parameter learning in the entire space. The core innovation lies in introducing CTR as an auxiliary task and optimizing CVR estimation through task multiplication relationship. This design not only solves the sample selection bias in traditional CVR estimation but also provides unbiased CTR and CTCVR estimation.</li> <li>Parameters:</li> <li><code>features</code> (list): List of features</li> <li><code>hidden_units</code> (list): List of hidden layer units</li> <li><code>tower_units</code> (list): List of task tower layer units</li> <li><code>embedding_dim</code> (int): Feature embedding dimension</li> </ul>"},{"location":"api-reference/models/#mmoe-multi-gate-mixture-of-experts","title":"MMoE (Multi-gate Mixture-of-Experts)","text":"<ul> <li>Introduction: A multi-task learning model proposed by Google that achieves soft parameter sharing through expert mechanism and task-related gating networks. Each expert network can learn specific feature transformations, while gating networks dynamically allocate expert importance for each task. This design allows the model to flexibly combine expert knowledge based on task requirements, effectively handling task differences.</li> <li>Parameters:</li> <li><code>features</code> (list): List of features</li> <li><code>expert_units</code> (list): Hidden layer units for expert networks</li> <li><code>num_experts</code> (int): Number of experts</li> <li><code>num_tasks</code> (int): Number of tasks</li> <li><code>expert_activation</code> (str): Activation function for expert networks</li> <li><code>gate_activation</code> (str): Activation function for gate networks</li> </ul>"},{"location":"api-reference/models/#ple-progressive-layered-extraction","title":"PLE (Progressive Layered Extraction)","text":"<ul> <li>Introduction: An improved version of MMoE that better models task relationships through progressive layered extraction. Introduces the concept of task-specific experts and shared experts, implementing progressive feature extraction through multi-level expert networks. Each layer contains both task-specific experts and shared experts, allowing the model to learn both commonalities and individualities of tasks. This progressive design enhances the model's ability for knowledge extraction and transfer.</li> <li>Parameters:</li> <li><code>features</code> (list): List of features</li> <li><code>expert_units</code> (list): Units for expert networks</li> <li><code>num_experts</code> (int): Number of experts per layer</li> <li><code>num_layers</code> (int): Number of layers</li> <li><code>num_shared_experts</code> (int): Number of shared experts</li> <li><code>task_types</code> (list): List of task types</li> </ul>"},{"location":"api-reference/trainers/","title":"Trainers API Reference","text":"<p>This section provides detailed API documentation for all trainers in Torch-RecHub.</p>"},{"location":"api-reference/trainers/#ctrtrainer","title":"CTRTrainer","text":"<p>CTRTrainer is a general trainer for single task learning, primarily used for binary classification tasks such as Click-Through Rate (CTR) prediction.</p>"},{"location":"api-reference/trainers/#parameters","title":"Parameters","text":"<ul> <li><code>model</code> (nn.Module): Any single task learning model</li> <li><code>optimizer_fn</code> (torch.optim): PyTorch optimizer function, defaults to <code>torch.optim.Adam</code></li> <li><code>optimizer_params</code> (dict): Optimizer parameters, defaults to <code>{\"lr\": 1e-3, \"weight_decay\": 1e-5}</code></li> <li><code>scheduler_fn</code> (torch.optim.lr_scheduler): PyTorch learning rate scheduler, e.g., <code>torch.optim.lr_scheduler.StepLR</code></li> <li><code>scheduler_params</code> (dict): Learning rate scheduler parameters</li> <li><code>n_epoch</code> (int): Number of training epochs</li> <li><code>earlystop_patience</code> (int): Number of epochs to wait before early stopping when validation performance doesn't improve, defaults to 10</li> <li><code>device</code> (str): Device to use, either <code>\"cpu\"</code> or <code>\"cuda:0\"</code></li> <li><code>gpus</code> (list): List of GPU IDs, defaults to empty. If length &gt;=1, model will be wrapped by nn.DataParallel</li> <li><code>loss_mode</code> (bool): Training mode, defaults to True</li> <li><code>model_path</code> (str): Path to save the model, defaults to <code>\"./\"</code></li> </ul>"},{"location":"api-reference/trainers/#main-methods","title":"Main Methods","text":"<ul> <li><code>train_one_epoch(data_loader, log_interval=10)</code>: Train for one epoch</li> <li><code>fit(train_dataloader, val_dataloader=None)</code>: Train the model</li> <li><code>evaluate(model, data_loader)</code>: Evaluate the model</li> <li><code>predict(model, data_loader)</code>: Make predictions</li> </ul>"},{"location":"api-reference/trainers/#matchtrainer","title":"MatchTrainer","text":"<p>MatchTrainer is a trainer for matching/retrieval tasks, supporting multiple training modes.</p>"},{"location":"api-reference/trainers/#parameters_1","title":"Parameters","text":"<ul> <li><code>model</code> (nn.Module): Any matching model</li> <li><code>mode</code> (int): Training mode, options:</li> <li>0: point-wise</li> <li>1: pair-wise</li> <li>2: list-wise</li> <li><code>optimizer_fn</code> (torch.optim): Same as CTRTrainer</li> <li><code>optimizer_params</code> (dict): Same as CTRTrainer</li> <li><code>scheduler_fn</code> (torch.optim.lr_scheduler): Same as CTRTrainer</li> <li><code>scheduler_params</code> (dict): Same as CTRTrainer</li> <li><code>n_epoch</code> (int): Same as CTRTrainer</li> <li><code>earlystop_patience</code> (int): Same as CTRTrainer</li> <li><code>device</code> (str): Same as CTRTrainer</li> <li><code>gpus</code> (list): Same as CTRTrainer</li> <li><code>model_path</code> (str): Same as CTRTrainer</li> </ul>"},{"location":"api-reference/trainers/#main-methods_1","title":"Main Methods","text":"<ul> <li><code>train_one_epoch(data_loader, log_interval=10)</code>: Train for one epoch</li> <li><code>fit(train_dataloader, val_dataloader=None)</code>: Train the model</li> <li><code>evaluate(model, data_loader)</code>: Evaluate the model</li> <li><code>predict(model, data_loader)</code>: Make predictions</li> <li><code>inference_embedding(model, mode, data_loader, model_path)</code>: Infer embeddings</li> <li><code>mode</code>: Either \"user\" or \"item\"</li> </ul>"},{"location":"api-reference/trainers/#mtltrainer","title":"MTLTrainer","text":"<p>MTLTrainer is a trainer for multi-task learning, supporting various adaptive loss weighting methods.</p>"},{"location":"api-reference/trainers/#parameters_2","title":"Parameters","text":"<ul> <li><code>model</code> (nn.Module): Any multi-task learning model</li> <li><code>task_types</code> (list): List of task types, supports [\"classification\", \"regression\"]</li> <li><code>optimizer_fn</code> (torch.optim): Same as CTRTrainer</li> <li><code>optimizer_params</code> (dict): Same as CTRTrainer</li> <li><code>scheduler_fn</code> (torch.optim.lr_scheduler): Same as CTRTrainer</li> <li><code>scheduler_params</code> (dict): Same as CTRTrainer</li> <li><code>adaptive_params</code> (dict): Adaptive loss weighting method parameters, supports:</li> <li><code>{\"method\": \"uwl\"}</code>: Uncertainty Weighted Loss</li> <li><code>{\"method\": \"metabalance\"}</code>: MetaBalance method</li> <li><code>{\"method\": \"gradnorm\", \"alpha\": 0.16}</code>: GradNorm method</li> <li><code>n_epoch</code> (int): Same as CTRTrainer</li> <li><code>earlystop_taskid</code> (int): Task ID for early stopping, defaults to 0</li> <li><code>earlystop_patience</code> (int): Same as CTRTrainer</li> <li><code>device</code> (str): Same as CTRTrainer</li> <li><code>gpus</code> (list): Same as CTRTrainer</li> <li><code>model_path</code> (str): Same as CTRTrainer</li> </ul>"},{"location":"api-reference/trainers/#main-methods_2","title":"Main Methods","text":"<ul> <li><code>train_one_epoch(data_loader)</code>: Train for one epoch</li> <li><code>fit(train_dataloader, val_dataloader, mode='base', seed=0)</code>: Train the model</li> <li><code>evaluate(model, data_loader)</code>: Evaluate the model</li> <li><code>predict(model, data_loader)</code>: Make predictions</li> </ul>"},{"location":"api-reference/trainers/#special-features","title":"Special Features","text":"<ol> <li>Support for Multiple Adaptive Loss Weighting Methods:</li> <li>UWL (Uncertainty Weighted Loss)</li> <li>MetaBalance</li> <li> <p>GradNorm</p> </li> <li> <p>Multi-task Early Stopping:</p> </li> <li>Early stopping based on specified task performance</li> <li> <p>Saves best model weights based on validation performance</p> </li> <li> <p>Support for Multiple Task Type Combinations:</p> </li> <li>Classification tasks</li> <li> <p>Regression tasks</p> </li> <li> <p>Training Log Recording:</p> </li> <li>Records loss for each task</li> <li>Records loss weights (when using adaptive methods)</li> <li>Records performance metrics on validation set</li> </ol>"},{"location":"api-reference/utils/","title":"Utilities API Reference","text":"<p>This document provides detailed API documentation for utility classes and functions in Torch-RecHub.</p>"},{"location":"api-reference/utils/#data-processing-tools-datapy","title":"Data Processing Tools (data.py)","text":""},{"location":"api-reference/utils/#dataset-classes","title":"Dataset Classes","text":""},{"location":"api-reference/utils/#torchdataset","title":"TorchDataset","text":"<ul> <li>Introduction: Basic implementation of PyTorch dataset for handling features and labels.</li> <li>Parameters:</li> <li><code>x</code> (dict): Feature dictionary, keys are feature names, values are feature data</li> <li><code>y</code> (array): Label data</li> </ul>"},{"location":"api-reference/utils/#predictdataset","title":"PredictDataset","text":"<ul> <li>Introduction: Dataset class for prediction phase, containing only feature data.</li> <li>Parameters:</li> <li><code>x</code> (dict): Feature dictionary, keys are feature names, values are feature data</li> </ul>"},{"location":"api-reference/utils/#matchdatagenerator","title":"MatchDataGenerator","text":"<ul> <li>Introduction: Data generator for recall tasks, used to generate training and testing data loaders.</li> <li>Main Methods:</li> <li><code>generate_dataloader(x_test_user, x_all_item, batch_size, num_workers=8)</code>: Generate training, testing, and item data loaders</li> <li>Parameters:<ul> <li><code>x_test_user</code> (dict): Test user features</li> <li><code>x_all_item</code> (dict): All item features</li> <li><code>batch_size</code> (int): Batch size</li> <li><code>num_workers</code> (int): Number of worker processes for data loading</li> </ul> </li> </ul>"},{"location":"api-reference/utils/#datagenerator","title":"DataGenerator","text":"<ul> <li>Introduction: General data generator supporting dataset splitting and loading.</li> <li>Main Methods:</li> <li><code>generate_dataloader(x_val=None, y_val=None, x_test=None, y_test=None, split_ratio=None, batch_size=16, num_workers=0)</code>: Generate training, validation, and test data loaders</li> <li>Parameters:<ul> <li><code>x_val</code>, <code>y_val</code>: Validation set features and labels</li> <li><code>x_test</code>, <code>y_test</code>: Test set features and labels</li> <li><code>split_ratio</code> (list): Split ratios for train, validation, and test sets</li> <li><code>batch_size</code> (int): Batch size</li> <li><code>num_workers</code> (int): Number of worker processes for data loading</li> </ul> </li> </ul>"},{"location":"api-reference/utils/#utility-functions","title":"Utility Functions","text":""},{"location":"api-reference/utils/#get_auto_embedding_dim","title":"get_auto_embedding_dim","text":"<ul> <li>Introduction: Automatically calculate embedding vector dimension based on number of categories.</li> <li>Parameters:</li> <li><code>num_classes</code> (int): Number of categories</li> <li>Returns:</li> <li>int: Embedding vector dimension, formula: <code>[6 * (num_classes)^(1/4)]</code></li> </ul>"},{"location":"api-reference/utils/#get_loss_func","title":"get_loss_func","text":"<ul> <li>Introduction: Get loss function.</li> <li>Parameters:</li> <li><code>task_type</code> (str): Task type, \"classification\" or \"regression\"</li> <li>Returns:</li> <li>torch.nn.Module: Corresponding loss function</li> </ul>"},{"location":"api-reference/utils/#get_metric_func","title":"get_metric_func","text":"<ul> <li>Introduction: Get evaluation metric function.</li> <li>Parameters:</li> <li><code>task_type</code> (str): Task type, \"classification\" or \"regression\"</li> <li>Returns:</li> <li>function: Corresponding evaluation metric function</li> </ul>"},{"location":"api-reference/utils/#generate_seq_feature","title":"generate_seq_feature","text":"<ul> <li>Introduction: Generate sequence features and negative samples.</li> <li>Parameters:</li> <li><code>data</code> (pd.DataFrame): Raw data</li> <li><code>user_col</code> (str): User ID column name</li> <li><code>item_col</code> (str): Item ID column name</li> <li><code>time_col</code> (str): Timestamp column name</li> <li><code>item_attribute_cols</code> (list): Item attribute columns for sequence feature generation</li> <li><code>min_item</code> (int): Minimum number of items per user</li> <li><code>shuffle</code> (bool): Whether to shuffle data</li> <li><code>max_len</code> (int): Maximum sequence length</li> </ul>"},{"location":"api-reference/utils/#recall-tools-matchpy","title":"Recall Tools (match.py)","text":""},{"location":"api-reference/utils/#data-processing-functions","title":"Data Processing Functions","text":""},{"location":"api-reference/utils/#gen_model_input","title":"gen_model_input","text":"<ul> <li>Introduction: Merge user and item features, process sequence features.</li> <li>Parameters:</li> <li><code>df</code> (pd.DataFrame): Data with history sequence features</li> <li><code>user_profile</code> (pd.DataFrame): User feature data</li> <li><code>user_col</code> (str): User column name</li> <li><code>item_profile</code> (pd.DataFrame): Item feature data</li> <li><code>item_col</code> (str): Item column name</li> <li><code>seq_max_len</code> (int): Maximum sequence length</li> <li><code>padding</code> (str): Padding method, 'pre' or 'post'</li> <li><code>truncating</code> (str): Truncating method, 'pre' or 'post'</li> </ul>"},{"location":"api-reference/utils/#negative_sample","title":"negative_sample","text":"<ul> <li>Introduction: Negative sampling method for recall models.</li> <li>Parameters:</li> <li><code>items_cnt_order</code> (dict): Item count dictionary, sorted by count in descending order</li> <li><code>ratio</code> (int): Negative sample ratio</li> <li><code>method_id</code> (int): Sampling method ID<ul> <li>0: Random sampling</li> <li>1: Word2Vec-style popularity sampling</li> <li>2: Log popularity sampling</li> <li>3: Tencent RALM sampling</li> </ul> </li> </ul>"},{"location":"api-reference/utils/#vector-retrieval-classes","title":"Vector Retrieval Classes","text":""},{"location":"api-reference/utils/#annoy","title":"Annoy","text":"<ul> <li>Introduction: Vector recall tool based on Annoy.</li> <li>Parameters:</li> <li><code>metric</code> (str): Distance metric method</li> <li><code>n_trees</code> (int): Number of trees</li> <li><code>search_k</code> (int): Search parameter</li> <li>Main Methods:</li> <li><code>fit(X)</code>: Build index</li> <li><code>query(v, n)</code>: Query nearest neighbors</li> </ul>"},{"location":"api-reference/utils/#milvus","title":"Milvus","text":"<ul> <li>Introduction: Vector recall tool based on Milvus.</li> <li>Parameters:</li> <li><code>dim</code> (int): Vector dimension</li> <li><code>host</code> (str): Milvus server address</li> <li><code>port</code> (str): Milvus server port</li> <li>Main Methods:</li> <li><code>fit(X)</code>: Build index</li> <li><code>query(v, n)</code>: Query nearest neighbors</li> </ul>"},{"location":"api-reference/utils/#multi-task-learning-tools-mtlpy","title":"Multi-task Learning Tools (mtl.py)","text":""},{"location":"api-reference/utils/#utility-functions_1","title":"Utility Functions","text":""},{"location":"api-reference/utils/#shared_task_layers","title":"shared_task_layers","text":"<ul> <li>Introduction: Get shared layer and task-specific layer parameters in multi-task models.</li> <li>Parameters:</li> <li><code>model</code> (torch.nn.Module): Multi-task model, supports MMOE, SharedBottom, PLE, AITM</li> <li>Returns:</li> <li>list: Shared layer parameter list</li> <li>list: Task-specific layer parameter list</li> </ul>"},{"location":"api-reference/utils/#optimizer-classes","title":"Optimizer Classes","text":""},{"location":"api-reference/utils/#metabalance","title":"MetaBalance","text":"<ul> <li>Introduction: MetaBalance optimizer for balancing gradients in multi-task learning.</li> <li>Parameters:</li> <li><code>parameters</code> (list): Model parameters</li> <li><code>relax_factor</code> (float): Relaxation factor for gradient scaling, default 0.7</li> <li><code>beta</code> (float): Moving average coefficient, default 0.9</li> <li>Main Methods:</li> <li><code>step(losses)</code>: Execute optimization step, update parameters</li> </ul>"},{"location":"api-reference/utils/#gradient-processing-functions","title":"Gradient Processing Functions","text":""},{"location":"api-reference/utils/#gradnorm","title":"gradnorm","text":"<ul> <li>Introduction: Implement GradNorm algorithm for dynamically adjusting task weights in multi-task learning.</li> <li>Parameters:</li> <li><code>loss_list</code> (list): List of task losses</li> <li><code>loss_weight</code> (list): List of task weights</li> <li><code>share_layer</code> (torch.nn.Parameter): Shared layer parameters</li> <li><code>initial_task_loss</code> (list): List of initial task losses</li> <li><code>alpha</code> (float): GradNorm algorithm hyperparameter</li> </ul>"},{"location":"blog/match/","title":"Match Blog","text":""},{"location":"blog/match/#i-understanding-different-loss-functions-3-training-methods","title":"I. Understanding Different Loss Functions \u2014 3 Training Methods","text":"<p>In recall tasks, there are generally three training methods: point-wise, pair-wise, and list-wise. In RecHub, we use the mode parameter to specify the training method, with each method corresponding to a different loss function.</p>"},{"location":"blog/match/#11-point-wise-mode-0","title":"1.1 Point-wise (mode = 0)","text":"<p>\ud83e\udd70Core Idea: Treat recall as binary classification.</p> <p>For a recall model, the input is a tuple \\, and the output is \\(P(User, Item)\\), representing the user's interest level in the item. <p>Training objective: For positive samples, the output should be as close to 1 as possible; for negative samples, as close to 0 as possible.</p> <p>The most commonly used loss function is BCELoss (Binary Cross Entropy Loss).</p>"},{"location":"blog/match/#12-pair-wise-mode-1","title":"1.2 Pair-wise (mode = 1)","text":"<p>\ud83d\ude1dCore Idea: A user's interest in positive samples should be higher than in negative samples.</p> <p>For a recall model, the input is a triple \\&lt;User, ItemPositive, ItemNegative&gt;, outputting interest scores \\(P(User, ItemPositive)\\) and \\(P(User, ItemNegative)\\), representing the user's interest scores for positive and negative item samples.</p> <p>Training objective: The interest score for positive samples should be higher than that for negative samples.</p> <p>The framework uses BPRLoss (Bayes Personalized Ranking Loss). Here's the loss formula (for more details, see here - note that there are slight differences between the linked content and the formula below, but the core idea remains the same):</p> \\[ Loss=\\frac{1}{N}\\sum^N\\ _{i=1}-log(sigmoid(pos\\_score - neg\\_score)) \\]"},{"location":"blog/match/#13-list-wise-mode-2","title":"1.3 List-wise (mode = 2)","text":"<p>\ud83d\ude07Core Idea: A user's interest in positive samples should be higher than in negative samples.</p> <p>Wait, isn't this the same as Pair-wise?</p> <p>Yes! The core idea of List-wise training is the same as Pair-wise, but the implementation differs.</p> <p>For a recall model, the input is an N+2 tuple \\(&lt;User, ItemPositive, ItemNeg\\_1, ... , ItemNeg\\_N&gt;\\), outputting interest scores for 1 positive sample and N negative samples.</p> <p>Training objective: The interest score for the positive sample should be higher than all negative samples.</p> <p>The framework uses \\(torch.nn.CrossEntropyLoss\\), applying Softmax to the outputs.</p> <p>PS: This List-wise approach can be easily confused with List-wise in Ranking. Although they share the same name, List-wise in ranking considers the order relationship between samples. For example, ranking uses order-sensitive metrics like MAP and NDCG for evaluation, while List-wise in Matching doesn't consider order.</p>"},{"location":"blog/match/#ii-how-far-apart-are-two-vectors-3-similarity-metrics","title":"II. How Far Apart Are Two Vectors? \u2014 3 Similarity Metrics","text":"<p>\ud83e\udd14Given a user vector and an item vector, how do we measure their similarity?</p> <p>Let's first define user vector \\(user \\in \\mathcal R^D\\) and item vector \\(item\\in \\mathcal R^D\\), where D represents their dimension.</p>"},{"location":"blog/match/#21-cosine","title":"2.1 Cosine","text":"<p>From middle school math:</p> \\[ cos(a,b)=\\frac{&lt;a,b&gt;}{|a|*|b|} \\] <p>This represents the angle between two vectors, outputting a real number between [-1, 1]. We can use this as a similarity measure: the smaller the angle between vectors, the more similar they are.</p> <p>In all two-tower models in RecHub, cosine similarity is used during the training phase.</p>"},{"location":"blog/match/#22-dot-product","title":"2.2 Dot Product","text":"<p>This is the inner product of vectors, denoted as \\(&lt;a,b&gt;\\) for vectors a and b.</p> <p>A simple insight: If we L2 normalize vectors a and b, i.e., \\(\\tilde{a}=\\frac{a}{|a|}\\ ,\\tilde{b}=\\frac{b}{|b|}\\), then computing their dot product is equivalent to \\(cos(a,b)\\). (This is straightforward, so we'll skip the proof)</p> <p>In fact, this is exactly how all two-tower models in RecHub work: first computing User Embedding and Item Embedding, then applying L2 Norm to each, and finally computing their dot product to get cosine similarity. This approach improves model validation and inference speed.</p>"},{"location":"blog/match/#23-euclidean-distance","title":"2.3 Euclidean Distance","text":"<p>Euclidean distance is what we commonly understand as \"distance\" in everyday life.</p> <p>\ud83d\ude4bFor L2 normalized vectors a and b, maximizing their cosine similarity is equivalent to minimizing their Euclidean distance</p> <p>Why? See the formula below:</p> \\[ \\begin{align*}   EuclidianDistance(a,b)^2 &amp;= \\sum_{i=1}^N(a_i-b_i)^2 \\\\     &amp;= \\sum_{i=1}^Na_i^2+\\sum_{i=1}^Nb_i^2-\\sum_{i=1}^N2*a_i*b_i\\\\     &amp;= 2-2*\\sum_{i=1}^Na_i*b_i \\\\     &amp;= 2*(1-cos(a,b)) \\end{align*} \\] <p>Two points to explain:</p> <ol> <li>From second line to third line, \\(\\sum\\ _{i=1}^N a\\_i^2=1\\). Why? Because a is L2 normalized. Same for b.</li> <li>From third line to fourth line, \\(\\sum_{i=1}^Na_i*b_i\\) is the dot product of vectors a and b; since they're L2 normalized, this equals cos.</li> </ol> <p>In RecHub, we use Annoy's Euclidean distance during the validation phase.</p> <p>\ud83d\ude4bSummary: For L2 normalized vectors, maximizing dot product is equivalent to maximizing cosine similarity is equivalent to minimizing Euclidean distance</p>"},{"location":"blog/match/#iii-how-hot-is-the-temperature","title":"III. How Hot is the Temperature?","text":"<p>Before proceeding, please make sure you understand the operations in torch.nn.CrossEntropyLoss (LogSoftmax + NLLLoss). This is crucial for understanding the source code.</p> <p>Consider a scenario: Using List-wise training with 1 positive sample and 3 negative samples, with cosine similarity as the training metric.</p> <p>Suppose our model perfectly predicts a training sample, outputting logits (1, -1, -1, -1). Theoretically, the Loss should be 0, or at least very small. However, with CrossEntropyLoss, we get:</p> \\[ -log(exp(1)/(exp(1)+exp(-1)*3))=0.341 \\] <p>But if we divide the logits by a temperature coefficient \\(temperature=0.2\\), making them (5, -5, -5, -5), after CrossEntropyLoss, we get:</p> \\[ -log(exp(5)/(exp(5)+exp(-5)*3))=0.016 \\] <p>This gives us a negligibly small Loss.</p> <p>In other words, dividing logits by a temperature expands the upper and lower bounds of each element in the logits, bringing them back into the sensitive range of softmax operations.</p> <p>In practice, L2 Norm is commonly used together with temperature scaling.</p>"},{"location":"tutorials/matching/","title":"Recall Model Tutorial","text":"<p>This tutorial will introduce how to use various recall models in Torch-RecHub. We'll use the MovieLens dataset as an example.</p>"},{"location":"tutorials/matching/#data-preparation","title":"Data Preparation","text":"<p>First, we need to prepare the data. The MovieLens dataset contains user ratings for movies:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom rechub.utils import DataGenerator\nfrom rechub.models import *\nfrom rechub.trainers import *\n\n# Load data\ndf = pd.read_csv(\"movielens.csv\")\nuser_features = ['user_id', 'age', 'gender', 'occupation']\nitem_features = ['movie_id', 'genre', 'year']\n</code></pre>"},{"location":"tutorials/matching/#basic-two-tower-model-dssm","title":"Basic Two-Tower Model (DSSM)","text":"<p>DSSM is the most basic two-tower model, modeling users and items separately:</p> <pre><code># Model configuration\nmodel = DSSM(user_features=user_features,\n             item_features=item_features,\n             hidden_units=[64, 32, 16],\n             dropout_rates=[0.1, 0.1, 0.1])\n\n# Training configuration\ntrainer = MatchTrainer(model=model,\n                      mode=0,  # point-wise training\n                      optimizer_params={'lr': 0.001},\n                      n_epochs=10)\n\n# Train model\ntrainer.fit(train_dataloader, val_dataloader)\n</code></pre>"},{"location":"tutorials/matching/#sequential-recommendation-model-gru4rec","title":"Sequential Recommendation Model (GRU4Rec)","text":"<p>GRU4Rec models user behavior sequences through GRU networks:</p> <pre><code># Generate sequence features\nseq_features = generate_seq_feature(df,\n                                  user_col='user_id',\n                                  item_col='movie_id',\n                                  time_col='timestamp',\n                                  item_attribute_cols=['genre'])\n\n# Model configuration\nmodel = GRU4Rec(item_num=item_num,\n                hidden_size=64,\n                num_layers=2,\n                dropout_rate=0.1)\n\n# Training configuration\ntrainer = MatchTrainer(model=model,\n                      mode=1,  # pair-wise training\n                      optimizer_params={'lr': 0.001},\n                      n_epochs=10)\n</code></pre>"},{"location":"tutorials/matching/#multi-interest-model-mind","title":"Multi-Interest Model (MIND)","text":"<p>The MIND model can capture users' diverse interests:</p> <pre><code># Model configuration\nmodel = MIND(item_num=item_num,\n            num_interests=4,\n            hidden_size=64,\n            routing_iterations=3)\n\n# Training configuration\ntrainer = MatchTrainer(model=model,\n                      mode=2,  # list-wise training\n                      optimizer_params={'lr': 0.001},\n                      n_epochs=10)\n</code></pre>"},{"location":"tutorials/matching/#model-evaluation","title":"Model Evaluation","text":"<p>Use common recall metrics for evaluation:</p> <pre><code># Calculate recall rate and hit rate\nrecall_score = evaluate_recall(model, test_dataloader, k=10)\nhit_rate = evaluate_hit_rate(model, test_dataloader, k=10)\nprint(f\"Recall@10: {recall_score:.4f}\")\nprint(f\"HitRate@10: {hit_rate:.4f}\")\n</code></pre>"},{"location":"tutorials/matching/#vector-retrieval","title":"Vector Retrieval","text":"<p>The trained model can be used to generate vector representations of users and items for fast retrieval:</p> <pre><code># Use Annoy for vector retrieval\nfrom rechub.utils import Annoy\n\n# Build index\nitem_vectors = model.get_item_vectors()\nannoy = Annoy(metric='angular')\nannoy.fit(item_vectors)\n\n# Query similar items\nuser_vector = model.get_user_vector(user_id=1)\nsimilar_items = annoy.query(user_vector, n=10)\n</code></pre>"},{"location":"tutorials/matching/#advanced-techniques","title":"Advanced Techniques","text":"<ol> <li> <p>Temperature Coefficient Adjustment <pre><code>trainer = MatchTrainer(model=model,\n                      temperature=0.2,  # Add temperature coefficient\n                      mode=2)\n</code></pre></p> </li> <li> <p>Negative Sampling <pre><code>from rechub.utils import negative_sample\n\nneg_samples = negative_sample(items_cnt_order,\n                            ratio=5,\n                            method_id=1)  # Word2Vec-style sampling\n</code></pre></p> </li> <li> <p>Model Saving and Loading <pre><code># Save model\ntorch.save(model.state_dict(), 'model.pth')\n\n# Load model\nmodel.load_state_dict(torch.load('model.pth'))\n</code></pre></p> </li> </ol>"},{"location":"tutorials/matching/#important-notes","title":"Important Notes","text":"<ol> <li>Choose appropriate training mode (point-wise/pair-wise/list-wise)</li> <li>Pay attention to sequence feature length and padding method</li> <li>Adjust negative sample ratio based on actual scenarios</li> <li>Set appropriate batch_size and learning rate</li> <li>Use L2 regularization to prevent overfitting</li> </ol>"},{"location":"tutorials/multi-task/","title":"Multi-Task Learning Tutorial","text":"<p>This tutorial will introduce how to use multi-task learning models in Torch-RecHub. We'll use Alibaba's e-commerce dataset as an example.</p>"},{"location":"tutorials/multi-task/#data-preparation","title":"Data Preparation","text":"<p>First, we need to prepare data for multi-task learning:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom rechub.utils import DataGenerator\nfrom rechub.models import *\nfrom rechub.trainers import *\n\n# Load data\ndf = pd.read_csv(\"ali_ccp_data.csv\")\n\n# Feature definitions\nuser_features = ['user_id', 'age', 'gender', 'occupation']\nitem_features = ['item_id', 'category_id', 'shop_id', 'brand_id']\nfeatures = user_features + item_features\n\n# Multi-task labels\ntasks = ['click', 'conversion']  # CTR and CVR tasks\n</code></pre>"},{"location":"tutorials/multi-task/#sharedbottom-model","title":"SharedBottom Model","text":"<p>The most basic multi-task learning model with shared parameters in bottom layers:</p> <pre><code># Model configuration\nmodel = SharedBottom(\n    features=features,\n    hidden_units=[256, 128],\n    task_hidden_units=[64, 32],\n    num_tasks=2,\n    task_types=['binary', 'binary'])\n\n# Training configuration\ntrainer = MTLTrainer(\n    model=model,\n    optimizer_params={'lr': 0.001},\n    n_epochs=10)\n\n# Train model\ntrainer.fit(train_dataloader, val_dataloader)\n</code></pre>"},{"location":"tutorials/multi-task/#esmm-entire-space-multi-task-model","title":"ESMM (Entire Space Multi-Task Model)","text":"<p>A multi-task model that addresses sample selection bias:</p> <pre><code># Model configuration\nmodel = ESMM(\n    features=features,\n    hidden_units=[256, 128, 64],\n    tower_units=[32, 16],\n    embedding_dim=16)\n\n# Training configuration\ntrainer = MTLTrainer(\n    model=model,\n    optimizer_params={'lr': 0.001},\n    n_epochs=10)\n</code></pre>"},{"location":"tutorials/multi-task/#mmoe-multi-gate-mixture-of-experts","title":"MMoE (Multi-gate Mixture-of-Experts)","text":"<p>Implements soft parameter sharing between tasks through expert mechanism:</p> <pre><code># Model configuration\nmodel = MMoE(\n    features=features,\n    expert_units=[256, 128],\n    num_experts=8,\n    num_tasks=2,\n    expert_activation='relu',\n    gate_activation='softmax')\n\n# Training configuration\ntrainer = MTLTrainer(\n    model=model,\n    optimizer_params={'lr': 0.001},\n    n_epochs=10)\n</code></pre>"},{"location":"tutorials/multi-task/#ple-progressive-layered-extraction","title":"PLE (Progressive Layered Extraction)","text":"<p>Better models task relationships through layered extraction:</p> <pre><code># Model configuration\nmodel = PLE(\n    features=features,\n    expert_units=[256, 128],\n    num_experts=4,\n    num_layers=3,\n    num_shared_experts=2,\n    task_types=['binary', 'binary'])\n\n# Training configuration\ntrainer = MTLTrainer(\n    model=model,\n    optimizer_params={'lr': 0.001},\n    n_epochs=10)\n</code></pre>"},{"location":"tutorials/multi-task/#task-weight-optimization","title":"Task Weight Optimization","text":""},{"location":"tutorials/multi-task/#gradnorm","title":"GradNorm","text":"<p>Use GradNorm algorithm to dynamically adjust task weights:</p> <pre><code># Configure GradNorm\ntrainer = MTLTrainer(\n    model=model,\n    optimizer_params={'lr': 0.001},\n    task_weights_strategy='gradnorm',\n    gradnorm_alpha=1.5)\n</code></pre>"},{"location":"tutorials/multi-task/#metabalance","title":"MetaBalance","text":"<p>Use MetaBalance optimizer to balance task gradients:</p> <pre><code>from rechub.utils import MetaBalance\n\n# Configure MetaBalance optimizer\noptimizer = MetaBalance(\n    model.parameters(),\n    relax_factor=0.7,\n    beta=0.9)\n\ntrainer = MTLTrainer(\n    model=model,\n    optimizer=optimizer)\n</code></pre>"},{"location":"tutorials/multi-task/#model-evaluation","title":"Model Evaluation","text":"<p>Use appropriate evaluation metrics for different tasks:</p> <pre><code># Evaluate model\nresults = evaluate_multi_task(model, test_dataloader)\nfor task, metrics in results.items():\n    print(f\"Task: {task}\")\n    print(f\"AUC: {metrics['auc']:.4f}\")\n    print(f\"LogLoss: {metrics['logloss']:.4f}\")\n</code></pre>"},{"location":"tutorials/multi-task/#advanced-applications","title":"Advanced Applications","text":"<ol> <li> <p>Custom Task Loss Weights <pre><code>trainer = MTLTrainer(\n    model=model,\n    task_weights=[1.0, 0.5])  # Set fixed task weights\n</code></pre></p> </li> <li> <p>Get Shared and Task-Specific Layers <pre><code>from rechub.utils import shared_task_layers\n\nshared_params, task_params = shared_task_layers(model)\n</code></pre></p> </li> <li> <p>Task-Specific Learning Rates <pre><code>trainer = MTLTrainer(\n    model=model,\n    task_specific_lr={'click': 0.001, 'conversion': 0.0005})\n</code></pre></p> </li> </ol>"},{"location":"tutorials/multi-task/#important-notes","title":"Important Notes","text":"<ol> <li>Choose appropriate multi-task learning architecture</li> <li>Pay attention to task correlations</li> <li>Handle data imbalance between tasks</li> <li>Set reasonable task weights</li> <li>Monitor training progress for each task</li> <li>Prevent negative transfer between tasks</li> <li>Consider computational resource constraints</li> </ol>"},{"location":"tutorials/ranking/","title":"Ranking Model Tutorial","text":"<p>This tutorial will introduce how to use various ranking models in Torch-RecHub. We'll use the Criteo and Avazu datasets as examples.</p>"},{"location":"tutorials/ranking/#data-preparation","title":"Data Preparation","text":"<p>First, we need to prepare the data and process features:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom rechub.utils import DataGenerator\nfrom rechub.models import *\nfrom rechub.trainers import *\n\n# Load data\ndf = pd.read_csv(\"criteo_sample.csv\")\n\n# Feature column definitions\nsparse_features = ['C1', 'C2', 'C3', ..., 'C26']\ndense_features = ['I1', 'I2', 'I3', ..., 'I13']\nfeatures = sparse_features + dense_features\n</code></pre>"},{"location":"tutorials/ranking/#wide-deep-model","title":"Wide &amp; Deep Model","text":"<p>Wide &amp; Deep model combines memorization and generalization capabilities:</p> <pre><code># Model configuration\nmodel = WideDeep(\n    wide_features=sparse_features,\n    deep_features=features,\n    hidden_units=[256, 128, 64],\n    dropout_rates=[0.1, 0.1, 0.1])\n\n# Training configuration\ntrainer = Trainer(model=model,\n                 optimizer_params={'lr': 0.001},\n                 n_epochs=10,\n                 device='cuda:0')\n\n# Train model\ntrainer.fit(train_dataloader, val_dataloader)\n</code></pre>"},{"location":"tutorials/ranking/#deepfm-model","title":"DeepFM Model","text":"<p>DeepFM model uses factorization machines and deep networks to model feature interactions:</p> <pre><code># Model configuration\nmodel = DeepFM(\n    features=features,\n    hidden_units=[256, 128, 64],\n    dropout_rates=[0.1, 0.1, 0.1],\n    embedding_dim=16)\n\n# Training configuration\ntrainer = Trainer(model=model,\n                 optimizer_params={'lr': 0.001},\n                 n_epochs=10)\n</code></pre>"},{"location":"tutorials/ranking/#din-deep-interest-network","title":"DIN (Deep Interest Network)","text":"<p>DIN model uses attention mechanism to model user interests:</p> <pre><code># Generate behavior sequence features\nbehavior_features = ['item_id', 'category_id']\nseq_features = generate_seq_feature(df,\n                                  user_col='user_id',\n                                  item_col='item_id',\n                                  time_col='timestamp',\n                                  item_attribute_cols=['category_id'])\n\n# Model configuration\nmodel = DIN(\n    features=features,\n    behavior_features=behavior_features,\n    attention_units=[80, 40],\n    hidden_units=[256, 128, 64],\n    dropout_rate=0.1)\n\n# Training configuration\ntrainer = Trainer(model=model,\n                 optimizer_params={'lr': 0.001},\n                 n_epochs=10)\n</code></pre>"},{"location":"tutorials/ranking/#dcn-v2-model","title":"DCN-V2 Model","text":"<p>DCN-V2 explicitly models feature interactions through cross network:</p> <pre><code># Model configuration\nmodel = DCNV2(\n    features=features,\n    cross_num=3,\n    hidden_units=[256, 128, 64],\n    dropout_rates=[0.1, 0.1, 0.1],\n    cross_parameterization='matrix')  # or 'vector'\n\n# Training configuration\ntrainer = Trainer(model=model,\n                 optimizer_params={'lr': 0.001},\n                 n_epochs=10)\n</code></pre>"},{"location":"tutorials/ranking/#model-evaluation","title":"Model Evaluation","text":"<p>Use common ranking metrics for evaluation:</p> <pre><code># Evaluate model\nauc = evaluate_auc(model, test_dataloader)\nlog_loss = evaluate_logloss(model, test_dataloader)\nprint(f\"AUC: {auc:.4f}\")\nprint(f\"LogLoss: {log_loss:.4f}\")\n</code></pre>"},{"location":"tutorials/ranking/#feature-engineering-tips","title":"Feature Engineering Tips","text":"<ol> <li> <p>Feature Preprocessing <pre><code># Categorical feature encoding\nfrom sklearn.preprocessing import LabelEncoder\nfor feat in sparse_features:\n    lbe = LabelEncoder()\n    df[feat] = lbe.fit_transform(df[feat])\n\n# Numerical feature normalization\nfrom sklearn.preprocessing import MinMaxScaler\nfor feat in dense_features:\n    scaler = MinMaxScaler()\n    df[feat] = scaler.fit_transform(df[feat].values.reshape(-1, 1))\n</code></pre></p> </li> <li> <p>Feature Crossing <pre><code># Manual feature crossing\ndf['cross_feat'] = df['feat1'].astype(str) + '_' + df['feat2'].astype(str)\n</code></pre></p> </li> </ol>"},{"location":"tutorials/ranking/#advanced-applications","title":"Advanced Applications","text":"<ol> <li> <p>Custom Loss Function <pre><code>class FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, pred, target):\n        # Implement Focal Loss\n        pass\n\ntrainer = Trainer(model=model,\n                 loss_fn=FocalLoss(alpha=0.25, gamma=2))\n</code></pre></p> </li> <li> <p>Learning Rate Scheduling <pre><code>from torch.optim.lr_scheduler import CosineAnnealingLR\n\ntrainer = Trainer(model=model,\n                 scheduler='cosine',  # Use cosine annealing scheduler\n                 scheduler_params={'T_max': 10})\n</code></pre></p> </li> </ol>"},{"location":"tutorials/ranking/#important-notes","title":"Important Notes","text":"<ol> <li>Handle missing values and outliers appropriately</li> <li>Pay attention to feature engineering importance</li> <li>Choose appropriate evaluation metrics</li> <li>Focus on model interpretability</li> <li>Balance model complexity and efficiency</li> <li>Handle class imbalance issues</li> </ol>"},{"location":"zh/","title":"\u6b22\u8fce\u4f7f\u7528 Torch-RecHub","text":"Torch-RecHub <p>\u4e00\u4e2a\u57fa\u4e8e PyTorch \u7684\u6613\u7528\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u6027\u80fd\u7684\u63a8\u8350\u7cfb\u7edf\u6846\u67b6</p>            \u5feb\u901f\u5f00\u59cb                     \u67e5\u770b GitHub"},{"location":"zh/contributing/","title":"\u8d21\u732e\u6307\u5357","text":"<p>\u975e\u5e38\u611f\u8c22\u60a8\u5bf9 Torch-RecHub \u9879\u76ee\u7684\u5174\u8da3\u5e76\u8003\u8651\u4e3a\u5176\u505a\u51fa\u8d21\u732e\uff01\u60a8\u7684\u5e2e\u52a9\u5bf9\u9879\u76ee\u7684\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002\u672c\u6307\u5357\u5c06\u8be6\u7ec6\u4ecb\u7ecd\u5982\u4f55\u53c2\u4e0e\u8d21\u732e\u3002</p>"},{"location":"zh/contributing/#_2","title":"\u5982\u4f55\u8d21\u732e","text":"<p>\u6211\u4eec\u9f13\u52b1\u5404\u79cd\u5f62\u5f0f\u7684\u8d21\u732e\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\uff1a</p> <ul> <li>\u62a5\u544a Bug</li> <li>\u63d0\u4ea4\u529f\u80fd\u8bf7\u6c42</li> <li>\u7f16\u5199\u6216\u6539\u8fdb\u6587\u6863</li> <li>\u63d0\u4ea4\u4ee3\u7801\u4fee\u590d\u6216\u65b0\u529f\u80fd\uff08\u901a\u8fc7 Pull Request\uff09</li> </ul>"},{"location":"zh/contributing/#_3","title":"\u5bfb\u627e\u8d21\u732e\u70b9","text":"<p>\u60a8\u53ef\u4ee5\u4ece\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\u5165\u624b\uff1a</p> <ol> <li>\u67e5\u770b Issues\uff1a\u6d4f\u89c8 \u9879\u76ee Issues \u5217\u8868\uff0c\u5bfb\u627e\u6807\u8bb0\u4e3a <code>help wanted</code> \u7684\u95ee\u9898\u3002</li> <li>\u6539\u8fdb\u73b0\u6709\u529f\u80fd\uff1a\u5982\u679c\u60a8\u5728\u4f7f\u7528\u4e2d\u53d1\u73b0\u53ef\u4ee5\u4f18\u5316\u7684\u5730\u65b9\uff0c\u6b22\u8fce\u63d0\u51fa\u5efa\u8bae\u6216\u76f4\u63a5\u63d0\u4ea4\u6539\u8fdb\u3002</li> <li>\u5b9e\u73b0\u65b0\u529f\u80fd\uff1a\u5982\u679c\u60a8\u6709\u65b0\u7684\u60f3\u6cd5\uff0c\u5efa\u8bae\u5148\u521b\u5efa\u4e00\u4e2a Issue \u8fdb\u884c\u8ba8\u8bba\uff0c\u4ee5\u786e\u4fdd\u5176\u7b26\u5408\u9879\u76ee\u65b9\u5411\u3002</li> </ol>"},{"location":"zh/contributing/#_4","title":"\u8d21\u732e\u6d41\u7a0b\uff08\u4ee3\u7801\u548c\u6587\u6863\uff09","text":"<p>\u6211\u4eec\u4f7f\u7528\u6807\u51c6\u7684 GitHub Fork &amp; Pull Request \u6d41\u7a0b\u6765\u63a5\u53d7\u4ee3\u7801\u548c\u6587\u6863\u7684\u8d21\u732e\u3002(\u4e0b\u5217\u64cd\u4f5c\u60a8\u4e5f\u53ef\u4ee5\u5728Github\u7f51\u9875\u4e0a\u8fdb\u884c)</p> <ol> <li> <p>Fork \u4ed3\u5e93     \u8bbf\u95ee Torch-RecHub \u7684 GitHub \u4ed3\u5e93\u9875\u9762 \uff0c\u70b9\u51fb\u53f3\u4e0a\u89d2\u7684 \"Fork\" \u6309\u94ae\uff0c\u5c06\u9879\u76ee\u590d\u5236\u5230\u60a8\u81ea\u5df1\u7684 GitHub \u8d26\u6237\u4e0b\u3002</p> </li> <li> <p>Clone \u60a8\u7684 Fork     \u5c06\u60a8 Fork \u7684\u4ed3\u5e93\u514b\u9686\u5230\u672c\u5730\uff1a     <pre><code>git clone https://github.com/YOUR_USERNAME/torch-rechub.git\ncd torch-rechub\n</code></pre>     \u8bf7\u5c06 <code>YOUR_USERNAME</code> \u66ff\u6362\u4e3a\u60a8\u7684 GitHub \u7528\u6237\u540d\u3002</p> </li> <li> <p>\u8bbe\u7f6e\u4e0a\u6e38\u4ed3\u5e93 (Optional but Recommended)     \u6dfb\u52a0\u539f\u59cb\u9879\u76ee\u4ed3\u5e93\u4f5c\u4e3a\u4e0a\u6e38\u8fdc\u7a0b\u4ed3\u5e93\uff0c\u65b9\u4fbf\u540c\u6b65\u66f4\u65b0\uff1a     <pre><code>git remote add upstream https://github.com/datawhalechina/torch-rechub.git\n</code></pre></p> </li> <li> <p>\u8fdb\u884c\u4fee\u6539     \u5728\u60a8\u7684\u65b0\u4ed3\u5e93\u4e0a\u8fdb\u884c\u4ee3\u7801\u7f16\u5199\u3001\u6587\u6863\u4fee\u6539\u6216\u5176\u4ed6\u6539\u8fdb\u3002</p> </li> <li> <p>\u786e\u4fdd\u4ee3\u7801\u8d28\u91cf</p> <ul> <li>\u4ee3\u7801\u98ce\u683c\uff1a\u8bf7\u9075\u5faa\u9879\u76ee\u73b0\u6709\u7684\u4ee3\u7801\u98ce\u683c\u3002</li> <li>\u6587\u6863\uff1a\u5bf9\u4e8e\u7528\u6237\u53ef\u89c1\u7684\u66f4\u6539\uff08\u5982\u65b0\u529f\u80fd\u3001API \u53d8\u52a8\uff09\uff0c\u8bf7\u66f4\u65b0\u76f8\u5173\u6587\u6863\uff08README\u3001<code>docs/</code> \u76ee\u5f55\u4e0b\u7684\u6587\u4ef6\u7b49\uff09\u3002</li> </ul> </li> <li> <p>\u63d0\u4ea4\u66f4\u6539     \u4f7f\u7528\u6e05\u6670\u3001\u6709\u610f\u4e49\u7684 Commit Message \u63d0\u4ea4\u60a8\u7684\u66f4\u6539\u3002\u6211\u4eec\u63a8\u8350\u9075\u5faa Conventional Commits \u89c4\u8303\u3002     <pre><code>git add .\ngit commit -m \"feat: \u6dfb\u52a0 XXX \u6a21\u578b\u652f\u6301\"\n# \u6216\u8005\ngit commit -m \"fix: \u4fee\u590d README \u4e2d\u7684\u62fc\u5199\u9519\u8bef\"\n# \u6216\u8005\ngit commit -m \"docs: \u66f4\u65b0\u8d21\u732e\u6307\u5357\"\n</code></pre></p> </li> <li> <p>\u63a8\u9001\u5206\u652f     \u5c06\u60a8\u7684\u672c\u5730\u4ee3\u7801\u63a8\u9001\u5230\u60a8 Fork \u7684 GitHub \u4ed3\u5e93\uff1a     <pre><code>git push origin main\n</code></pre></p> </li> <li> <p>\u521b\u5efa Pull Request (PR)     \u8fd4\u56de\u60a8\u5728 GitHub \u4e0a\u7684 Fork \u4ed3\u5e93\u9875\u9762\uff0c\u60a8\u4f1a\u770b\u5230\u4e00\u4e2a\u63d0\u793a\uff0c\u5efa\u8bae\u60a8\u57fa\u4e8e\u65b0\u63a8\u9001\u7684\u5206\u652f\u521b\u5efa\u4e00\u4e2a Pull Request\u3002\u70b9\u51fb\u8be5\u63d0\u793a\u6216\u624b\u52a8\u5bfc\u822a\u5230 \"Pull requests\" \u6807\u7b7e\u9875\uff0c\u70b9\u51fb \"New pull request\"\u3002</p> <ul> <li>\u9009\u62e9\u5206\u652f\uff1a\u786e\u4fdd\u57fa\u7840\u4ed3\u5e93 (Base repository) \u662f <code>datawhale/torch-rechub</code> \u7684 <code>main</code> \u5206\u652f\uff0c\u5934\u90e8\u4ed3\u5e93 (Head repository) \u662f\u60a8 Fork \u7684\u4ed3\u5e93\u4ee5\u53ca\u60a8\u521a\u521a\u63a8\u9001\u7684\u4ee3\u7801\u3002</li> <li>\u586b\u5199 PR \u4fe1\u606f\uff1a<ul> <li>\u6807\u9898\uff1a\u7b80\u660e\u627c\u8981\u5730\u63cf\u8ff0 PR \u7684\u76ee\u7684\uff0c\u901a\u5e38\u53ef\u4ee5\u57fa\u4e8e Commit Message\u3002</li> <li>\u63cf\u8ff0\uff1a\u8be6\u7ec6\u8bf4\u660e\u60a8\u6240\u505a\u7684\u66f4\u6539\u3001\u89e3\u51b3\u7684\u95ee\u9898\uff08\u53ef\u4ee5\u94fe\u63a5\u76f8\u5173\u7684 Issue\uff0c\u4f8b\u5982 <code>Closes #123</code>\uff09\u3001\u4ee5\u53ca\u4efb\u4f55\u9700\u8981\u8bc4\u5ba1\u8005\u6ce8\u610f\u7684\u4e8b\u9879\u3002\u5982\u679c PR \u5305\u542b UI \u66f4\u6539\uff0c\u8bf7\u9644\u4e0a\u622a\u56fe\u6216\u5f55\u5c4f\u3002</li> </ul> </li> <li>\u5141\u8bb8\u7ef4\u62a4\u8005\u4fee\u6539 (\u53ef\u9009)\uff1a\u52fe\u9009 \"Allow edits by maintainers\" \u901a\u5e38\u6709\u52a9\u4e8e\u7ef4\u62a4\u8005\u5feb\u901f\u4fee\u590d\u5c0f\u95ee\u9898\u3002</li> <li>\u63d0\u4ea4 PR\uff1a\u70b9\u51fb \"Create pull request\"\u3002</li> </ul> </li> </ol>"},{"location":"zh/contributing/#pull-request","title":"Pull Request \u8bc4\u5ba1","text":"<ul> <li>\u63d0\u4ea4 PR \u540e\uff0c\u9879\u76ee\u7684 CI/CD \u6d41\u7a0b\u4f1a\u81ea\u52a8\u8fd0\u884c\u6d4b\u8bd5\u548c\u68c0\u67e5\u3002</li> <li>\u9879\u76ee\u7ef4\u62a4\u8005\u4f1a\u8bc4\u5ba1\u60a8\u7684\u4ee3\u7801\u548c\u6587\u6863\uff0c\u5e76\u53ef\u80fd\u63d0\u51fa\u4fee\u6539\u610f\u89c1\u3002</li> <li>\u8bf7\u53ca\u65f6\u56de\u5e94\u8bc4\u5ba1\u610f\u89c1\u5e76\u8fdb\u884c\u5fc5\u8981\u7684\u4fee\u6539\u3002\u7ef4\u62a4\u8005\u53ef\u80fd\u4f1a\u76f4\u63a5\u5728\u60a8\u7684\u5206\u652f\u4e0a\u8fdb\u884c\u5c0f\u7684\u4fee\u8ba2\uff08\u5982\u679c\u60a8\u5141\u8bb8\u7684\u8bdd\uff09\uff0c\u6216\u8005\u60a8\u9700\u8981\u81ea\u5df1\u66f4\u65b0\u4ee3\u7801\u5e76\u91cd\u65b0\u63a8\u9001\u3002</li> <li>\u4e00\u65e6 PR \u88ab\u6279\u51c6\u5e76\u901a\u8fc7\u6240\u6709\u68c0\u67e5\uff0c\u7ef4\u62a4\u8005\u4f1a\u5c06\u5176\u5408\u5e76\u5230\u4e3b\u5206\u652f\u4e2d\u3002</li> </ul> <p>\u518d\u6b21\u611f\u8c22\u60a8\u5bf9 Torch-RecHub \u7684\u8d21\u732e\uff01</p>"},{"location":"zh/faq/","title":"\u5e38\u89c1\u95ee\u9898\u89e3\u7b54","text":"<p>Torch-RecHub \u5e38\u89c1\u95ee\u9898\u53ca\u6545\u969c\u6392\u9664\u6307\u5357</p> <ul> <li> <p>\u4f1a\u63a8\u51fa tensorflow \u7248\u672c\u5417\uff1f</p> <ul> <li> <p>\u6682\u4e0d\u8003\u8651</p> </li> <li> <p>\u672c\u9879\u76ee\u6838\u5fc3\u5b9a\u4f4d\u662f\u9762\u5411\u521d\u5b66\u8005\u63d0\u4f9b\u5bb9\u6613\u4e0a\u624b\u7684\u3001\u4e1a\u754c\u4f7f\u7528\u7684\u6a21\u578b\u590d\u73b0\u53c2\u8003\uff0cpytorch \u53d7\u4f17\u9762\u8f83\u5e7f</p> </li> </ul> </li> <li> <p>\u4e3a\u4ec0\u4e48\u8dd1 example \u5f97\u51fa\u7684 auc=0</p> <ul> <li>example \u4e3a 100 \u6761\u793a\u4f8b\u6570\u636e\uff0c\u4f9b\u7528\u6237\u53c2\u8003\u6570\u636e\u683c\u5f0f\u3001\u7279\u5f81\u7c7b\u578b\uff0c\u4fdd\u8bc1\u4ee3\u7801\u7545\u901a\u8fd0\u884c\uff0c\u4e0d\u4fdd\u8bc1\u7cbe\u5ea6</li> <li>\u5982\u679c\u9700\u8981\u6d4b\u8bd5\u6027\u80fd\uff0c\u53ef\u4ee5\u6309\u7167 readme \u4e2d\u6570\u636e\u96c6\u63cf\u8ff0\u7684\u4e0b\u8f7d\u94fe\u63a5\u4e0b\u8f7d\u6570\u636e\uff0c\u7136\u540e\u53c2\u8003 example \u4e2d\u7684\u53c2\u6570\u914d\u7f6e\u6587\u4ef6\uff0c\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30</li> </ul> </li> <li> <p>annoy \u5b89\u88c5</p> <ul> <li> <p>windows \u5b89\u88c5 annoy</p> <ul> <li>\u5728\u7ebf\u5b89\u88c5</li> </ul> <pre><code>pip install annoy\n</code></pre> <p>\u5982\u679c windows \u4e0a\u6ca1\u6709 C++\u76f8\u5173\u7f16\u8bd1\u73af\u5883\uff0c\u51fa\u73b0\u5982\u4e0b\u62a5\u9519\uff1a</p> <pre><code>error: Microsoft Visual C++ 14.0 or greater is required. Get it with \u201cMicrosoft C++ Build Tools\u201d: https://visualstudio.microsoft.com/visual-cpp-build-tools/\n---------------------------------------------------------------------\n</code></pre> <p>\u62a5\u9519\u622a\u56fe\uff1a </p> <p>\u5219\u53ef\u4ee5\u91c7\u7528\u79bb\u7ebf\u5b89\u88c5\u65b9\u5f0f</p> <ul> <li>\u79bb\u7ebf\u5b89\u88c5</li> </ul> <p>annoy \u5e93\u4e0b\u8f7d\u5730\u5740\uff1ahttps://www.lfd.uci.edu/~gohlke/pythonlibs/#_annoy</p> <pre><code>pip install annoy\u20111.17.0\u2011cp39\u2011cp39\u2011win_amd64.whl\n</code></pre> </li> <li> <p>linux/mac os \u5b89\u88c5 annoy</p> <ul> <li>\u5728\u7ebf\u5b89\u88c5</li> </ul> <pre><code>pip install annoy\n</code></pre> <p>\u6b63\u5e38 mac \u53ef\u4ee5\u5728\u7ebf\u5b89\u88c5\u6210\u529f\uff0c\u5982\u679c\u5728\u7ebf\u5b89\u88c5\u62a5\u9519\uff0c\u6700\u4e0b\u65b9\u63d0\u793a\u548c nose \u76f8\u5173\u62a5\u9519\uff0c\u5219\u8fdb\u884c\u79bb\u7ebf\u7f16\u8bd1\u5b89\u88c5</p> <ul> <li> <p>\u79bb\u7ebf\u5b89\u88c5</p> </li> <li> <p>\u4e0b\u8f7d nose</p> <p>\u4e0b\u8f7d\u5730\u5740\uff1ahttps://www.lfd.uci.edu/~gohlke/pythonlibs/#_annoy</p> <pre><code>pip install nose\u20111.3.7\u2011py3\u2011none\u2011any.whl\n</code></pre> </li> <li> <p>\u4e0b\u8f7d annoy</p> <p>\u4e0b\u8f7d\u5730\u5740\uff1ahttps://files.pythonhosted.org/packages/a1/5b/1c22129f608b3f438713b91cd880dc681d747a860afe3e8e0af86e921942/annoy-1.17.0.tar.gz</p> <pre><code>tar -zxvf annoy-1.17.0.tar.gz\ncd annoy-1.17.0\npython setup.py install\n</code></pre> </li> </ul> </li> </ul> <p>annoy \u5b89\u88c5\u540e\uff0c\u5373\u53ef\u76f4\u63a5\u5b89\u88c5 torch-rechub \u4e86</p> <pre><code>pip install --upgrade torch-rechub\n</code></pre> </li> </ul>"},{"location":"zh/getting-started/","title":"\u5165\u95e8\u6559\u7a0b","text":"<p>\u9996\u5148\uff0c\u5b89\u88c5Torch-RecHub\uff1a</p> <pre><code>pip install torch-rechub\n</code></pre> <p>\u7136\u540e\uff0c\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u8fdb\u884c\u63a8\u8350\u7cfb\u7edf\u6a21\u578b\u7684\u8bad\u7ec3\uff1a</p>"},{"location":"zh/getting-started/#ctr","title":"\u7cbe\u6392\uff08CTR\u9884\u6d4b\uff09","text":"<pre><code>from torch_rechub.models.ranking import DeepFM\nfrom torch_rechub.trainers import CTRTrainer\nfrom torch_rechub.utils.data import DataGenerator\n\ndg = DataGenerator(x, y)\ntrain_dataloader, val_dataloader, test_dataloader = dg.generate_dataloader(split_ratio=[0.7, 0.1], batch_size=256)\n\nmodel = DeepFM(deep_features=deep_features, fm_features=fm_features, mlp_params={\"dims\": [256, 128], \"dropout\": 0.2, \"activation\": \"relu\"})\n\nctr_trainer = CTRTrainer(model)\nctr_trainer.fit(train_dataloader, val_dataloader)\nauc = ctr_trainer.evaluate(ctr_trainer.model, test_dataloader)\n</code></pre>"},{"location":"zh/getting-started/#_1","title":"\u591a\u4efb\u52a1\u6392\u5e8f","text":"<pre><code>from torch_rechub.models.multi_task import SharedBottom, ESMM, MMOE, PLE, AITM\nfrom torch_rechub.trainers import MTLTrainer\n\ntask_types = [\"classification\", \"classification\"] \nmodel = MMOE(features, task_types, 8, expert_params={\"dims\": [32,16]}, tower_params_list=[{\"dims\": [32, 16]}, {\"dims\": [32, 16]}])\n\nmtl_trainer = MTLTrainer(model)\nmtl_trainer.fit(train_dataloader, val_dataloader)\nauc = ctr_trainer.evaluate(ctr_trainer.model, test_dataloader)\n</code></pre>"},{"location":"zh/getting-started/#_2","title":"\u53ec\u56de\u6a21\u578b","text":"<pre><code>from torch_rechub.models.matching import DSSM\nfrom torch_rechub.trainers import MatchTrainer\nfrom torch_rechub.utils.data import MatchDataGenerator\n\ndg = MatchDataGenerator(x y)\ntrain_dl, test_dl, item_dl = dg.generate_dataloader(test_user, all_item, batch_size=256)\n\nmodel = DSSM(user_features, item_features, temperature=0.02,\n             user_params={\n                 \"dims\": [256, 128, 64],\n                 \"activation\": 'prelu',  \n             },\n             item_params={\n                 \"dims\": [256, 128, 64],\n                 \"activation\": 'prelu', \n             })\n\nmatch_trainer = MatchTrainer(model)\nmatch_trainer.fit(train_dl)\n</code></pre>"},{"location":"zh/getting-started/#_3","title":"\u6a21\u578b\u5e93","text":""},{"location":"zh/getting-started/#_4","title":"\u6a21\u578b\u6e05\u5355","text":"\u6807\u9898 \u6807\u7b7e \u5f00\u53d1\u72b6\u6001 \u5f00\u53d1\u4eba\u5458 \u673a\u6784 \u4f1a\u8bae \u5e74\u4efd URL pdf DIN \u6392\u5e8f,\u5e8f\u5217\u5efa\u6a21 \u5df2\u5b8c\u6210 \u8d56\u654f\u6750 \u963f\u91cc\u5df4\u5df4 KDD 2018 https://arxiv.org/abs/1706.06978 1706.06978.pdf ESMM \u6392\u5e8f \u5df2\u5b8c\u6210 \u8d56\u654f\u6750 \u963f\u91cc\u5df4\u5df4 SIGIR 2018 https://arxiv.org/abs/1804.07931 1804.07931.pdf Youtube-SBC \u53ec\u56de \u5df2\u5b8c\u6210 \u8d56\u654f\u6750 \u8c37\u6b4c RecSys 2019 https://research.google/pubs/pub48840/ 6c8a86c981a62b0126a11896b7f6ae0dae4c3566.pdf DSSM \u53ec\u56de \u5df2\u5b8c\u6210 \u8d56\u654f\u6750 \u5fae\u8f6f CIKM 2013 https://posenhuang.github.io/papers/cikm2013_DSSM_fullversion.pdf cikm2013_DSSM_fullversion.pdf MetaBalance \u5176\u4ed6 \u5df2\u5b8c\u6210 Facebook www 2022 https://arxiv.org/pdf/2203.06801v1.pdf 2203.06801v1-3.pdf Wide &amp; Deep \u6392\u5e8f \u5df2\u5b8c\u6210 \u8d56\u654f\u6750 \u8c37\u6b4c DLRS 2016 https://arxiv.org/pdf/1606.07792.pdf 1606.07792.pdf DSSM-Facebook \u53ec\u56de \u5df2\u5b8c\u6210 \u8d56\u654f\u6750 Facebook KDD 2020 https://arxiv.org/abs/2006.11632 2006.11632.pdf DeepFM \u6392\u5e8f \u5df2\u5b8c\u6210 \u8d56\u654f\u6750 \u534e\u4e3a IJCAI 2017 https://arxiv.org/abs/1703.04247 1703.04247.pdf SasRec \u53ec\u56de \u8fdb\u884c\u4e2d \u738b\u5b87\u5bb8 UCSD ICDM 2018 https://arxiv.org/abs/1808.09781 1808.09781v1.pdf PLE \u6392\u5e8f \u5df2\u5b8c\u6210 \u8d56\u654f\u6750 \u817e\u8baf RecSys 2020 https://dl.acm.org/doi/abs/10.1145/3383313.3412236?casa_token=4g_ErWbxWf8AAAAA%3APhbcdBa6b-SXHlpFtKh1Lybjtv48sYV2l1GsPeL43N5Lpih_GwarAwV5hzxOYUVZoWd8dimltm4czmI 2020 (Tencent) (Recsys) [PLE] Progressive Layered Extraction (PLE) - A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations.pdf AITM \u6392\u5e8f \u5df2\u5b8c\u6210 \u8d56\u654f\u6750 \u7f8e\u56e2 KDD 2021 https://arxiv.org/abs/2105.08489 2105.08489-2.pdf Shared-Bottom \u6392\u5e8f \u5df2\u5b8c\u6210 \u8d56\u654f\u6750 CMU ML 1997 https://link.springer.com/content/pdf/10.1023/A:1007379606734.pdf Caruana1997_Article_MultitaskLearning.pdf DCN \u6392\u5e8f \u5df2\u5b8c\u6210 \u8d56\u654f\u6750 \u8c37\u6b4c,\u65af\u5766\u798f AKDD 2017 https://arxiv.org/abs/1708.05123 1708.05123.pdf Youtube-DNN \u53ec\u56de \u5df2\u5b8c\u6210 \u8d56\u654f\u6750 \u8c37\u6b4c RecSys 2016 https://dl.acm.org/doi/10.1145/2959100.2959190 2959100.2959190.pdf MMOE \u6392\u5e8f \u5df2\u5b8c\u6210 \u8d56\u654f\u6750 \u8c37\u6b4c KDD 2018 https://dl.acm.org/doi/pdf/10.1145/3219819.3220007 3219819.3220007.pdf GRU4Rec \u53ec\u56de,\u5e8f\u5217\u5efa\u6a21 \u5df2\u5b8c\u6210 \u738b\u51ef \u817e\u8baf KDD 2022 https://arxiv.org/abs/1511.06939 SASRec \u53ec\u56de,\u5e8f\u5217\u5efa\u6a21 \u5df2\u5b8c\u6210 \u738b\u5b87\u5bb8 UC ICDM 2018 https://arxiv.org/pdf/1808.09781.pdf 1808.09781-3.pdf SINE \u53ec\u56de \u5df2\u5b8c\u6210 \u5eb7\u535a \u963f\u91cc\u5df4\u5df4 WSDM 2021 https://arxiv.org/pdf/2102.09267.pdf 2102.09267.pdf (FAT-)DeepFFM \u6392\u5e8f \u5df2\u5b8c\u6210 \u5eb7\u535a \u65b0\u6d6a arXiv 2019 https://arxiv.org/pdf/1905.06336.pdf 1905.06336.pdf STAMP \u53ec\u56de,\u5e8f\u5217\u5efa\u6a21 \u5df2\u5b8c\u6210 \u5eb7\u535a \u7535\u5b50\u79d1\u5927 KDD 2018 https://dl.acm.org/doi/10.1145/3219819.3219950 3219819.3219950.pdf NARM \u53ec\u56de,\u5e8f\u5217\u5efa\u6a21 \u5df2\u5b8c\u6210 \u5eb7\u535a \u4eac\u4e1c,\u5c71\u4e1c\u5927\u5b66 CIKM 2017 https://arxiv.org/pdf/1711.04725.pdf 1711.00165.pdf DCN_v2 \u6392\u5e8f \u5df2\u5b8c\u6210 \u53f6\u5fd7\u96c4 \u8c37\u6b4c www 2021 https://arxiv.org/abs/2008.13535 DCN V2 Improved Deep &amp; Cross Network and Practical Lessons.pdf EDCN \u6392\u5e8f \u5df2\u5b8c\u6210 \u53f6\u5fd7\u96c4 \u534e\u4e3a KDD 2021 https://dlp-kdd.github.io/assets/pdf/DLP-KDD_2021_paper_12.pdf FiBiNet \u6392\u5e8f \u5df2\u5b8c\u6210 \u53f6\u5fd7\u96c4 \u65b0\u6d6a RecSys 2019 https://dl.acm.org/doi/abs/10.1145/3298689.3347043 DIEN \u6392\u5e8f,\u5e8f\u5217\u5efa\u6a21 \u5df2\u5b8c\u6210 \u8303\u6d9b \u963f\u91cc\u5df4\u5df4 AAAI 2019 https://ojs.aaai.org/index.php/AAAI/article/view/4545 4545-Article Text-7584-1-10-20190706.pdf BST \u6392\u5e8f,\u5e8f\u5217\u5efa\u6a21 \u5df2\u5b8c\u6210 \u8303\u6d9b \u963f\u91cc\u5df4\u5df4 arXiv 2019 Behavior Sequence Transformer for E-commerce Recommendation in Alibaba pdf <p>\u53c2\u8003\u8d44\u6599</p>"},{"location":"zh/installation/","title":"\u5b89\u88c5\u6307\u5357","text":"<p>\u8fd9\u91cc\u53ef\u4ee5\u8be6\u7ec6\u4ecb\u7ecd Torch-RecHub \u7684\u5b89\u88c5\u65b9\u6cd5\uff0c\u5305\u62ec\u7a33\u5b9a\u7248\u548c\u6700\u65b0\u7248\u7684\u5b89\u88c5\u6b65\u9aa4\u3002</p>"},{"location":"zh/installation/#_2","title":"\u7a33\u5b9a\u7248","text":"<pre><code>pip install torch-rechub\n</code></pre>"},{"location":"zh/installation/#_3","title":"\u6700\u65b0\u7248\uff08\u63a8\u8350\uff09","text":"<pre><code>git clone https://github.com/datawhalechina/torch-rechub.git\ncd torch-rechub\npython setup.py install\n</code></pre>"},{"location":"zh/introduction/","title":"\u9879\u76ee\u4ecb\u7ecd","text":""},{"location":"zh/introduction/#_2","title":"\u9879\u76ee\u6982\u8ff0","text":"<p>Torch-RecHub \u662f\u4e00\u4e2a\u4f7f\u7528 PyTorch \u6784\u5efa\u7684\u3001\u7075\u6d3b\u4e14\u6613\u4e8e\u6269\u5c55\u7684\u63a8\u8350\u7cfb\u7edf\u6846\u67b6\u3002\u5b83\u65e8\u5728\u7b80\u5316\u63a8\u8350\u7b97\u6cd5\u7684\u7814\u7a76\u548c\u5e94\u7528\uff0c\u63d0\u4f9b\u5e38\u89c1\u7684\u6a21\u578b\u5b9e\u73b0\u3001\u6570\u636e\u5904\u7406\u5de5\u5177\u548c\u8bc4\u4f30\u6307\u6807\u3002</p>"},{"location":"zh/introduction/#_3","title":"\u7279\u6027","text":"<ul> <li>\u6a21\u5757\u5316\u8bbe\u8ba1: \u6613\u4e8e\u6dfb\u52a0\u65b0\u7684\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002</li> <li>\u57fa\u4e8e PyTorch: \u5229\u7528 PyTorch \u7684\u52a8\u6001\u56fe\u548c GPU \u52a0\u901f\u80fd\u529b\u3002</li> <li>\u4e30\u5bcc\u7684\u6a21\u578b\u5e93: \u5305\u542b\u591a\u79cd\u7ecf\u5178\u548c\u524d\u6cbf\u7684\u63a8\u8350\u7b97\u6cd5\uff08\u8bf7\u5728\u4e0b\u65b9\u5217\u51fa\uff09\u3002</li> <li>\u6807\u51c6\u5316\u6d41\u7a0b: \u63d0\u4f9b\u7edf\u4e00\u7684\u6570\u636e\u52a0\u8f7d\u3001\u8bad\u7ec3\u548c\u8bc4\u4f30\u6d41\u7a0b\u3002</li> <li>\u6613\u4e8e\u914d\u7f6e: \u901a\u8fc7\u914d\u7f6e\u6587\u4ef6\u6216\u547d\u4ee4\u884c\u53c2\u6570\u8f7b\u677e\u8c03\u6574\u5b9e\u9a8c\u8bbe\u7f6e\u3002</li> <li>\u53ef\u590d\u73b0\u6027: \u65e8\u5728\u786e\u4fdd\u5b9e\u9a8c\u7ed3\u679c\u7684\u53ef\u590d\u73b0\u6027\u3002</li> <li>\u6613\u6269\u5c55: \u6a21\u578b\u8bad\u7ec3\u4e0e\u6a21\u578b\u5b9a\u4e49\u89e3\u8026\uff0c\u65e0basemodel\u6982\u5ff5\u3002</li> <li>\u539f\u751f\u51fd\u6570: \u5c3d\u53ef\u80fd\u4f7f\u7528pytorch\u539f\u751f\u7684\u7c7b\u4e0e\u51fd\u6570\uff0c\u4e0d\u505a\u8fc7\u591a\u5b9a\u5236\u3002</li> <li>\u6a21\u578b\u4ee3\u7801\u7cbe\u7b80: \u5728\u7b26\u5408\u8bba\u6587\u601d\u60f3\u7684\u57fa\u7840\u4e0a\u65b9\u4fbf\u65b0\u624b\u5b66\u4e60</li> <li>\u5176\u4ed6\u7279\u6027: \u4f8b\u5982\uff0c\u652f\u6301\u8d1f\u91c7\u6837\u3001\u591a\u4efb\u52a1\u5b66\u4e60\u7b49\u3002</li> </ul>"},{"location":"zh/introduction/#_4","title":"\u6574\u4f53\u67b6\u6784","text":""},{"location":"zh/introduction/#_5","title":"\u6570\u636e\u5c42\u8bbe\u8ba1","text":""},{"location":"zh/introduction/#_6","title":"\u7279\u5f81\u7c7b","text":"<p>\u6570\u503c\u578b\u7279\u5f81</p> <ul> <li>\u4f8b\u5982\u5e74\u9f84\u3001\u85aa\u8d44\u3001\u65e5\u70b9\u51fb\u91cf\u7b49</li> </ul> <p>\u7c7b\u522b\u578b\u7279\u5f81</p> <ul> <li>\u4f8b\u5982\u57ce\u5e02\u3001\u5b66\u5386\u3001\u6027\u522b\u7b49</li> <li>LabelEncoder\u7f16\u7801\uff0c\u5f97\u5230Embedding\u5411\u91cf</li> </ul> <p>\u5e8f\u5217\u7279\u5f81</p> <ul> <li>\u6709\u5e8f\u5174\u8da3\u5e8f\u5217\uff1a\u4f8b\u5982\u6700\u8fd1\u4e00\u5468\u70b9\u51fb\u8fc7\u7684item list</li> <li>\u65e0\u5e8f\u6807\u7b7e\u7279\u5f81\uff1a\u4f8b\u5982\u7535\u5f71\u7c7b\u578b\uff08\u52a8\u4f5c|\u60ac\u7591|\u72af\u7f6a\uff09</li> <li>LabelEncoder\u7f16\u7801\uff0c\u5f97\u5230\u5e8f\u5217Embedding\u5411\u91cf</li> <li>\u6c60\u5316\uff0c\u964d\u7ef4</li> <li>\u4fdd\u7559\u5e8f\u5217\uff0c\u4e0e\u5176\u4ed6\u7279\u5f81\u8fdb\u884cattention\u7b49\u6a21\u578b\u64cd\u4f5c</li> <li>\u4e0eSparse\u7279\u5f81\u5171\u4eabEmbedding Table</li> </ul>"},{"location":"zh/introduction/#_7","title":"\u6570\u636e\u7c7b","text":"<ul> <li>Dataset</li> <li>Dataloader</li> </ul>"},{"location":"zh/introduction/#_8","title":"\u5de5\u5177\u7c7b","text":"<ul> <li>\u5e8f\u5217\u7279\u5f81\u751f\u6210</li> <li>\u6837\u672c\u6784\u9020</li> <li>\u8d1f\u91c7\u6837</li> <li>\u5411\u91cf\u5316\u53ec\u56de</li> </ul>"},{"location":"zh/introduction/#_9","title":"\u6a21\u578b\u5c42\u8bbe\u8ba1","text":""},{"location":"zh/introduction/#_10","title":"\u6a21\u578b\u7c7b","text":"<p>\u901a\u7528Layer \u6d45\u5c42\u7279\u5f81\u5efa\u6a21</p> <ul> <li>LR\uff1a\u903b\u8f91\u56de\u5f52</li> <li>MLP\uff1a\u591a\u5c42\u611f\u77e5\u673a\uff0c\u53ef\u901a\u8fc7\u5b57\u5178\u8bbe\u7f6edims\u7b49\u53c2\u6570</li> <li>EmbeddinLayer\uff1a\u901a\u7528Embedding\u5c42\uff0c\u542b\u4e09\u7c7b\u7279\u5f81\u7684\u5904\u7406\uff0c\u7ef4\u62a4\u4e00\u4e2adict\u683c\u5f0f\u7684EmbeddingTable\uff0c\u8f93\u51fa\u7ecf\u8fc7\u6a21\u578b\u6240\u9700\u8981\u7684\u8f93\u5165embedding</li> </ul> <p>\u6df1\u5c42\u7279\u5f81\u5efa\u6a21</p> <ul> <li>FM\u3001FFM\u3001CIN</li> <li>self-attention\u3001target-attention\u3001transformer</li> </ul> <p>\u5b9a\u5236Layer</p>"},{"location":"zh/api-reference/basic/","title":"\u57fa\u7840\u7ec4\u4ef6 API \u53c2\u8003","text":"<p>\u672c\u6587\u6863\u8be6\u7ec6\u4ecb\u7ecd Torch-RecHub \u4e2d\u7684\u57fa\u7840\u7ec4\u4ef6\uff0c\u5305\u62ec\u7279\u5f81\u5904\u7406\u3001\u6570\u636e\u8f6c\u6362\u7b49\u57fa\u7840\u529f\u80fd\u3002</p>"},{"location":"zh/api-reference/basic/#_1","title":"\u7279\u5f81\u5904\u7406","text":""},{"location":"zh/api-reference/basic/#feature-columns","title":"\u7279\u5f81\u5217 (Feature Columns)","text":""},{"location":"zh/api-reference/basic/#densefeature","title":"DenseFeature","text":"<ul> <li>\u7b80\u4ecb\uff1a\u5904\u7406\u8fde\u7eed\u578b\u6570\u503c\u7279\u5f81\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>name</code> (str): \u7279\u5f81\u540d\u79f0</li> <li><code>dimension</code> (int): \u7279\u5f81\u7ef4\u5ea6</li> <li><code>dtype</code> (str): \u6570\u636e\u7c7b\u578b\uff0c\u9ed8\u8ba4'float32'</li> </ul>"},{"location":"zh/api-reference/basic/#sparsefeature","title":"SparseFeature","text":"<ul> <li>\u7b80\u4ecb\uff1a\u5904\u7406\u79bb\u6563\u578b\u7c7b\u522b\u7279\u5f81\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>name</code> (str): \u7279\u5f81\u540d\u79f0</li> <li><code>vocabulary_size</code> (int): \u7c7b\u522b\u8bcd\u8868\u5927\u5c0f</li> <li><code>embedding_dim</code> (int): \u5d4c\u5165\u5411\u91cf\u7ef4\u5ea6</li> <li><code>dtype</code> (str): \u6570\u636e\u7c7b\u578b\uff0c\u9ed8\u8ba4'int32'</li> <li><code>embedding_name</code> (str): \u5d4c\u5165\u5c42\u540d\u79f0\uff0c\u9ed8\u8ba4None</li> </ul>"},{"location":"zh/api-reference/basic/#varlensparsefeature","title":"VarLenSparseFeature","text":"<ul> <li>\u7b80\u4ecb\uff1a\u5904\u7406\u53d8\u957f\u79bb\u6563\u578b\u7279\u5f81\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>name</code> (str): \u7279\u5f81\u540d\u79f0</li> <li><code>vocabulary_size</code> (int): \u7c7b\u522b\u8bcd\u8868\u5927\u5c0f</li> <li><code>embedding_dim</code> (int): \u5d4c\u5165\u5411\u91cf\u7ef4\u5ea6</li> <li><code>maxlen</code> (int): \u5e8f\u5217\u6700\u5927\u957f\u5ea6</li> <li><code>dtype</code> (str): \u6570\u636e\u7c7b\u578b\uff0c\u9ed8\u8ba4'int32'</li> <li><code>embedding_name</code> (str): \u5d4c\u5165\u5c42\u540d\u79f0\uff0c\u9ed8\u8ba4None</li> <li><code>combiner</code> (str): \u5e8f\u5217\u6c60\u5316\u65b9\u5f0f\uff0c\u53ef\u9009'sum'\uff0c'mean'\uff0c'max'\uff0c\u9ed8\u8ba4'mean'</li> </ul>"},{"location":"zh/api-reference/basic/#_2","title":"\u6570\u636e\u8f6c\u6362","text":""},{"location":"zh/api-reference/basic/#_3","title":"\u6570\u636e\u9884\u5904\u7406","text":""},{"location":"zh/api-reference/basic/#minmaxscaler","title":"MinMaxScaler","text":"<ul> <li>\u7b80\u4ecb\uff1a\u6570\u503c\u7279\u5f81\u5f52\u4e00\u5316\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>feature_range</code> (tuple): \u5f52\u4e00\u5316\u8303\u56f4\uff0c\u9ed8\u8ba4(0, 1)</li> </ul>"},{"location":"zh/api-reference/basic/#standardscaler","title":"StandardScaler","text":"<ul> <li>\u7b80\u4ecb\uff1a\u6570\u503c\u7279\u5f81\u6807\u51c6\u5316\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>with_mean</code> (bool): \u662f\u5426\u51cf\u53bb\u5747\u503c\uff0c\u9ed8\u8ba4True</li> <li><code>with_std</code> (bool): \u662f\u5426\u9664\u4ee5\u6807\u51c6\u5dee\uff0c\u9ed8\u8ba4True</li> </ul>"},{"location":"zh/api-reference/basic/#labelencoder","title":"LabelEncoder","text":"<ul> <li>\u7b80\u4ecb\uff1a\u7c7b\u522b\u7279\u5f81\u7f16\u7801\u3002</li> <li>\u65b9\u6cd5\uff1a</li> <li><code>fit(values)</code>: \u62df\u5408\u7f16\u7801\u5668</li> <li><code>transform(values)</code>: \u8f6c\u6362\u6570\u636e</li> <li><code>fit_transform(values)</code>: \u62df\u5408\u5e76\u8f6c\u6362</li> </ul>"},{"location":"zh/api-reference/basic/#_4","title":"\u6570\u636e\u683c\u5f0f\u8f6c\u6362","text":""},{"location":"zh/api-reference/basic/#pandas_to_torch","title":"pandas_to_torch","text":"<ul> <li>\u7b80\u4ecb\uff1a\u5c06Pandas\u6570\u636e\u8f6c\u6362\u4e3aPyTorch\u5f20\u91cf\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>df</code> (pd.DataFrame): \u8f93\u5165\u6570\u636e\u6846</li> <li><code>dense_cols</code> (list): \u8fde\u7eed\u7279\u5f81\u5217\u540d\u5217\u8868</li> <li><code>sparse_cols</code> (list): \u79bb\u6563\u7279\u5f81\u5217\u540d\u5217\u8868</li> <li><code>device</code> (str): \u8bbe\u5907\u7c7b\u578b\uff0c'cpu'\u6216'cuda'</li> </ul>"},{"location":"zh/api-reference/basic/#numpy_to_torch","title":"numpy_to_torch","text":"<ul> <li>\u7b80\u4ecb\uff1a\u5c06NumPy\u6570\u7ec4\u8f6c\u6362\u4e3aPyTorch\u5f20\u91cf\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>arrays</code> (list): NumPy\u6570\u7ec4\u5217\u8868</li> <li><code>device</code> (str): \u8bbe\u5907\u7c7b\u578b\uff0c'cpu'\u6216'cuda'</li> </ul>"},{"location":"zh/api-reference/basic/#_5","title":"\u6a21\u578b\u7ec4\u4ef6","text":""},{"location":"zh/api-reference/basic/#_6","title":"\u6fc0\u6d3b\u51fd\u6570","text":""},{"location":"zh/api-reference/basic/#dice","title":"Dice","text":"<ul> <li>\u7b80\u4ecb\uff1aDice\u6fc0\u6d3b\u51fd\u6570\uff0c\u5728\u6df1\u5ea6\u5174\u8da3\u7f51\u7edc(DIN)\u4e2d\u63d0\u51fa\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>epsilon</code> (float): \u5e73\u6ed1\u53c2\u6570\uff0c\u9ed8\u8ba41e-3</li> <li><code>device</code> (str): \u8bbe\u5907\u7c7b\u578b\uff0c\u9ed8\u8ba4'cpu'</li> </ul>"},{"location":"zh/api-reference/basic/#_7","title":"\u6ce8\u610f\u529b\u673a\u5236","text":""},{"location":"zh/api-reference/basic/#scaleddotproductattention","title":"ScaledDotProductAttention","text":"<ul> <li>\u7b80\u4ecb\uff1a\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\u673a\u5236\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>temperature</code> (float): \u6e29\u5ea6\u53c2\u6570\uff0c\u7528\u4e8e\u7f29\u653e</li> <li><code>attn_dropout</code> (float): \u6ce8\u610f\u529bdropout\u7387</li> </ul>"},{"location":"zh/api-reference/basic/#multiheadattention","title":"MultiHeadAttention","text":"<ul> <li>\u7b80\u4ecb\uff1a\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>d_model</code> (int): \u6a21\u578b\u7ef4\u5ea6</li> <li><code>n_heads</code> (int): \u6ce8\u610f\u529b\u5934\u6570</li> <li><code>d_k</code> (int): \u952e\u5411\u91cf\u7ef4\u5ea6</li> <li><code>d_v</code> (int): \u503c\u5411\u91cf\u7ef4\u5ea6</li> <li><code>dropout</code> (float): dropout\u7387 </li> </ul>"},{"location":"zh/api-reference/models/","title":"\u6a21\u578b API \u53c2\u8003","text":"<p>\u8fd9\u91cc\u8be6\u7ec6\u4ecb\u7ecd Torch-RecHub \u4e2d\u5404\u4e2a\u6a21\u578b\u7684 API \u63a5\u53e3\u548c\u53c2\u6570\u8bf4\u660e\u3002</p>"},{"location":"zh/api-reference/models/#recall-models","title":"\u53ec\u56de\u6a21\u578b (Recall Models)","text":"<p>\u53ec\u56de\u6a21\u578b\u4e3b\u8981\u7528\u4e8e\u53ec\u56de\u9636\u6bb5\uff0c\u4ece\u6d77\u91cf\u5019\u9009\u96c6\u4e2d\u5feb\u901f\u68c0\u7d22\u76f8\u5173\u7269\u54c1\u3002\u901a\u5e38\u91c7\u7528\u53cc\u5854\u7ed3\u6784\u6216\u5e8f\u5217\u6a21\u578b\u7ed3\u6784\uff0c\u4ee5\u6ee1\u8db3\u53ec\u56de\u9636\u6bb5\u7684\u6548\u7387\u8981\u6c42\u3002</p>"},{"location":"zh/api-reference/models/#_1","title":"\u53cc\u5854\u6a21\u578b\u7cfb\u5217","text":""},{"location":"zh/api-reference/models/#dssm-deep-structured-semantic-model","title":"DSSM (Deep Structured Semantic Model)","text":"<ul> <li>\u7b80\u4ecb\uff1a\u6700\u65e9\u7531\u5fae\u8f6f\u63d0\u51fa\u7684\u8bed\u4e49\u5339\u914d\u6a21\u578b\uff0c\u540e\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u3002\u91c7\u7528\u7ecf\u5178\u7684\u53cc\u5854\u7ed3\u6784\uff0c\u5206\u522b\u5bf9\u7528\u6237\u548c\u7269\u54c1\u8fdb\u884c\u8868\u5f81\uff0c\u901a\u8fc7\u5185\u79ef\u8ba1\u7b97\u76f8\u4f3c\u5ea6\u3002\u8fd9\u79cd\u7ed3\u6784\u5141\u8bb8\u5728\u7ebf\u670d\u52a1\u65f6\u9884\u5148\u8ba1\u7b97\u7269\u54c1\u5411\u91cf\uff0c\u6781\u5927\u63d0\u9ad8\u4e86\u670d\u52a1\u6548\u7387\u3002\u6a21\u578b\u7684\u5173\u952e\u5728\u4e8e\u5982\u4f55\u5b66\u4e60\u5230\u6709\u6548\u7684\u7528\u6237\u548c\u7269\u54c1\u8868\u5f81\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>user_features</code> (list): \u7528\u6237\u7279\u5f81\u5217\u8868</li> <li><code>item_features</code> (list): \u7269\u54c1\u7279\u5f81\u5217\u8868</li> <li><code>hidden_units</code> (list): \u9690\u85cf\u5c42\u5355\u5143\u6570\u5217\u8868</li> <li><code>dropout_rates</code> (list): Dropout\u6bd4\u7387\u5217\u8868</li> <li><code>embedding_dim</code> (int): \u6700\u7ec8\u7684\u8868\u5f81\u5411\u91cf\u7ef4\u5ea6</li> </ul>"},{"location":"zh/api-reference/models/#facebook-dssm","title":"Facebook DSSM","text":"<ul> <li>\u7b80\u4ecb\uff1aFacebook\u5bf9DSSM\u7684\u6539\u8fdb\u7248\u672c\uff0c\u5f15\u5165\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u3002\u9664\u4e86\u4e3b\u8981\u7684\u53ec\u56de\u4efb\u52a1\u5916\uff0c\u8fd8\u589e\u52a0\u4e86\u8f85\u52a9\u4efb\u52a1\u6765\u5e2e\u52a9\u5b66\u4e60\u66f4\u597d\u7684\u7279\u5f81\u8868\u5f81\u3002\u6a21\u578b\u53ef\u4ee5\u540c\u65f6\u4f18\u5316\u591a\u4e2a\u76f8\u5173\u76ee\u6807\uff0c\u5982\u70b9\u51fb\u3001\u6536\u85cf\u3001\u8d2d\u4e70\u7b49\uff0c\u4ece\u800c\u5b66\u4e60\u5230\u66f4\u4e30\u5bcc\u7684\u7528\u6237\u548c\u7269\u54c1\u8868\u5f81\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>user_features</code> (list): \u7528\u6237\u7279\u5f81\u5217\u8868</li> <li><code>item_features</code> (list): \u7269\u54c1\u7279\u5f81\u5217\u8868</li> <li><code>hidden_units</code> (list): \u9690\u85cf\u5c42\u5355\u5143\u6570\u5217\u8868</li> <li><code>num_tasks</code> (int): \u4efb\u52a1\u6570\u91cf</li> <li><code>task_types</code> (list): \u4efb\u52a1\u7c7b\u578b\u5217\u8868</li> </ul>"},{"location":"zh/api-reference/models/#youtube-dnn","title":"YouTube DNN","text":"<ul> <li>\u7b80\u4ecb\uff1aYouTube\u63d0\u51fa\u7684\u6df1\u5ea6\u53ec\u56de\u6a21\u578b\uff0c\u9488\u5bf9\u5927\u89c4\u6a21\u89c6\u9891\u63a8\u8350\u573a\u666f\u8bbe\u8ba1\u3002\u6a21\u578b\u5c06\u7528\u6237\u89c2\u770b\u5386\u53f2\u901a\u8fc7\u5e73\u5747\u6c60\u5316\u8fdb\u884c\u805a\u5408\uff0c\u5e76\u7ed3\u5408\u7528\u6237\u5176\u4ed6\u7279\u5f81\u8fdb\u884c\u5efa\u6a21\u3002\u521b\u65b0\u6027\u5730\u5f15\u5165\u4e86\u8d1f\u91c7\u6837\u6280\u672f\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6548\u679c\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>user_features</code> (list): \u7528\u6237\u7279\u5f81\u5217\u8868</li> <li><code>item_features</code> (list): \u7269\u54c1\u7279\u5f81\u5217\u8868</li> <li><code>hidden_units</code> (list): \u9690\u85cf\u5c42\u5355\u5143\u6570\u5217\u8868</li> <li><code>embedding_dim</code> (int): \u5d4c\u5165\u7ef4\u5ea6</li> <li><code>max_seq_len</code> (int): \u6700\u5927\u5e8f\u5217\u957f\u5ea6</li> </ul>"},{"location":"zh/api-reference/models/#_2","title":"\u5e8f\u5217\u63a8\u8350\u7cfb\u5217","text":""},{"location":"zh/api-reference/models/#gru4rec","title":"GRU4Rec","text":"<ul> <li>\u7b80\u4ecb\uff1a\u9996\u6b21\u5c06GRU\u7f51\u7edc\u5e94\u7528\u4e8e\u4f1a\u8bdd\u5e8f\u5217\u63a8\u8350\u7684\u5f00\u521b\u6027\u5de5\u4f5c\u3002\u901a\u8fc7GRU\u7f51\u7edc\u6355\u6349\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u4e2d\u7684\u65f6\u5e8f\u4f9d\u8d56\u5173\u7cfb\uff0c\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u9690\u85cf\u72b6\u6001\u90fd\u5305\u542b\u4e86\u7528\u6237\u5386\u53f2\u884c\u4e3a\u7684\u4fe1\u606f\u3002\u6a21\u578b\u8fd8\u5f15\u5165\u4e86\u7279\u6b8a\u7684mini-batch\u6784\u9020\u65b9\u6cd5\u548c\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\uff0c\u4ee5\u9002\u5e94\u5e8f\u5217\u63a8\u8350\u7684\u7279\u70b9\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>item_num</code> (int): \u7269\u54c1\u603b\u6570</li> <li><code>hidden_size</code> (int): GRU\u9690\u85cf\u5c42\u5927\u5c0f</li> <li><code>num_layers</code> (int): GRU\u5c42\u6570</li> <li><code>dropout_rate</code> (float): Dropout\u6bd4\u7387</li> <li><code>embedding_dim</code> (int): \u7269\u54c1\u5d4c\u5165\u7ef4\u5ea6</li> </ul>"},{"location":"zh/api-reference/models/#narm-neural-attentive-recommendation-machine","title":"NARM (Neural Attentive Recommendation Machine)","text":"<ul> <li>\u7b80\u4ecb\uff1a\u5728GRU4Rec\u57fa\u7840\u4e0a\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\u7684\u5e8f\u5217\u63a8\u8350\u6a21\u578b\u3002\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6a21\u578b\u53ef\u4ee5\u6839\u636e\u5f53\u524d\u9884\u6d4b\u76ee\u6807\uff0c\u52a8\u6001\u5730\u5173\u6ce8\u5e8f\u5217\u4e2d\u7684\u76f8\u5173\u884c\u4e3a\u3002\u540c\u65f6\u7ef4\u62a4\u5168\u5c40\u548c\u5c40\u90e8\u4e24\u4e2a\u5e8f\u5217\u8868\u5f81\uff0c\u5168\u9762\u6355\u6349\u7528\u6237\u7684\u77ed\u671f\u5174\u8da3\u3002\u8fd9\u79cd\u8bbe\u8ba1\u8ba9\u6a21\u578b\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u7528\u6237\u5174\u8da3\u7684\u591a\u6837\u6027\u548c\u52a8\u6001\u6027\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>item_num</code> (int): \u7269\u54c1\u603b\u6570</li> <li><code>hidden_size</code> (int): \u9690\u85cf\u5c42\u5927\u5c0f</li> <li><code>attention_size</code> (int): \u6ce8\u610f\u529b\u5c42\u5927\u5c0f</li> <li><code>dropout_rate</code> (float): Dropout\u6bd4\u7387</li> <li><code>embedding_dim</code> (int): \u7269\u54c1\u5d4c\u5165\u7ef4\u5ea6</li> </ul>"},{"location":"zh/api-reference/models/#sasrec-self-attentive-sequential-recommendation","title":"SASRec (Self-Attentive Sequential Recommendation)","text":"<ul> <li>\u7b80\u4ecb\uff1a\u5c06Transformer\u7ed3\u6784\u5e94\u7528\u4e8e\u5e8f\u5217\u63a8\u8350\u7684\u4ee3\u8868\u6027\u5de5\u4f5c\u3002\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6a21\u578b\u53ef\u4ee5\u76f4\u63a5\u8ba1\u7b97\u5e76\u5b66\u4e60\u5e8f\u5217\u4e2d\u4efb\u610f\u4e24\u4e2a\u884c\u4e3a\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4e0d\u53d7RNN\u56fa\u6709\u7684\u5e8f\u5217\u4f9d\u8d56\u9650\u5236\u3002\u4f4d\u7f6e\u7f16\u7801\u5e2e\u52a9\u4fdd\u7559\u4e86\u884c\u4e3a\u7684\u65f6\u5e8f\u4fe1\u606f\uff0c\u591a\u5c42\u7ed3\u6784\u5219\u5141\u8bb8\u6a21\u578b\u9010\u5c42\u62bd\u53d6\u8d8a\u6765\u8d8a\u62bd\u8c61\u7684\u884c\u4e3a\u6a21\u5f0f\u3002\u76f8\u6bd4RNN\u7c7b\u6a21\u578b\uff0c\u5177\u6709\u66f4\u597d\u7684\u5e76\u884c\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>item_num</code> (int): \u7269\u54c1\u603b\u6570</li> <li><code>max_len</code> (int): \u6700\u5927\u5e8f\u5217\u957f\u5ea6</li> <li><code>num_heads</code> (int): \u6ce8\u610f\u529b\u5934\u6570</li> <li><code>num_layers</code> (int): Transformer\u5c42\u6570</li> <li><code>hidden_size</code> (int): \u9690\u85cf\u5c42\u7ef4\u5ea6</li> <li><code>dropout_rate</code> (float): Dropout\u6bd4\u7387</li> </ul>"},{"location":"zh/api-reference/models/#mind-multi-interest-network-with-dynamic-routing","title":"MIND (Multi-Interest Network with Dynamic routing)","text":"<ul> <li>\u7b80\u4ecb\uff1a\u9488\u5bf9\u7528\u6237\u591a\u6837\u5316\u5174\u8da3\u8bbe\u8ba1\u7684\u53ec\u56de\u6a21\u578b\u3002\u901a\u8fc7\u80f6\u56ca\u7f51\u7edc\u548c\u52a8\u6001\u8def\u7531\u673a\u5236\uff0c\u4ece\u7528\u6237\u7684\u884c\u4e3a\u5e8f\u5217\u4e2d\u63d0\u53d6\u591a\u4e2a\u5174\u8da3\u5411\u91cf\u3002\u6bcf\u4e2a\u5174\u8da3\u5411\u91cf\u4ee3\u8868\u7528\u6237\u5728\u4e0d\u540c\u65b9\u9762\u7684\u504f\u597d\uff0c\u8fd9\u79cd\u591a\u5174\u8da3\u8868\u793a\u65b9\u5f0f\u80fd\u66f4\u5168\u9762\u5730\u523b\u753b\u7528\u6237\u7684\u5174\u8da3\u5206\u5e03\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>item_num</code> (int): \u7269\u54c1\u603b\u6570</li> <li><code>num_interests</code> (int): \u5174\u8da3\u5411\u91cf\u6570\u91cf</li> <li><code>routing_iterations</code> (int): \u52a8\u6001\u8def\u7531\u8fed\u4ee3\u6b21\u6570</li> <li><code>hidden_size</code> (int): \u9690\u85cf\u5c42\u7ef4\u5ea6</li> <li><code>embedding_dim</code> (int): \u7269\u54c1\u5d4c\u5165\u7ef4\u5ea6</li> </ul>"},{"location":"zh/api-reference/models/#ranking-models","title":"\u6392\u5e8f\u6a21\u578b (Ranking Models)","text":"<p>\u6392\u5e8f\u6a21\u578b\u4e3b\u8981\u7528\u4e8e\u7cbe\u6392\u9636\u6bb5\uff0c\u5bf9\u5019\u9009\u96c6\u8fdb\u884c\u7cbe\u786e\u6392\u5e8f\u3002\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5b66\u4e60\u7528\u6237\u548c\u7269\u54c1\u4e4b\u95f4\u7684\u590d\u6742\u4ea4\u4e92\u5173\u7cfb\uff0c\u751f\u6210\u6700\u7ec8\u7684\u6392\u5e8f\u5f97\u5206\u3002</p>"},{"location":"zh/api-reference/models/#wide-deep","title":"Wide &amp; Deep \u7cfb\u5217","text":""},{"location":"zh/api-reference/models/#widedeep","title":"WideDeep","text":"<ul> <li>\u7b80\u4ecb\uff1aGoogle \u5728 2016 \u5e74\u63d0\u51fa\u7684\u7ecf\u5178\u6a21\u578b\uff0c\u7ed3\u5408\u7ebf\u6027\u6a21\u578b\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u4f18\u52bf\u3002Wide \u90e8\u5206\u901a\u8fc7\u7279\u5f81\u4ea4\u53c9\u8fdb\u884c\u8bb0\u5fc6\uff0c\u9002\u5408\u5efa\u6a21\u76f4\u63a5\u7684\u3001\u663e\u5f0f\u7684\u7279\u5f81\u76f8\u5173\u6027\uff1bDeep \u90e8\u5206\u901a\u8fc7\u6df1\u5ea6\u7f51\u7edc\u8fdb\u884c\u6cdb\u5316\uff0c\u53ef\u4ee5\u5b66\u4e60\u7279\u5f81\u95f4\u7684\u9690\u5f0f\u3001\u9ad8\u9636\u5173\u7cfb\u3002\u8fd9\u79cd\u7ed3\u5408\u8ba9\u6a21\u578b\u65e2\u80fd\u8bb0\u4f4f\u5386\u53f2\u89c4\u5f8b\uff0c\u53c8\u80fd\u6cdb\u5316\u5230\u65b0\u6a21\u5f0f\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>wide_features</code> (list): Wide\u90e8\u5206\u7279\u5f81\u5217\u8868\uff0c\u7528\u4e8e\u7ebf\u6027\u5c42</li> <li><code>deep_features</code> (list): Deep\u90e8\u5206\u7279\u5f81\u5217\u8868\uff0c\u7528\u4e8e\u6df1\u5ea6\u7f51\u7edc</li> <li><code>hidden_units</code> (list): Deep\u7f51\u7edc\u7684\u9690\u85cf\u5c42\u5355\u5143\u6570\u5217\u8868\uff0c\u5982 [256, 128, 64]</li> <li><code>dropout_rates</code> (list): \u6bcf\u5c42\u7684dropout\u6bd4\u7387\uff0c\u7528\u4e8e\u9632\u6b62\u8fc7\u62df\u5408</li> </ul>"},{"location":"zh/api-reference/models/#deepfm","title":"DeepFM","text":"<ul> <li>\u7b80\u4ecb\uff1a\u5c06FM\uff08\u56e0\u5b50\u5206\u89e3\u673a\uff09\u7684\u7279\u5f81\u4ea4\u4e92\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u7ed3\u5408\u7684\u6a21\u578b\u3002FM\u90e8\u5206\u53ef\u4ee5\u9ad8\u6548\u5730\u5efa\u6a21\u4e8c\u9636\u7279\u5f81\u4ea4\u4e92\uff0c\u800cDeep\u90e8\u5206\u5219\u53ef\u4ee5\u5b66\u4e60\u9ad8\u9636\u7279\u5f81\u5173\u7cfb\u3002\u76f8\u6bd4Wide&amp;Deep\uff0cDeepFM\u4e0d\u9700\u8981\u624b\u52a8\u8fdb\u884c\u7279\u5f81\u5de5\u7a0b\uff0c\u53ef\u4ee5\u81ea\u52a8\u5b66\u4e60\u7279\u5f81\u4ea4\u53c9\u3002\u6a21\u578b\u5305\u542b\u4e09\u90e8\u5206\uff1a\u4e00\u9636\u7279\u5f81\u3001FM\u7684\u4e8c\u9636\u4ea4\u4e92\u3001\u6df1\u5ea6\u7f51\u7edc\u7684\u9ad8\u9636\u4ea4\u4e92\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>features</code> (list): \u7279\u5f81\u5217\u8868</li> <li><code>hidden_units</code> (list): DNN\u90e8\u5206\u9690\u85cf\u5c42\u5355\u5143\u6570</li> <li><code>dropout_rates</code> (list): Dropout\u6bd4\u7387\u5217\u8868</li> <li><code>embedding_dim</code> (int): \u7279\u5f81\u5d4c\u5165\u7ef4\u5ea6</li> </ul>"},{"location":"zh/api-reference/models/#dcn-dcn-v2","title":"DCN / DCN-V2","text":"<ul> <li>\u7b80\u4ecb\uff1a\u901a\u8fc7\u7279\u6b8a\u8bbe\u8ba1\u7684\u4ea4\u53c9\u7f51\u7edc\u5c42\uff08Cross Network\uff09\u663e\u5f0f\u5b66\u4e60\u7279\u5f81\u4ea4\u4e92\u3002\u6bcf\u4e00\u5c42\u4ea4\u53c9\u7f51\u7edc\u90fd\u6267\u884c\u7279\u5f81\u5411\u91cf\u4e0e\u5176\u539f\u59cb\u5f62\u5f0f\u7684\u4ea4\u4e92\uff0c\u4f7f\u5f97\u4ea4\u53c9\u7279\u5f81\u7684\u7a0b\u5ea6\u968f\u7740\u5c42\u6570\u7684\u52a0\u6df1\u800c\u63d0\u9ad8\u3002DCN-V2\u6539\u8fdb\u4e86\u4ea4\u53c9\u7f51\u7edc\u7684\u53c2\u6570\u5316\u65b9\u5f0f\uff0c\u63d0\u4f9b\u4e86\"\u5411\u91cf\"\u548c\"\u77e9\u9635\"\u4e24\u79cd\u53c2\u6570\u5316\u9009\u9879\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6548\u7387\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>features</code> (list): \u7279\u5f81\u5217\u8868</li> <li><code>cross_num</code> (int): \u4ea4\u53c9\u5c42\u6570\u91cf</li> <li><code>hidden_units</code> (list): DNN\u90e8\u5206\u9690\u85cf\u5c42\u5355\u5143\u6570</li> <li><code>cross_parameterization</code> (str, DCN-V2): \u4ea4\u53c9\u53c2\u6570\u5316\u65b9\u5f0f\uff0c\"vector\"\u6216\"matrix\"</li> </ul>"},{"location":"zh/api-reference/models/#afm-attentional-factorization-machine","title":"AFM (Attentional Factorization Machine)","text":"<ul> <li>\u7b80\u4ecb\uff1a\u5728FM\u7684\u57fa\u7840\u4e0a\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5bf9\u4e0d\u540c\u7279\u5f81\u4ea4\u4e92\u8d4b\u4e88\u4e0d\u540c\u7684\u91cd\u8981\u6027\u6743\u91cd\u3002\u901a\u8fc7\u6ce8\u610f\u529b\u7f51\u7edc\u81ea\u9002\u5e94\u5730\u5b66\u4e60\u7279\u5f81\u4ea4\u4e92\u7684\u91cd\u8981\u6027\uff0c\u80fd\u591f\u8bc6\u522b\u51fa\u5bf9\u9884\u6d4b\u76ee\u6807\u66f4\u91cd\u8981\u7684\u7279\u5f81\u7ec4\u5408\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>features</code> (list): \u7279\u5f81\u5217\u8868</li> <li><code>attention_units</code> (list): \u6ce8\u610f\u529b\u7f51\u7edc\u9690\u85cf\u5c42\u5355\u5143\u6570</li> <li><code>embedding_dim</code> (int): \u7279\u5f81\u5d4c\u5165\u7ef4\u5ea6</li> <li><code>dropout_rate</code> (float): \u6ce8\u610f\u529b\u7f51\u7edc\u7684dropout\u6bd4\u7387</li> </ul>"},{"location":"zh/api-reference/models/#fibinet-feature-importance-and-bilinear-feature-interaction-network","title":"FiBiNET (Feature Importance and Bilinear feature Interaction Network)","text":"<ul> <li>\u7b80\u4ecb\uff1a\u901a\u8fc7SENET\u673a\u5236\u52a8\u6001\u5b66\u4e60\u7279\u5f81\u91cd\u8981\u6027\uff0c\u5e76\u4f7f\u7528\u53cc\u7ebf\u6027\u5c42\u8fdb\u884c\u7279\u5f81\u4ea4\u4e92\u3002SENET\u6a21\u5757\u5e2e\u52a9\u6a21\u578b\u8bc6\u522b\u91cd\u8981\u7279\u5f81\uff0c\u53cc\u7ebf\u6027\u4ea4\u4e92\u5219\u63d0\u4f9b\u4e86\u6bd4\u5185\u79ef\u66f4\u4e30\u5bcc\u7684\u7279\u5f81\u4ea4\u4e92\u65b9\u5f0f\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>features</code> (list): \u7279\u5f81\u5217\u8868</li> <li><code>bilinear_type</code> (str): \u53cc\u7ebf\u6027\u5c42\u7c7b\u578b\uff0c\u53ef\u9009\"field_all\"/\"field_each\"/\"field_interaction\"</li> <li><code>hidden_units</code> (list): DNN\u90e8\u5206\u9690\u85cf\u5c42\u5355\u5143\u6570</li> <li><code>reduction_ratio</code> (int): SENET\u6a21\u5757\u7684\u538b\u7f29\u6bd4\u7387</li> </ul>"},{"location":"zh/api-reference/models/#_3","title":"\u6ce8\u610f\u529b\u673a\u5236\u7cfb\u5217","text":""},{"location":"zh/api-reference/models/#din-deep-interest-network","title":"DIN (Deep Interest Network)","text":"<ul> <li>\u7b80\u4ecb\uff1a\u9488\u5bf9\u7528\u6237\u5174\u8da3\u591a\u6837\u6027\u8bbe\u8ba1\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u7528\u6237\u5386\u53f2\u884c\u4e3a\u8fdb\u884c\u81ea\u9002\u5e94\u5b66\u4e60\u3002\u6a21\u578b\u4f1a\u6839\u636e\u5f53\u524d\u5019\u9009\u5e7f\u544a\uff0c\u52a8\u6001\u5730\u8ba1\u7b97\u7528\u6237\u5386\u53f2\u884c\u4e3a\u7684\u76f8\u5173\u6027\u6743\u91cd\uff0c\u4ece\u800c\u6fc0\u6d3b\u7528\u6237\u76f8\u5173\u7684\u5174\u8da3\uff0c\u6355\u6349\u7528\u6237\u591a\u6837\u5316\u7684\u5174\u8da3\u7231\u597d\u3002\u521b\u65b0\u6027\u5730\u5c06\u6ce8\u610f\u529b\u673a\u5236\u5f15\u5165\u63a8\u8350\u7cfb\u7edf\uff0c\u5f00\u521b\u4e86\u884c\u4e3a\u5e8f\u5217\u5efa\u6a21\u7684\u65b0\u8303\u5f0f\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>features</code> (list): \u57fa\u7840\u7279\u5f81\u5217\u8868</li> <li><code>behavior_features</code> (list): \u884c\u4e3a\u7279\u5f81\u5217\u8868\uff0c\u7528\u4e8e\u6ce8\u610f\u529b\u8ba1\u7b97</li> <li><code>attention_units</code> (list): \u6ce8\u610f\u529b\u7f51\u7edc\u9690\u85cf\u5c42\u5355\u5143\u6570</li> <li><code>hidden_units</code> (list): DNN\u90e8\u5206\u9690\u85cf\u5c42\u5355\u5143\u6570</li> <li><code>activation</code> (str): \u6fc0\u6d3b\u51fd\u6570\u7c7b\u578b</li> </ul>"},{"location":"zh/api-reference/models/#dien-deep-interest-evolution-network","title":"DIEN (Deep Interest Evolution Network)","text":"<ul> <li>\u7b80\u4ecb\uff1aDIN\u7684\u8fdb\u9636\u7248\u672c\uff0c\u901a\u8fc7\u5f15\u5165\u5174\u8da3\u8fdb\u5316\u5c42\u6765\u5efa\u6a21\u7528\u6237\u5174\u8da3\u7684\u52a8\u6001\u53d8\u5316\u8fc7\u7a0b\u3002\u4f7f\u7528GRU\u7ed3\u6784\u6355\u6349\u7528\u6237\u5174\u8da3\u7684\u6f14\u53d8\uff0c\u5e76\u521b\u65b0\u6027\u5730\u8bbe\u8ba1\u4e86AUGRU\uff08GRU with Attentional Update Gate\uff09\u6765\u8ba9\u5174\u8da3\u8fdb\u5316\u8fc7\u7a0b\u611f\u77e5\u76ee\u6807\u7269\u54c1\u3002\u6b64\u5916\u8fd8\u5305\u542b\u8f85\u52a9\u635f\u5931\u6765\u76d1\u7763\u5174\u8da3\u62bd\u53d6\u5c42\u7684\u8bad\u7ec3\u3002\u8fd9\u79cd\u8bbe\u8ba1\u4e0d\u4ec5\u80fd\u6355\u6349\u7528\u6237\u5174\u8da3\u7684\u52a8\u6001\u53d8\u5316\uff0c\u8fd8\u80fd\u5efa\u6a21\u5174\u8da3\u7684\u65f6\u5e8f\u4f9d\u8d56\u5173\u7cfb\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>features</code> (list): \u57fa\u7840\u7279\u5f81\u5217\u8868</li> <li><code>behavior_features</code> (list): \u884c\u4e3a\u7279\u5f81\u5217\u8868</li> <li><code>interest_units</code> (list): \u5174\u8da3\u62bd\u53d6\u5c42\u5355\u5143\u6570</li> <li><code>gru_type</code> (str): GRU\u7c7b\u578b\uff0c\"AUGRU\"\u6216\"AIGRU\"</li> <li><code>hidden_units</code> (list): DNN\u90e8\u5206\u9690\u85cf\u5c42\u5355\u5143\u6570</li> </ul>"},{"location":"zh/api-reference/models/#bst-behavior-sequence-transformer","title":"BST (Behavior Sequence Transformer)","text":"<ul> <li>\u7b80\u4ecb\uff1a\u5c06Transformer\u67b6\u6784\u5f15\u5165\u63a8\u8350\u7cfb\u7edf\u7684\u5f00\u521b\u6027\u5de5\u4f5c\uff0c\u7528\u4e8e\u5efa\u6a21\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u3002\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6a21\u578b\u53ef\u4ee5\u76f4\u63a5\u8ba1\u7b97\u884c\u4e3a\u5e8f\u5217\u4e2d\u4efb\u610f\u4e24\u4e2a\u884c\u4e3a\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u514b\u670d\u4e86RNN\u7c7b\u6a21\u578b\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u7684\u5c40\u9650\u3002Position embedding\u5e2e\u52a9\u6a21\u578b\u611f\u77e5\u884c\u4e3a\u7684\u65f6\u5e8f\u4fe1\u606f\uff0c\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5219\u8ba9\u6a21\u578b\u80fd\u591f\u4ece\u591a\u4e2a\u89d2\u5ea6\u7406\u89e3\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>features</code> (list): \u57fa\u7840\u7279\u5f81\u5217\u8868</li> <li><code>behavior_features</code> (list): \u884c\u4e3a\u7279\u5f81\u5217\u8868</li> <li><code>num_heads</code> (int): \u6ce8\u610f\u529b\u5934\u6570</li> <li><code>num_layers</code> (int): Transformer\u5c42\u6570</li> <li><code>hidden_size</code> (int): \u9690\u85cf\u5c42\u7ef4\u5ea6</li> <li><code>dropout_rate</code> (float): Dropout\u6bd4\u7387</li> </ul>"},{"location":"zh/api-reference/models/#edcn-enhancing-explicit-and-implicit-feature-interactions","title":"EDCN (Enhancing Explicit and Implicit Feature Interactions)","text":"<ul> <li>\u7b80\u4ecb\uff1a\u589e\u5f3a\u663e\u5f0f\u548c\u9690\u5f0f\u7279\u5f81\u4ea4\u4e92\u7684\u6df1\u5ea6\u4ea4\u53c9\u7f51\u7edc\u3002\u901a\u8fc7\u8bbe\u8ba1\u65b0\u7684\u4ea4\u53c9\u7f51\u7edc\u7ed3\u6784\uff0c\u540c\u65f6\u8003\u8651\u7279\u5f81\u7684\u663e\u5f0f\u548c\u9690\u5f0f\u4ea4\u4e92\u3002\u5f15\u5165\u95e8\u63a7\u673a\u5236\u6765\u8c03\u63a7\u4e0d\u540c\u9636\u7279\u5f81\u4ea4\u4e92\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4f7f\u7528\u6b8b\u5dee\u8fde\u63a5\u6765\u7f13\u89e3\u6df1\u5c42\u7f51\u7edc\u7684\u8bad\u7ec3\u95ee\u9898\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>features</code> (list): \u7279\u5f81\u5217\u8868</li> <li><code>cross_num</code> (int): \u4ea4\u53c9\u5c42\u6570\u91cf</li> <li><code>hidden_units</code> (list): DNN\u90e8\u5206\u9690\u85cf\u5c42\u5355\u5143\u6570</li> <li><code>gate_type</code> (str): \u95e8\u63a7\u7c7b\u578b\uff0c\"FGU\"\u6216\"BGU\"</li> </ul>"},{"location":"zh/api-reference/models/#multi-task-models","title":"\u591a\u4efb\u52a1\u6a21\u578b (Multi-task Models)","text":"<p>\u591a\u4efb\u52a1\u6a21\u578b\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u591a\u4e2a\u76f8\u5173\u4efb\u52a1\uff0c\u5b9e\u73b0\u77e5\u8bc6\u5171\u4eab\u548c\u8fc1\u79fb\uff0c\u63d0\u9ad8\u6a21\u578b\u6574\u4f53\u6027\u80fd\u3002</p>"},{"location":"zh/api-reference/models/#sharedbottom","title":"SharedBottom","text":"<ul> <li>\u7b80\u4ecb\uff1a\u6700\u57fa\u7840\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u5e95\u5c42\u7f51\u7edc\u5171\u4eab\u53c2\u6570\u6765\u63d0\u53d6\u901a\u7528\u7279\u5f81\u8868\u793a\u3002\u5171\u4eab\u5c42\u5b66\u4e60\u4efb\u52a1\u95f4\u7684\u5171\u6027\u7279\u5f81\uff0c\u800c\u4efb\u52a1\u7279\u5b9a\u5c42\u5219\u8d1f\u8d23\u5b66\u4e60\u6bcf\u4e2a\u4efb\u52a1\u7684\u4e2a\u6027\u5316\u7279\u5f81\u3002\u8fd9\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u7ed3\u6784\u4e3a\u591a\u4efb\u52a1\u5b66\u4e60\u5960\u5b9a\u4e86\u57fa\u7840\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>features</code> (list): \u7279\u5f81\u5217\u8868</li> <li><code>hidden_units</code> (list): \u5171\u4eab\u7f51\u7edc\u9690\u85cf\u5c42\u5355\u5143\u6570</li> <li><code>task_hidden_units</code> (list): \u4efb\u52a1\u7279\u5b9a\u7f51\u7edc\u9690\u85cf\u5c42\u5355\u5143\u6570</li> <li><code>num_tasks</code> (int): \u4efb\u52a1\u6570\u91cf</li> <li><code>task_types</code> (list): \u4efb\u52a1\u7c7b\u578b\u5217\u8868</li> </ul>"},{"location":"zh/api-reference/models/#esmm-entire-space-multi-task-model","title":"ESMM (Entire Space Multi-Task Model)","text":"<ul> <li>\u7b80\u4ecb\uff1a\u963f\u91cc\u5df4\u5df4\u63d0\u51fa\u7684\u521b\u65b0\u591a\u4efb\u52a1\u6a21\u578b\uff0c\u4e13\u95e8\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6837\u672c\u9009\u62e9\u504f\u5dee\u95ee\u9898\u3002\u901a\u8fc7CVR\u548cCTR\u4efb\u52a1\u7684\u8054\u5408\u5efa\u6a21\uff0c\u5728\u5b8c\u6574\u7a7a\u95f4\u4e0a\u8fdb\u884c\u53c2\u6570\u5b66\u4e60\u3002\u6a21\u578b\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5f15\u5165\u4e86CTR\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u4efb\u52a1\u95f4\u7684\u4e58\u79ef\u5173\u7cfb\u4f18\u5316CVR\u9884\u4f30\u3002\u8fd9\u79cd\u8bbe\u8ba1\u4e0d\u4ec5\u89e3\u51b3\u4e86\u4f20\u7edfCVR\u9884\u4f30\u4e2d\u7684\u6837\u672c\u9009\u62e9\u504f\u5dee\uff0c\u8fd8\u63d0\u4f9b\u4e86\u65e0\u504f\u7684CTR\u548cCTCVR\u9884\u4f30\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>features</code> (list): \u7279\u5f81\u5217\u8868</li> <li><code>hidden_units</code> (list): \u9690\u85cf\u5c42\u5355\u5143\u6570\u5217\u8868</li> <li><code>tower_units</code> (list): \u4efb\u52a1\u5854\u5c42\u5355\u5143\u6570\u5217\u8868</li> <li><code>embedding_dim</code> (int): \u7279\u5f81\u5d4c\u5165\u7ef4\u5ea6</li> </ul>"},{"location":"zh/api-reference/models/#mmoe-multi-gate-mixture-of-experts","title":"MMoE (Multi-gate Mixture-of-Experts)","text":"<ul> <li>\u7b80\u4ecb\uff1aGoogle\u63d0\u51fa\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u4e13\u5bb6\u673a\u5236\u548c\u4efb\u52a1\u76f8\u5173\u7684\u95e8\u63a7\u7f51\u7edc\u6765\u5b9e\u73b0\u4efb\u52a1\u95f4\u7684\u8f6f\u53c2\u6570\u5171\u4eab\u3002\u6bcf\u4e2a\u4e13\u5bb6\u7f51\u7edc\u53ef\u4ee5\u5b66\u4e60\u7279\u5b9a\u7684\u7279\u5f81\u8f6c\u6362\uff0c\u800c\u95e8\u63a7\u7f51\u7edc\u5219\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u52a8\u6001\u5206\u914d\u4e13\u5bb6\u7684\u91cd\u8981\u6027\u3002\u8fd9\u79cd\u8bbe\u8ba1\u8ba9\u6a21\u578b\u80fd\u591f\u6839\u636e\u4efb\u52a1\u7684\u9700\u6c42\u7075\u6d3b\u5730\u7ec4\u5408\u4e13\u5bb6\u77e5\u8bc6\uff0c\u6709\u6548\u5904\u7406\u4efb\u52a1\u95f4\u7684\u5dee\u5f02\u6027\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>features</code> (list): \u7279\u5f81\u5217\u8868</li> <li><code>expert_units</code> (list): \u4e13\u5bb6\u7f51\u7edc\u9690\u85cf\u5c42\u5355\u5143\u6570</li> <li><code>num_experts</code> (int): \u4e13\u5bb6\u6570\u91cf</li> <li><code>num_tasks</code> (int): \u4efb\u52a1\u6570\u91cf</li> <li><code>expert_activation</code> (str): \u4e13\u5bb6\u7f51\u7edc\u6fc0\u6d3b\u51fd\u6570</li> <li><code>gate_activation</code> (str): \u95e8\u63a7\u7f51\u7edc\u6fc0\u6d3b\u51fd\u6570</li> </ul>"},{"location":"zh/api-reference/models/#ple-progressive-layered-extraction","title":"PLE (Progressive Layered Extraction)","text":"<ul> <li>\u7b80\u4ecb\uff1a\u5bf9MMoE\u7684\u6539\u8fdb\u7248\u672c\uff0c\u901a\u8fc7\u5206\u5c42\u63d0\u53d6\u7684\u65b9\u5f0f\u66f4\u597d\u5730\u5efa\u6a21\u4efb\u52a1\u95f4\u7684\u5173\u7cfb\u3002\u5f15\u5165\u4e86\u4efb\u52a1\u7279\u5b9a\u4e13\u5bb6\u548c\u5171\u4eab\u4e13\u5bb6\u7684\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u591a\u5c42\u7ea7\u7684\u4e13\u5bb6\u7f51\u7edc\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u7279\u5f81\u63d0\u53d6\u3002\u6bcf\u4e00\u5c42\u90fd\u5305\u542b\u7279\u5b9a\u4efb\u52a1\u7684\u4e13\u5bb6\u548c\u5171\u4eab\u4e13\u5bb6\uff0c\u8ba9\u6a21\u578b\u80fd\u591f\u540c\u65f6\u5b66\u4e60\u4efb\u52a1\u7684\u5171\u6027\u548c\u4e2a\u6027\u3002\u8fd9\u79cd\u6e10\u8fdb\u5f0f\u7684\u8bbe\u8ba1\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u77e5\u8bc6\u63d0\u53d6\u548c\u8fc1\u79fb\u7684\u80fd\u529b\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>features</code> (list): \u7279\u5f81\u5217\u8868</li> <li><code>expert_units</code> (list): \u4e13\u5bb6\u7f51\u7edc\u5355\u5143\u6570</li> <li><code>num_experts</code> (int): \u6bcf\u5c42\u4e13\u5bb6\u6570\u91cf</li> <li><code>num_layers</code> (int): \u5c42\u6570</li> <li><code>num_shared_experts</code> (int): \u5171\u4eab\u4e13\u5bb6\u6570\u91cf</li> <li><code>task_types</code> (list): \u4efb\u52a1\u7c7b\u578b\u5217\u8868</li> </ul>"},{"location":"zh/api-reference/trainers/","title":"\u8bad\u7ec3\u5668 API \u53c2\u8003","text":"<p>\u8fd9\u91cc\u8be6\u7ec6\u4ecb\u7ecd Torch-RecHub \u4e2d\u5404\u4e2a\u8bad\u7ec3\u5668\u7684 API \u63a5\u53e3\u548c\u53c2\u6570\u8bf4\u660e\u3002</p>"},{"location":"zh/api-reference/trainers/#ctrtrainer","title":"CTRTrainer","text":"<p>CTRTrainer \u662f\u4e00\u4e2a\u7528\u4e8e\u5355\u4efb\u52a1\u5b66\u4e60\u7684\u901a\u7528\u8bad\u7ec3\u5668\uff0c\u4e3b\u8981\u7528\u4e8e\u70b9\u51fb\u7387\u9884\u6d4b\uff08CTR\uff09\u7b49\u4e8c\u5206\u7c7b\u4efb\u52a1\u3002</p>"},{"location":"zh/api-reference/trainers/#_1","title":"\u53c2\u6570\u8bf4\u660e","text":"<ul> <li><code>model</code> (nn.Module): \u4efb\u4f55\u5355\u4efb\u52a1\u5b66\u4e60\u6a21\u578b</li> <li><code>optimizer_fn</code> (torch.optim): PyTorch\u4f18\u5316\u5668\u51fd\u6570\uff0c\u9ed8\u8ba4\u4e3a <code>torch.optim.Adam</code></li> <li><code>optimizer_params</code> (dict): \u4f18\u5316\u5668\u53c2\u6570\uff0c\u9ed8\u8ba4\u4e3a <code>{\"lr\": 1e-3, \"weight_decay\": 1e-5}</code></li> <li><code>scheduler_fn</code> (torch.optim.lr_scheduler): PyTorch\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\uff0c\u4f8b\u5982 <code>torch.optim.lr_scheduler.StepLR</code></li> <li><code>scheduler_params</code> (dict): \u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u53c2\u6570</li> <li><code>n_epoch</code> (int): \u8bad\u7ec3\u8f6e\u6570</li> <li><code>earlystop_patience</code> (int): \u65e9\u505c\u8010\u5fc3\u503c\uff0c\u5373\u5728\u9a8c\u8bc1\u96c6\u6027\u80fd\u591a\u5c11\u8f6e\u672a\u63d0\u5347\u540e\u505c\u6b62\u8bad\u7ec3\uff0c\u9ed8\u8ba4\u4e3a10</li> <li><code>device</code> (str): \u4f7f\u7528\u7684\u8bbe\u5907\uff0c\u53ef\u9009 <code>\"cpu\"</code> \u6216 <code>\"cuda:0\"</code></li> <li><code>gpus</code> (list): \u591aGPU ID\u5217\u8868\uff0c\u9ed8\u8ba4\u4e3a\u7a7a\u3002\u5982\u679c\u957f\u5ea6&gt;=1\uff0c\u6a21\u578b\u5c06\u88ab nn.DataParallel \u5305\u88c5</li> <li><code>loss_mode</code> (bool): \u8bad\u7ec3\u6a21\u5f0f\uff0c\u9ed8\u8ba4\u4e3aTrue</li> <li><code>model_path</code> (str): \u6a21\u578b\u4fdd\u5b58\u8def\u5f84\uff0c\u9ed8\u8ba4\u4e3a <code>\"./\"</code></li> </ul>"},{"location":"zh/api-reference/trainers/#_2","title":"\u4e3b\u8981\u65b9\u6cd5","text":"<ul> <li><code>train_one_epoch(data_loader, log_interval=10)</code>: \u8bad\u7ec3\u4e00\u4e2aepoch</li> <li><code>fit(train_dataloader, val_dataloader=None)</code>: \u8bad\u7ec3\u6a21\u578b</li> <li><code>evaluate(model, data_loader)</code>: \u8bc4\u4f30\u6a21\u578b</li> <li><code>predict(model, data_loader)</code>: \u6a21\u578b\u9884\u6d4b</li> </ul>"},{"location":"zh/api-reference/trainers/#matchtrainer","title":"MatchTrainer","text":"<p>MatchTrainer \u662f\u4e00\u4e2a\u7528\u4e8e\u5339\u914d/\u68c0\u7d22\u4efb\u52a1\u7684\u8bad\u7ec3\u5668\uff0c\u652f\u6301\u591a\u79cd\u8bad\u7ec3\u6a21\u5f0f\u3002</p>"},{"location":"zh/api-reference/trainers/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"<ul> <li><code>model</code> (nn.Module): \u4efb\u4f55\u5339\u914d\u6a21\u578b</li> <li><code>mode</code> (int): \u8bad\u7ec3\u6a21\u5f0f\uff0c\u53ef\u9009\u503c\uff1a</li> <li>0: \u9010\u70b9\u5f0f(point-wise)</li> <li>1: \u6210\u5bf9\u5f0f(pair-wise)</li> <li>2: \u5217\u8868\u5f0f(list-wise)</li> <li><code>optimizer_fn</code> (torch.optim): \u540cCTRTrainer</li> <li><code>optimizer_params</code> (dict): \u540cCTRTrainer</li> <li><code>scheduler_fn</code> (torch.optim.lr_scheduler): \u540cCTRTrainer</li> <li><code>scheduler_params</code> (dict): \u540cCTRTrainer</li> <li><code>n_epoch</code> (int): \u540cCTRTrainer</li> <li><code>earlystop_patience</code> (int): \u540cCTRTrainer</li> <li><code>device</code> (str): \u540cCTRTrainer</li> <li><code>gpus</code> (list): \u540cCTRTrainer</li> <li><code>model_path</code> (str): \u540cCTRTrainer</li> </ul>"},{"location":"zh/api-reference/trainers/#_4","title":"\u4e3b\u8981\u65b9\u6cd5","text":"<ul> <li><code>train_one_epoch(data_loader, log_interval=10)</code>: \u8bad\u7ec3\u4e00\u4e2aepoch</li> <li><code>fit(train_dataloader, val_dataloader=None)</code>: \u8bad\u7ec3\u6a21\u578b</li> <li><code>evaluate(model, data_loader)</code>: \u8bc4\u4f30\u6a21\u578b</li> <li><code>predict(model, data_loader)</code>: \u6a21\u578b\u9884\u6d4b</li> <li><code>inference_embedding(model, mode, data_loader, model_path)</code>: \u63a8\u7406\u5d4c\u5165\u5411\u91cf</li> <li><code>mode</code>: \u53ef\u9009 \"user\" \u6216 \"item\"</li> </ul>"},{"location":"zh/api-reference/trainers/#mtltrainer","title":"MTLTrainer","text":"<p>MTLTrainer \u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u8bad\u7ec3\u5668\uff0c\u652f\u6301\u591a\u79cd\u81ea\u9002\u5e94\u635f\u5931\u6743\u91cd\u65b9\u6cd5\u3002</p>"},{"location":"zh/api-reference/trainers/#_5","title":"\u53c2\u6570\u8bf4\u660e","text":"<ul> <li><code>model</code> (nn.Module): \u4efb\u4f55\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u578b</li> <li><code>task_types</code> (list): \u4efb\u52a1\u7c7b\u578b\u5217\u8868\uff0c\u652f\u6301 [\"classification\", \"regression\"]</li> <li><code>optimizer_fn</code> (torch.optim): \u540cCTRTrainer</li> <li><code>optimizer_params</code> (dict): \u540cCTRTrainer</li> <li><code>scheduler_fn</code> (torch.optim.lr_scheduler): \u540cCTRTrainer</li> <li><code>scheduler_params</code> (dict): \u540cCTRTrainer</li> <li><code>adaptive_params</code> (dict): \u81ea\u9002\u5e94\u635f\u5931\u6743\u91cd\u65b9\u6cd5\u53c2\u6570\uff0c\u652f\u6301\uff1a</li> <li><code>{\"method\": \"uwl\"}</code>: \u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u635f\u5931</li> <li><code>{\"method\": \"metabalance\"}</code>: \u5143\u5e73\u8861\u65b9\u6cd5</li> <li><code>{\"method\": \"gradnorm\", \"alpha\": 0.16}</code>: \u68af\u5ea6\u5f52\u4e00\u5316\u65b9\u6cd5</li> <li><code>n_epoch</code> (int): \u540cCTRTrainer</li> <li><code>earlystop_taskid</code> (int): \u7528\u4e8e\u65e9\u505c\u7684\u4efb\u52a1ID\uff0c\u9ed8\u8ba4\u4e3a0</li> <li><code>earlystop_patience</code> (int): \u540cCTRTrainer</li> <li><code>device</code> (str): \u540cCTRTrainer</li> <li><code>gpus</code> (list): \u540cCTRTrainer</li> <li><code>model_path</code> (str): \u540cCTRTrainer</li> </ul>"},{"location":"zh/api-reference/trainers/#_6","title":"\u4e3b\u8981\u65b9\u6cd5","text":"<ul> <li><code>train_one_epoch(data_loader)</code>: \u8bad\u7ec3\u4e00\u4e2aepoch</li> <li><code>fit(train_dataloader, val_dataloader, mode='base', seed=0)</code>: \u8bad\u7ec3\u6a21\u578b</li> <li><code>evaluate(model, data_loader)</code>: \u8bc4\u4f30\u6a21\u578b</li> <li><code>predict(model, data_loader)</code>: \u6a21\u578b\u9884\u6d4b</li> </ul>"},{"location":"zh/api-reference/trainers/#_7","title":"\u7279\u6b8a\u529f\u80fd","text":"<ol> <li>\u652f\u6301\u591a\u79cd\u81ea\u9002\u5e94\u635f\u5931\u6743\u91cd\u65b9\u6cd5\uff1a</li> <li>UWL (Uncertainty Weighted Loss)</li> <li>MetaBalance</li> <li> <p>GradNorm</p> </li> <li> <p>\u652f\u6301\u591a\u4efb\u52a1\u65e9\u505c\uff1a</p> </li> <li>\u53ef\u4ee5\u57fa\u4e8e\u6307\u5b9a\u4efb\u52a1\u7684\u6027\u80fd\u8fdb\u884c\u65e9\u505c</li> <li> <p>\u4fdd\u5b58\u9a8c\u8bc1\u96c6\u4e0a\u6700\u4f73\u6027\u80fd\u7684\u6a21\u578b\u6743\u91cd</p> </li> <li> <p>\u652f\u6301\u591a\u79cd\u4efb\u52a1\u7c7b\u578b\u7ec4\u5408\uff1a</p> </li> <li>\u5206\u7c7b\u4efb\u52a1</li> <li> <p>\u56de\u5f52\u4efb\u52a1</p> </li> <li> <p>\u8bad\u7ec3\u65e5\u5fd7\u8bb0\u5f55\uff1a</p> </li> <li>\u8bb0\u5f55\u6bcf\u4e2a\u4efb\u52a1\u7684\u635f\u5931</li> <li>\u8bb0\u5f55\u635f\u5931\u6743\u91cd\uff08\u5f53\u4f7f\u7528\u81ea\u9002\u5e94\u65b9\u6cd5\u65f6\uff09</li> <li>\u8bb0\u5f55\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6027\u80fd\u6307\u6807</li> </ol>"},{"location":"zh/api-reference/utils/","title":"\u5de5\u5177\u7c7b API \u53c2\u8003","text":"<p>\u672c\u6587\u6863\u8be6\u7ec6\u4ecb\u7ecd Torch-RecHub \u4e2d\u5404\u4e2a\u5de5\u5177\u7c7b\u7684 API \u63a5\u53e3\u548c\u53c2\u6570\u8bf4\u660e\u3002</p>"},{"location":"zh/api-reference/utils/#datapy","title":"\u6570\u636e\u5904\u7406\u5de5\u5177 (data.py)","text":""},{"location":"zh/api-reference/utils/#_1","title":"\u6570\u636e\u96c6\u7c7b","text":""},{"location":"zh/api-reference/utils/#torchdataset","title":"TorchDataset","text":"<ul> <li>\u7b80\u4ecb\uff1aPyTorch\u6570\u636e\u96c6\u7684\u57fa\u7840\u5b9e\u73b0\uff0c\u7528\u4e8e\u5904\u7406\u7279\u5f81\u548c\u6807\u7b7e\u6570\u636e\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>x</code> (dict): \u7279\u5f81\u5b57\u5178\uff0c\u952e\u4e3a\u7279\u5f81\u540d\uff0c\u503c\u4e3a\u7279\u5f81\u6570\u636e</li> <li><code>y</code> (array): \u6807\u7b7e\u6570\u636e</li> </ul>"},{"location":"zh/api-reference/utils/#predictdataset","title":"PredictDataset","text":"<ul> <li>\u7b80\u4ecb\uff1a\u7528\u4e8e\u9884\u6d4b\u9636\u6bb5\u7684\u6570\u636e\u96c6\u7c7b\uff0c\u53ea\u5305\u542b\u7279\u5f81\u6570\u636e\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>x</code> (dict): \u7279\u5f81\u5b57\u5178\uff0c\u952e\u4e3a\u7279\u5f81\u540d\uff0c\u503c\u4e3a\u7279\u5f81\u6570\u636e</li> </ul>"},{"location":"zh/api-reference/utils/#matchdatagenerator","title":"MatchDataGenerator","text":"<ul> <li>\u7b80\u4ecb\uff1a\u53ec\u56de\u4efb\u52a1\u7684\u6570\u636e\u751f\u6210\u5668\uff0c\u7528\u4e8e\u751f\u6210\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u52a0\u8f7d\u5668\u3002</li> <li>\u4e3b\u8981\u65b9\u6cd5\uff1a</li> <li><code>generate_dataloader(x_test_user, x_all_item, batch_size, num_workers=8)</code>: \u751f\u6210\u8bad\u7ec3\u3001\u6d4b\u8bd5\u548c\u7269\u54c1\u6570\u636e\u52a0\u8f7d\u5668</li> <li>\u53c2\u6570\uff1a<ul> <li><code>x_test_user</code> (dict): \u6d4b\u8bd5\u7528\u6237\u7279\u5f81</li> <li><code>x_all_item</code> (dict): \u6240\u6709\u7269\u54c1\u7279\u5f81</li> <li><code>batch_size</code> (int): \u6279\u6b21\u5927\u5c0f</li> <li><code>num_workers</code> (int): \u6570\u636e\u52a0\u8f7d\u7684\u5de5\u4f5c\u8fdb\u7a0b\u6570</li> </ul> </li> </ul>"},{"location":"zh/api-reference/utils/#datagenerator","title":"DataGenerator","text":"<ul> <li>\u7b80\u4ecb\uff1a\u901a\u7528\u6570\u636e\u751f\u6210\u5668\uff0c\u652f\u6301\u6570\u636e\u96c6\u7684\u5212\u5206\u548c\u52a0\u8f7d\u3002</li> <li>\u4e3b\u8981\u65b9\u6cd5\uff1a</li> <li><code>generate_dataloader(x_val=None, y_val=None, x_test=None, y_test=None, split_ratio=None, batch_size=16, num_workers=0)</code>: \u751f\u6210\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u6570\u636e\u52a0\u8f7d\u5668</li> <li>\u53c2\u6570\uff1a<ul> <li><code>x_val</code>, <code>y_val</code>: \u9a8c\u8bc1\u96c6\u7279\u5f81\u548c\u6807\u7b7e</li> <li><code>x_test</code>, <code>y_test</code>: \u6d4b\u8bd5\u96c6\u7279\u5f81\u548c\u6807\u7b7e</li> <li><code>split_ratio</code> (list): \u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u96c6\u3001\u6d4b\u8bd5\u96c6\u7684\u5212\u5206\u6bd4\u4f8b</li> <li><code>batch_size</code> (int): \u6279\u6b21\u5927\u5c0f</li> <li><code>num_workers</code> (int): \u6570\u636e\u52a0\u8f7d\u7684\u5de5\u4f5c\u8fdb\u7a0b\u6570</li> </ul> </li> </ul>"},{"location":"zh/api-reference/utils/#_2","title":"\u5de5\u5177\u51fd\u6570","text":""},{"location":"zh/api-reference/utils/#get_auto_embedding_dim","title":"get_auto_embedding_dim","text":"<ul> <li>\u7b80\u4ecb\uff1a\u6839\u636e\u7c7b\u522b\u6570\u81ea\u52a8\u8ba1\u7b97\u5d4c\u5165\u5411\u91cf\u7ef4\u5ea6\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>num_classes</code> (int): \u7c7b\u522b\u6570\u91cf</li> <li>\u8fd4\u56de\uff1a</li> <li>int: \u5d4c\u5165\u5411\u91cf\u7ef4\u5ea6\uff0c\u8ba1\u7b97\u516c\u5f0f\uff1a<code>[6 * (num_classes)^(1/4)]</code></li> </ul>"},{"location":"zh/api-reference/utils/#get_loss_func","title":"get_loss_func","text":"<ul> <li>\u7b80\u4ecb\uff1a\u83b7\u53d6\u635f\u5931\u51fd\u6570\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>task_type</code> (str): \u4efb\u52a1\u7c7b\u578b\uff0c\"classification\"\u6216\"regression\"</li> <li>\u8fd4\u56de\uff1a</li> <li>torch.nn.Module: \u5bf9\u5e94\u7684\u635f\u5931\u51fd\u6570</li> </ul>"},{"location":"zh/api-reference/utils/#get_metric_func","title":"get_metric_func","text":"<ul> <li>\u7b80\u4ecb\uff1a\u83b7\u53d6\u8bc4\u4f30\u6307\u6807\u51fd\u6570\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>task_type</code> (str): \u4efb\u52a1\u7c7b\u578b\uff0c\"classification\"\u6216\"regression\"</li> <li>\u8fd4\u56de\uff1a</li> <li>function: \u5bf9\u5e94\u7684\u8bc4\u4f30\u6307\u6807\u51fd\u6570</li> </ul>"},{"location":"zh/api-reference/utils/#generate_seq_feature","title":"generate_seq_feature","text":"<ul> <li>\u7b80\u4ecb\uff1a\u751f\u6210\u5e8f\u5217\u7279\u5f81\u548c\u8d1f\u6837\u672c\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>data</code> (pd.DataFrame): \u539f\u59cb\u6570\u636e</li> <li><code>user_col</code> (str): \u7528\u6237ID\u5217\u540d</li> <li><code>item_col</code> (str): \u7269\u54c1ID\u5217\u540d</li> <li><code>time_col</code> (str): \u65f6\u95f4\u6233\u5217\u540d</li> <li><code>item_attribute_cols</code> (list): \u9700\u8981\u751f\u6210\u5e8f\u5217\u7279\u5f81\u7684\u7269\u54c1\u5c5e\u6027\u5217</li> <li><code>min_item</code> (int): \u7528\u6237\u6700\u5c11\u4ea4\u4e92\u7269\u54c1\u6570</li> <li><code>shuffle</code> (bool): \u662f\u5426\u6253\u4e71\u6570\u636e</li> <li><code>max_len</code> (int): \u5e8f\u5217\u6700\u5927\u957f\u5ea6</li> </ul>"},{"location":"zh/api-reference/utils/#matchpy","title":"\u53ec\u56de\u5de5\u5177 (match.py)","text":""},{"location":"zh/api-reference/utils/#_3","title":"\u6570\u636e\u5904\u7406\u51fd\u6570","text":""},{"location":"zh/api-reference/utils/#gen_model_input","title":"gen_model_input","text":"<ul> <li>\u7b80\u4ecb\uff1a\u5408\u5e76\u7528\u6237\u548c\u7269\u54c1\u7279\u5f81\uff0c\u5904\u7406\u5e8f\u5217\u7279\u5f81\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>df</code> (pd.DataFrame): \u5e26\u6709\u5386\u53f2\u5e8f\u5217\u7279\u5f81\u7684\u6570\u636e</li> <li><code>user_profile</code> (pd.DataFrame): \u7528\u6237\u7279\u5f81\u6570\u636e</li> <li><code>user_col</code> (str): \u7528\u6237\u5217\u540d</li> <li><code>item_profile</code> (pd.DataFrame): \u7269\u54c1\u7279\u5f81\u6570\u636e</li> <li><code>item_col</code> (str): \u7269\u54c1\u5217\u540d</li> <li><code>seq_max_len</code> (int): \u5e8f\u5217\u6700\u5927\u957f\u5ea6</li> <li><code>padding</code> (str): \u586b\u5145\u65b9\u5f0f\uff0c'pre'\u6216'post'</li> <li><code>truncating</code> (str): \u622a\u65ad\u65b9\u5f0f\uff0c'pre'\u6216'post'</li> </ul>"},{"location":"zh/api-reference/utils/#negative_sample","title":"negative_sample","text":"<ul> <li>\u7b80\u4ecb\uff1a\u53ec\u56de\u6a21\u578b\u7684\u8d1f\u91c7\u6837\u65b9\u6cd5\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>items_cnt_order</code> (dict): \u7269\u54c1\u8ba1\u6570\u5b57\u5178\uff0c\u6309\u8ba1\u6570\u964d\u5e8f\u6392\u5e8f</li> <li><code>ratio</code> (int): \u8d1f\u6837\u672c\u6bd4\u4f8b</li> <li><code>method_id</code> (int): \u91c7\u6837\u65b9\u6cd5ID<ul> <li>0: \u968f\u673a\u91c7\u6837</li> <li>1: Word2Vec\u5f0f\u6d41\u884c\u5ea6\u91c7\u6837</li> <li>2: \u5bf9\u6570\u6d41\u884c\u5ea6\u91c7\u6837</li> <li>3: \u817e\u8bafRALM\u91c7\u6837</li> </ul> </li> </ul>"},{"location":"zh/api-reference/utils/#_4","title":"\u5411\u91cf\u68c0\u7d22\u7c7b","text":""},{"location":"zh/api-reference/utils/#annoy","title":"Annoy","text":"<ul> <li>\u7b80\u4ecb\uff1a\u57fa\u4e8eAnnoy\u7684\u5411\u91cf\u53ec\u56de\u5de5\u5177\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>metric</code> (str): \u8ddd\u79bb\u5ea6\u91cf\u65b9\u5f0f</li> <li><code>n_trees</code> (int): \u6811\u7684\u6570\u91cf</li> <li><code>search_k</code> (int): \u641c\u7d22\u53c2\u6570</li> <li>\u4e3b\u8981\u65b9\u6cd5\uff1a</li> <li><code>fit(X)</code>: \u6784\u5efa\u7d22\u5f15</li> <li><code>query(v, n)</code>: \u67e5\u8be2\u6700\u8fd1\u90bb</li> </ul>"},{"location":"zh/api-reference/utils/#milvus","title":"Milvus","text":"<ul> <li>\u7b80\u4ecb\uff1a\u57fa\u4e8eMilvus\u7684\u5411\u91cf\u53ec\u56de\u5de5\u5177\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>dim</code> (int): \u5411\u91cf\u7ef4\u5ea6</li> <li><code>host</code> (str): Milvus\u670d\u52a1\u5668\u5730\u5740</li> <li><code>port</code> (str): Milvus\u670d\u52a1\u5668\u7aef\u53e3</li> <li>\u4e3b\u8981\u65b9\u6cd5\uff1a</li> <li><code>fit(X)</code>: \u6784\u5efa\u7d22\u5f15</li> <li><code>query(v, n)</code>: \u67e5\u8be2\u6700\u8fd1\u90bb</li> </ul>"},{"location":"zh/api-reference/utils/#mtlpy","title":"\u591a\u4efb\u52a1\u5b66\u4e60\u5de5\u5177 (mtl.py)","text":""},{"location":"zh/api-reference/utils/#_5","title":"\u5de5\u5177\u51fd\u6570","text":""},{"location":"zh/api-reference/utils/#shared_task_layers","title":"shared_task_layers","text":"<ul> <li>\u7b80\u4ecb\uff1a\u83b7\u53d6\u591a\u4efb\u52a1\u6a21\u578b\u4e2d\u7684\u5171\u4eab\u5c42\u548c\u4efb\u52a1\u7279\u5b9a\u5c42\u53c2\u6570\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>model</code> (torch.nn.Module): \u591a\u4efb\u52a1\u6a21\u578b\uff0c\u652f\u6301MMOE\u3001SharedBottom\u3001PLE\u3001AITM</li> <li>\u8fd4\u56de\uff1a</li> <li>list: \u5171\u4eab\u5c42\u53c2\u6570\u5217\u8868</li> <li>list: \u4efb\u52a1\u7279\u5b9a\u5c42\u53c2\u6570\u5217\u8868</li> </ul>"},{"location":"zh/api-reference/utils/#_6","title":"\u4f18\u5316\u5668\u7c7b","text":""},{"location":"zh/api-reference/utils/#metabalance","title":"MetaBalance","text":"<ul> <li>\u7b80\u4ecb\uff1aMetaBalance\u4f18\u5316\u5668\uff0c\u7528\u4e8e\u5e73\u8861\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u5404\u4efb\u52a1\u7684\u68af\u5ea6\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>parameters</code> (list): \u6a21\u578b\u53c2\u6570</li> <li><code>relax_factor</code> (float): \u68af\u5ea6\u7f29\u653e\u7684\u677e\u5f1b\u56e0\u5b50\uff0c\u9ed8\u8ba40.7</li> <li><code>beta</code> (float): \u79fb\u52a8\u5e73\u5747\u7cfb\u6570\uff0c\u9ed8\u8ba40.9</li> <li>\u4e3b\u8981\u65b9\u6cd5\uff1a</li> <li><code>step(losses)</code>: \u6267\u884c\u4f18\u5316\u6b65\u9aa4\uff0c\u66f4\u65b0\u53c2\u6570</li> </ul>"},{"location":"zh/api-reference/utils/#_7","title":"\u68af\u5ea6\u5904\u7406\u51fd\u6570","text":""},{"location":"zh/api-reference/utils/#gradnorm","title":"gradnorm","text":"<ul> <li>\u7b80\u4ecb\uff1a\u5b9e\u73b0GradNorm\u7b97\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u8c03\u6574\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u4efb\u52a1\u6743\u91cd\u3002</li> <li>\u53c2\u6570\uff1a</li> <li><code>loss_list</code> (list): \u5404\u4efb\u52a1\u7684\u635f\u5931\u5217\u8868</li> <li><code>loss_weight</code> (list): \u4efb\u52a1\u6743\u91cd\u5217\u8868</li> <li><code>share_layer</code> (torch.nn.Parameter): \u5171\u4eab\u5c42\u53c2\u6570</li> <li><code>initial_task_loss</code> (list): \u521d\u59cb\u4efb\u52a1\u635f\u5931\u5217\u8868</li> <li><code>alpha</code> (float): GradNorm\u7b97\u6cd5\u7684\u8d85\u53c2\u6570</li> </ul>"},{"location":"zh/blog/match/","title":"\u53ec\u56de\u535a\u5ba2","text":""},{"location":"zh/blog/match/#loss-3","title":"\u4e00\u3001\u4e71\u4e03\u516b\u7cdf\u7684Loss\uff1f\u2014\u2014 3\u79cd\u8bad\u7ec3\u65b9\u5f0f","text":"<p>\u53ec\u56de\u4e2d\uff0c\u4e00\u822c\u7684\u8bad\u7ec3\u65b9\u5f0f\u5206\u4e3a\u4e09\u79cd\uff1apoint-wise\u3001pair-wise\u3001list-wise\u3002\u5728RecHub\u4e2d\uff0c\u7528\u53c2\u6570mode\u6765\u6307\u5b9a\u8bad\u7ec3\u65b9\u5f0f\uff0c\u6bcf\u4e00\u79cd\u4e0d\u540c\u7684\u8bad\u7ec3\u65b9\u5f0f\u4e5f\u5bf9\u5e94\u4e0d\u540c\u7684Loss\u3002</p>"},{"location":"zh/blog/match/#11-point-wise-mode-0","title":"1.1 Point wise (mode = 0)","text":"<p>\ud83e\udd70\u6838\u5fc3\u601d\u60f3\uff1a\u5c06\u53ec\u56de\u89c6\u4e3a\u4e8c\u5206\u7c7b\u3002</p> <p>\u5bf9\u4e8e\u4e00\u4e2a\u53ec\u56de\u6a21\u578b\uff0c\u8f93\u5165\u4e8c\u5143\u7ec4\\\uff0c\u8f93\u51fa\\(P(User, Item)\\)\uff0c\u8868\u793aUser\u5bf9Item\u7684\u611f\u5174\u8da3\u7a0b\u5ea6\u3002 <p>\u8bad\u7ec3\u76ee\u6807\u4e3a\uff1a\u82e5\u7269\u54c1\u4e3a\u6b63\u6837\u672c\uff0c\u8f93\u51fa\u5e94\u5c3d\u53ef\u80fd\u63a5\u8fd11\uff0c\u8d1f\u6837\u672c\u5219\u8f93\u51fa\u5c3d\u53ef\u80fd\u63a5\u8fd10\u3002</p> <p>\u91c7\u7528\u7684Loss\u6700\u5e38\u89c1\u7684\u5c31\u662fBCELoss\uff08Binary Cross Entropy Loss\uff09\u3002</p>"},{"location":"zh/blog/match/#12-pair-wise-mode-1","title":"1.2 Pair wise (mode = 1)","text":"<p>\ud83d\ude1d\u6838\u5fc3\u601d\u60f3\uff1a\u7528\u6237\u5bf9\u6b63\u6837\u672c\u611f\u5174\u8da3\u7684\u7a0b\u5ea6\u5e94\u8be5\u5927\u4e8e\u8d1f\u6837\u672c\u3002</p> <p>\u5bf9\u4e8e\u4e00\u4e2a\u53ec\u56de\u6a21\u578b\uff0c\u8f93\u5165\u4e09\u5143\u7ec4\\&lt;User, ItemPositive, ItemNegative&gt;\uff0c\u8f93\u51fa\u5174\u8da3\u5f97\u5206\\(P(User, ItemPositive)\\)\uff0c\\(P(User, ItemNegative)\\)\uff0c\u8868\u793a\u7528\u6237\u5bf9\u6b63\u6837\u672c\u7269\u54c1\u548c\u8d1f\u6837\u672c\u7269\u54c1\u7684\u5174\u8da3\u5f97\u5206\u3002</p> <p>\u8bad\u7ec3\u76ee\u6807\u4e3a\uff1a\u6b63\u6837\u672c\u7684\u5174\u8da3\u5f97\u5206\u5e94\u5c3d\u53ef\u80fd\u5927\u4e8e\u8d1f\u6837\u672c\u7684\u5174\u8da3\u5f97\u5206\u3002</p> <p>\u6846\u67b6\u4e2d\u91c7\u7528\u7684Loss\u4e3aBPRLoss\uff08Bayes Personalized Ranking Loss\uff09\u3002Loss\u7684\u516c\u5f0f\u8fd9\u91cc\u653e\u4e00\u4e2a\u516c\u5f0f\uff0c\u8be6\u7ec6\u53ef\u4ee5\u53c2\u8003\u8fd9\u91cc\uff08\u94fe\u63a5\u91cc\u7684\u5185\u5bb9\u548c\u4e0b\u9762\u7684\u516c\u5f0f\u6709\u4e9b\u7ec6\u5fae\u7684\u5dee\u522b\uff0c\u4f46\u662f\u601d\u60f3\u662f\u4e00\u6837\u7684\uff09</p> \\[ Loss=\\frac{1}{N}\\sum^N\\ _{i=1}-log(sigmoid(pos\\_score - neg\\_score)) \\]"},{"location":"zh/blog/match/#13-list-wisemode-2","title":"1.3 List wise\uff08mode = 2\uff09","text":"<p>\ud83d\ude07\u6838\u5fc3\u601d\u60f3\uff1a\u7528\u6237\u5bf9\u6b63\u6837\u672c\u611f\u5174\u8da3\u7684\u7a0b\u5ea6\u5e94\u8be5\u5927\u4e8e\u8d1f\u6837\u672c \u3002</p> <p>\u55ef\uff1f\u600e\u4e48\u548cPair wise\u4e00\u6837\uff1f</p> <p>\u6ca1\u9519\uff01List wise\u7684\u8bad\u7ec3\u65b9\u5f0f\u7684\u601d\u60f3\u548cPair wise\u662f\u4e00\u6837\u7684\uff0c\u53ea\u4e0d\u8fc7\u5b9e\u73b0\u4e0a\u4e0d\u540c\u3002</p> <p>\u5bf9\u4e8e\u4e00\u4e2a\u53ec\u56de\u6a21\u578b\uff0c\u8f93\u5165N+2\u5143\u7ec4\\(&lt;User, ItemPositive, ItemNeg\\_1, ... , ItemNeg\\_N&gt;\\)\uff0c\u8f93\u51fa\u7528\u6237\u5bf91\u4e2a\u6b63\u6837\u672c\u548cN\u4e2a\u8d1f\u6837\u672c\u7684\u5174\u8da3\u5f97\u5206\u3002</p> <p>\u8bad\u7ec3\u76ee\u6807\u4e3a\uff1a\u5bf9\u6b63\u6837\u672c\u7684\u5174\u8da3\u5f97\u5206\u5e94\u8be5\u5c3d\u53ef\u80fd\u5927\u4e8e\u5176\u4ed6\u6240\u6709\u8d1f\u6837\u672c\u7684\u5174\u8da3\u5f97\u5206\u3002</p> <p>\u6846\u67b6\u4e2d\u91c7\u7528\u7684Loss\u4e3a\\(torch.nn.CrossEntropyLoss\\)\uff0c\u5373\u5bf9\u8f93\u51fa\u8fdb\u884cSoftmax\u5904\u7406\u540e\u53d6\u3002</p> <p>PS\uff1a\u8fd9\u91cc\u7684List wise\u65b9\u5f0f\u5bb9\u6613\u548cRanking\u4e2d\u7684List wise\u6df7\u6dc6\uff0c\u867d\u7136\u4e8c\u8005\u540d\u5b57\u4e00\u6837\uff0c\u4f46ranking\u7684List wise\u8003\u8651\u4e86\u6837\u672c\u4e4b\u95f4\u7684\u987a\u5e8f\u5173\u7cfb\u3002\u4f8b\u5982ranking\u4e2d\u4f1a\u8003\u8651MAP\u3001NDCP\u7b49\u8003\u8651\u987a\u5e8f\u7684\u6307\u6807\u4f5c\u4e3a\u8bc4\u4ef7\u6307\u6807\uff0c\u800cMatching\u4e2d\u7684List wise\u6ca1\u6709\u8003\u8651\u987a\u5e8f\u3002</p>"},{"location":"zh/blog/match/#3","title":"\u4e8c\u3001\u4e24\u4e2a\u5411\u91cf\u6709\u591a\u8fdc\uff1f\u2014\u2014 3 \u79cd\u8861\u91cf\u6307\u6807","text":"<p>\ud83e\udd14\u7ed9\u5b9a\u4e00\u4e2a\u7528\u6237\u5411\u91cf\u548c\u4e00\u4e2a\u7269\u54c1\u5411\u91cf\uff0c\u5982\u4f55\u8861\u91cf\u4ed6\u4eec\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\uff1f</p> <p>\u5148\u5b9a\u4e49\u7528\u6237\u5411\u91cf\\(user \\in \\mathcal R^D\\)\uff0c\\(item\\in \\mathcal R^D\\)\uff0cD\u8868\u793a\u7528\u6237\u5411\u91cf\u548c\u7269\u54c1\u5411\u91cf\u7684\u7ef4\u5ea6\u3002</p>"},{"location":"zh/blog/match/#21-cosine","title":"2.1 cosine","text":"<p>\u521d\u4e2d\u5b66\u8fc7\uff1a</p> \\[ cos(a,b)=\\frac{&lt;a,b&gt;}{|a|*|b|} \\] <p>\u8868\u793a\u4e24\u4e2a\u5411\u91cf\u7684\u5939\u89d2\uff0c\u4f1a\u8f93\u51fa\u4e00\u4e2a[-1, 1]\u4e4b\u95f4\u7684\u5b9e\u6570\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u4ee5\u6b64\u4f5c\u4e3a\u76f8\u4f3c\u5ea6\u7684\u8861\u91cf\u4f9d\u636e\uff1a\u4e24\u4e2a\u5411\u91cf\u4e4b\u95f4\u89d2\u5ea6\u8d8a\u5c0f\uff0c\u5c31\u8d8a\u76f8\u4f3c\u3002</p> <p>\u5728RecHub\u7684\u6240\u6709\u53cc\u5854\u6a21\u578b\u4e2d\uff0c\u8bad\u7ec3\u9636\u6bb5\u90fd\u662f\u8f93\u51fa\u7684cosine\u76f8\u4f3c\u5ea6\u3002</p>"},{"location":"zh/blog/match/#22-dot","title":"2.2 dot","text":"<p>\u5373\u5411\u91cf\u7684\u5185\u79ef\uff0c\u7528$ $\u8868\u793a\u5411\u91cfa\u3001b\u7684\u5185\u79ef\u3002 <p>\u4e00\u4e2a\u5f88\u7b80\u5355\u7684\u601d\u60f3\u662f\uff1a\u5982\u679c\u5c06a\u3001b \u5411\u91cfL2 normalize\uff0c\u5373 \\(\\tilde{a}=\\frac{a}{|a|}\\ ,\\tilde{b}=\\frac{b}{|b|}\\)\uff0c\u7136\u540e\u76f4\u63a5\u5c06 \\(\\tilde{a}\u3001\\tilde{b}\\)\u6c42\u5185\u79ef\uff0c\u5c31\u7b49\u4ef7\u4e8e\u4e8e \\(cos(a,b)\\)\u3002 \uff08\u5f88\u5bb9\u6613\uff0c\u8fd9\u91cc\u5c31\u4e0d\u63a8\u5bfc\u4e86\uff09</p> <p>\u5b9e\u9645\u4e0a\uff0cRecHub\u4e2d\u7684\u6240\u6709\u53cc\u5854\u6a21\u578b\u5c31\u662f\u8fd9\u4e48\u505a\u7684\uff0c\u5148\u8ba1\u7b97User Embedding\u548cItem Embedding\uff0c\u7136\u540e\u5206\u522b\u5c06\u5176\u8fdb\u884cL2 Norm\uff0c\u518d\u8ba1\u7b97\u5185\u79ef\uff0c\u5f97\u5230cosine\u76f8\u4f3c\u5ea6\u3002\u8fd9\u6837\u53ef\u4ee5\u63d0\u5347\u6a21\u578b \u9a8c\u8bc1\u63a8\u7406 \u7684\u901f\u5ea6\u3002</p>"},{"location":"zh/blog/match/#23-euclidian-distance","title":"2.3 Euclidian Distance\uff08\u6b27\u6c0f\u8ddd\u79bb\uff09","text":"<p>\u6b27\u6c0f\u8ddd\u79bb\u5373\u6211\u4eec\u751f\u6d3b\u4e2d\u201c\u8ddd\u79bb\u201d\u7684\u542b\u4e49\u3002</p> <p>\ud83d\ude4b\u7ecf\u8fc7L2 Norm\u540e\u7684\u5411\u91cfa,b\uff0c\u6700\u5927\u5316\u5176cosine\u76f8\u4f3c\u5ea6\u4e0e\u6700\u5c0f\u5316\u5176\u6b27\u6c0f\u8ddd\u79bb\uff0c\u662f\u7b49\u4ef7\u7684</p> <p>\u4e3a\u4ec0\u4e48\uff1f\u89c1\u4e0b\u9762\u516c\u5f0f\uff1a</p> \\[ \\begin{align*}   EuclidianDistance(a,b)^2 &amp;= \\sum_{i=1}^N(a_i-b_i)^2 \\\\     &amp;= \\sum_{i=1}^Na_i^2+\\sum_{i=1}^Nb_i^2-\\sum_{i=1}^N2*a_i*b_i\\\\     &amp;= 2-2*\\sum_{i=1}^Na_i*b_i \\\\     &amp;= 2*(1-cos(a,b)) \\end{align*} \\] <p>\u4e24\u70b9\u89e3\u91ca\u4e00\u4e0b\uff1a</p> <ol> <li>\u7b2c\u4e8c\u884c\u5230\u7b2c\u4e09\u884c\uff0c\\(\\sum\\ _{i=1}^N a\\_i^2=1\\)\u3002\u4e3a\u4ec0\u4e48\uff1f\u56e0\u4e3aa\u662fL2 Norm\u540e\u7684\u5411\u91cf\u3002b\u540c\u7406\u3002</li> <li>\u7b2c\u4e09\u884c\u5230\u7b2c\u56db\u884c\uff0c\\(\\sum_{i=1}^Na_i*b_i\\)\uff0c\u5373\u5411\u91cfa\u3001b\u7684\u5185\u79ef\uff0c\u56e0\u4e3aa\u3001b\u5df2\u7ecfL2 Norm\uff0c\u6240\u4ee5\u76f8\u5f53\u4e8ecos\u3002</li> </ol> <p>\u5728RecHub\u4e2d\uff0c\u9a8c\u8bc1\u9636\u6bb5\u91c7\u7528\u7684\u5c31\u662fannoy\u7684\u6b27\u6c0f\u8ddd\u79bb\u3002</p> <p>\ud83d\ude4b\u5c0f\u7ed3\uff1aL2 Norm\u540e\u7684\u4e24\u4e2a\u5411\u91cf\uff0cmax dot\u7b49\u4ef7\u4e8emax cosine\u7b49\u4ef7\u4e8emin EuclidianDistance</p>"},{"location":"zh/blog/match/#_1","title":"\u4e09\u3001 \u6e29\u5ea6\u7cfb\u6570\u6709\u591a\u70ed\uff1f","text":"<p>\u5728\u6b64\u4e4b\u524d\uff0c\u8bf7\u5148\u786e\u8ba4\u660e\u767dtorch.nn.CrossEntropyLoss\u4e2d\u90fd\u8fdb\u884c\u4e86\u4ec0\u4e48\u8fd0\u7b97\uff08LogSoftmax + NLLLoss\uff09\uff0c\u8fd9\u5bf9\u9605\u8bfb\u6e90\u7801\u4e5f\u662f\u5173\u952e\u7684\u4fe1\u606f\u3002\u8fd9\u91cc\u662f\u5b98\u65b9\u6587\u6863\u3002</p> <p>\u5047\u8bbe\u4e00\u4e2a\u573a\u666f\uff1a\u91c7\u7528List wise\u7684\u8bad\u7ec3\u65b9\u5f0f\uff0c1\u4e2a\u6b63\u6837\u672c\uff0c3\u4e2a\u8d1f\u6837\u672c\uff0ccosine\u76f8\u4f3c\u5ea6\u4f5c\u4e3a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u8861\u91cf\u6307\u6807\u3002</p> <p>\u5047\u8bbe\u6b64\u65f6\u6211\u7684\u6a21\u578b\u5b8c\u7f8e\u7684\u9884\u6d4b\u4e86\u4e00\u6761\u8bad\u7ec3\u6570\u636e\uff0c\u5373\u8f93\u51fa\u7684logits\u4e3a\uff081, -1, -1, -1\uff09\uff0c\u6309\u7406\u8bf4\u6211\u7684Loss\u5e94\u8be5\u4e3a0\uff0c\u81f3\u5c11\u5e94\u8be5\u5f88\u5c0f\u3002\u4f46\u6b64\u65f6\u5982\u679c\u91c7\u7528CrossEntropyLoss\uff0c\u5f97\u5230\u7684Loss\u662f\uff1a</p> \\[ -log(exp(1)/(exp(1)+exp(-1)*3))=0.341 \\] <p>\u4f46\u6b64\u65f6\u5982\u679c\u5bf9logits\u9664\u4e0a\u4e00\u4e2a\u6e29\u5ea6\u7cfb\u6570$temperature=0.2 $\uff0c\u5373logits\u4e3a\uff085, -5, -5, -5\uff09\uff0c\u7ecf\u8fc7CrossEntropyLoss\uff0c\u5f97\u5230\u7684Loss\u662f\uff1a</p> \\[ -log(exp(5)/(exp(5)+exp(-5)*3))=0.016 \\] <p>\u8fd9\u6837\u5c31\u4f1a\u5f97\u5230\u4e00\u4e2a\u5f88\u5c0f\u5230\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\u7684Loss\u4e86\u3002</p> <p>\u4e5f\u5c31\u662f\u8bf4\uff0c\u5bf9logits\u9664\u4e0a\u4e00\u4e2atemperature\u7684\u4f5c\u7528\u662f\u6269\u5927logits\u4e2d\u6bcf\u4e2a\u5143\u7d20\u4e2d\u7684\u4e0a\u4e0b\u9650\uff0c\u62c9\u56desoftmax\u8fd0\u7b97\u7684\u654f\u611f\u8303\u56f4 \u3002</p> <p>\u4e1a\u754c\u4e00\u822cL2 Norm\u4e0etemperature\u642d\u914d\u4f7f\u7528\u3002</p>"},{"location":"zh/tutorials/matching/","title":"\u53ec\u56de\u6a21\u578b\u6559\u7a0b","text":"<p>\u672c\u6559\u7a0b\u5c06\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 Torch-RecHub \u4e2d\u7684\u5404\u79cd\u53ec\u56de\u6a21\u578b\u3002\u6211\u4eec\u5c06\u4f7f\u7528 MovieLens \u6570\u636e\u96c6\u4f5c\u4e3a\u793a\u4f8b\u3002</p>"},{"location":"zh/tutorials/matching/#_2","title":"\u6570\u636e\u51c6\u5907","text":"<p>\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u51c6\u5907\u6570\u636e\u3002MovieLens \u6570\u636e\u96c6\u5305\u542b\u7528\u6237\u5bf9\u7535\u5f71\u7684\u8bc4\u5206\u4fe1\u606f\uff1a</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom rechub.utils import DataGenerator\nfrom rechub.models import *\nfrom rechub.trainers import *\n\n# \u52a0\u8f7d\u6570\u636e\ndf = pd.read_csv(\"movielens.csv\")\nuser_features = ['user_id', 'age', 'gender', 'occupation']\nitem_features = ['movie_id', 'genre', 'year']\n</code></pre>"},{"location":"zh/tutorials/matching/#dssm","title":"\u57fa\u7840\u53cc\u5854\u6a21\u578b (DSSM)","text":"<p>DSSM \u662f\u6700\u57fa\u7840\u7684\u53cc\u5854\u6a21\u578b\uff0c\u5206\u522b\u5bf9\u7528\u6237\u548c\u7269\u54c1\u8fdb\u884c\u5efa\u6a21\uff1a</p> <pre><code># \u6a21\u578b\u914d\u7f6e\nmodel = DSSM(user_features=user_features,\n             item_features=item_features,\n             hidden_units=[64, 32, 16],\n             dropout_rates=[0.1, 0.1, 0.1])\n\n# \u8bad\u7ec3\u914d\u7f6e\ntrainer = MatchTrainer(model=model,\n                      mode=0,  # point-wise \u8bad\u7ec3\n                      optimizer_params={'lr': 0.001},\n                      n_epochs=10)\n\n# \u8bad\u7ec3\u6a21\u578b\ntrainer.fit(train_dataloader, val_dataloader)\n</code></pre>"},{"location":"zh/tutorials/matching/#gru4rec","title":"\u5e8f\u5217\u63a8\u8350\u6a21\u578b (GRU4Rec)","text":"<p>GRU4Rec \u901a\u8fc7 GRU \u7f51\u7edc\u5efa\u6a21\u7528\u6237\u7684\u884c\u4e3a\u5e8f\u5217\uff1a</p> <pre><code># \u751f\u6210\u5e8f\u5217\u7279\u5f81\nseq_features = generate_seq_feature(df,\n                                  user_col='user_id',\n                                  item_col='movie_id',\n                                  time_col='timestamp',\n                                  item_attribute_cols=['genre'])\n\n# \u6a21\u578b\u914d\u7f6e\nmodel = GRU4Rec(item_num=item_num,\n                hidden_size=64,\n                num_layers=2,\n                dropout_rate=0.1)\n\n# \u8bad\u7ec3\u914d\u7f6e\ntrainer = MatchTrainer(model=model,\n                      mode=1,  # pair-wise \u8bad\u7ec3\n                      optimizer_params={'lr': 0.001},\n                      n_epochs=10)\n</code></pre>"},{"location":"zh/tutorials/matching/#mind","title":"\u591a\u5174\u8da3\u6a21\u578b (MIND)","text":"<p>MIND \u6a21\u578b\u53ef\u4ee5\u6355\u6349\u7528\u6237\u7684\u591a\u6837\u5316\u5174\u8da3\uff1a</p> <pre><code># \u6a21\u578b\u914d\u7f6e\nmodel = MIND(item_num=item_num,\n            num_interests=4,\n            hidden_size=64,\n            routing_iterations=3)\n\n# \u8bad\u7ec3\u914d\u7f6e\ntrainer = MatchTrainer(model=model,\n                      mode=2,  # list-wise \u8bad\u7ec3\n                      optimizer_params={'lr': 0.001},\n                      n_epochs=10)\n</code></pre>"},{"location":"zh/tutorials/matching/#_3","title":"\u6a21\u578b\u8bc4\u4f30","text":"<p>\u4f7f\u7528\u5e38\u89c1\u7684\u53ec\u56de\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\uff1a</p> <pre><code># \u8ba1\u7b97\u53ec\u56de\u7387\u548c\u547d\u4e2d\u7387\nrecall_score = evaluate_recall(model, test_dataloader, k=10)\nhit_rate = evaluate_hit_rate(model, test_dataloader, k=10)\nprint(f\"Recall@10: {recall_score:.4f}\")\nprint(f\"HitRate@10: {hit_rate:.4f}\")\n</code></pre>"},{"location":"zh/tutorials/matching/#_4","title":"\u5411\u91cf\u68c0\u7d22","text":"<p>\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u53ef\u4ee5\u7528\u4e8e\u751f\u6210\u7528\u6237\u548c\u7269\u54c1\u7684\u5411\u91cf\u8868\u793a\uff0c\u8fdb\u884c\u5feb\u901f\u68c0\u7d22\uff1a</p> <pre><code># \u4f7f\u7528 Annoy \u8fdb\u884c\u5411\u91cf\u68c0\u7d22\nfrom rechub.utils import Annoy\n\n# \u6784\u5efa\u7d22\u5f15\nitem_vectors = model.get_item_vectors()\nannoy = Annoy(metric='angular')\nannoy.fit(item_vectors)\n\n# \u67e5\u8be2\u76f8\u4f3c\u7269\u54c1\nuser_vector = model.get_user_vector(user_id=1)\nsimilar_items = annoy.query(user_vector, n=10)\n</code></pre>"},{"location":"zh/tutorials/matching/#_5","title":"\u9ad8\u7ea7\u6280\u5de7","text":"<ol> <li> <p>\u6e29\u5ea6\u7cfb\u6570\u8c03\u8282 <pre><code>trainer = MatchTrainer(model=model,\n                      temperature=0.2,  # \u6dfb\u52a0\u6e29\u5ea6\u7cfb\u6570\n                      mode=2)\n</code></pre></p> </li> <li> <p>\u8d1f\u6837\u672c\u91c7\u6837 <pre><code>from rechub.utils import negative_sample\n\nneg_samples = negative_sample(items_cnt_order,\n                            ratio=5,\n                            method_id=1)  # Word2Vec\u5f0f\u91c7\u6837\n</code></pre></p> </li> <li> <p>\u6a21\u578b\u4fdd\u5b58\u4e0e\u52a0\u8f7d <pre><code># \u4fdd\u5b58\u6a21\u578b\ntorch.save(model.state_dict(), 'model.pth')\n\n# \u52a0\u8f7d\u6a21\u578b\nmodel.load_state_dict(torch.load('model.pth'))\n</code></pre></p> </li> </ol>"},{"location":"zh/tutorials/matching/#_6","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ol> <li>\u9009\u62e9\u5408\u9002\u7684\u8bad\u7ec3\u6a21\u5f0f\uff08point-wise/pair-wise/list-wise\uff09</li> <li>\u6ce8\u610f\u5e8f\u5217\u7279\u5f81\u7684\u957f\u5ea6\u548c\u586b\u5145\u65b9\u5f0f</li> <li>\u6839\u636e\u5b9e\u9645\u573a\u666f\u8c03\u6574\u8d1f\u6837\u672c\u6bd4\u4f8b</li> <li>\u5408\u7406\u8bbe\u7f6e batch_size \u548c\u5b66\u4e60\u7387</li> <li>\u4f7f\u7528 L2 \u6b63\u5219\u5316\u9632\u6b62\u8fc7\u62df\u5408</li> </ol>"},{"location":"zh/tutorials/multi-task/","title":"\u591a\u4efb\u52a1\u5b66\u4e60\u6559\u7a0b","text":"<p>\u672c\u6559\u7a0b\u5c06\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 Torch-RecHub \u4e2d\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u578b\u3002\u6211\u4eec\u5c06\u4f7f\u7528\u963f\u91cc\u5df4\u5df4\u7684\u7535\u5546\u6570\u636e\u96c6\u4f5c\u4e3a\u793a\u4f8b\u3002</p>"},{"location":"zh/tutorials/multi-task/#_2","title":"\u6570\u636e\u51c6\u5907","text":"<p>\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u51c6\u5907\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u6570\u636e\uff1a</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom rechub.utils import DataGenerator\nfrom rechub.models import *\nfrom rechub.trainers import *\n\n# \u52a0\u8f7d\u6570\u636e\ndf = pd.read_csv(\"ali_ccp_data.csv\")\n\n# \u7279\u5f81\u5b9a\u4e49\nuser_features = ['user_id', 'age', 'gender', 'occupation']\nitem_features = ['item_id', 'category_id', 'shop_id', 'brand_id']\nfeatures = user_features + item_features\n\n# \u591a\u4efb\u52a1\u6807\u7b7e\ntasks = ['click', 'conversion']  # CTR \u548c CVR \u4efb\u52a1\n</code></pre>"},{"location":"zh/tutorials/multi-task/#sharedbottom","title":"SharedBottom \u6a21\u578b","text":"<p>\u6700\u57fa\u7840\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u578b\uff0c\u5e95\u5c42\u7f51\u7edc\u5171\u4eab\u53c2\u6570\uff1a</p> <pre><code># \u6a21\u578b\u914d\u7f6e\nmodel = SharedBottom(\n    features=features,\n    hidden_units=[256, 128],\n    task_hidden_units=[64, 32],\n    num_tasks=2,\n    task_types=['binary', 'binary'])\n\n# \u8bad\u7ec3\u914d\u7f6e\ntrainer = MTLTrainer(\n    model=model,\n    optimizer_params={'lr': 0.001},\n    n_epochs=10)\n\n# \u8bad\u7ec3\u6a21\u578b\ntrainer.fit(train_dataloader, val_dataloader)\n</code></pre>"},{"location":"zh/tutorials/multi-task/#esmm-entire-space-multi-task-model","title":"ESMM (Entire Space Multi-Task Model)","text":"<p>\u89e3\u51b3\u6837\u672c\u9009\u62e9\u504f\u5dee\u7684\u591a\u4efb\u52a1\u6a21\u578b\uff1a</p> <pre><code># \u6a21\u578b\u914d\u7f6e\nmodel = ESMM(\n    features=features,\n    hidden_units=[256, 128, 64],\n    tower_units=[32, 16],\n    embedding_dim=16)\n\n# \u8bad\u7ec3\u914d\u7f6e\ntrainer = MTLTrainer(\n    model=model,\n    optimizer_params={'lr': 0.001},\n    n_epochs=10)\n</code></pre>"},{"location":"zh/tutorials/multi-task/#mmoe-multi-gate-mixture-of-experts","title":"MMoE (Multi-gate Mixture-of-Experts)","text":"<p>\u901a\u8fc7\u4e13\u5bb6\u673a\u5236\u5b9e\u73b0\u4efb\u52a1\u95f4\u7684\u8f6f\u53c2\u6570\u5171\u4eab\uff1a</p> <pre><code># \u6a21\u578b\u914d\u7f6e\nmodel = MMoE(\n    features=features,\n    expert_units=[256, 128],\n    num_experts=8,\n    num_tasks=2,\n    expert_activation='relu',\n    gate_activation='softmax')\n\n# \u8bad\u7ec3\u914d\u7f6e\ntrainer = MTLTrainer(\n    model=model,\n    optimizer_params={'lr': 0.001},\n    n_epochs=10)\n</code></pre>"},{"location":"zh/tutorials/multi-task/#ple-progressive-layered-extraction","title":"PLE (Progressive Layered Extraction)","text":"<p>\u901a\u8fc7\u5206\u5c42\u63d0\u53d6\u66f4\u597d\u5730\u5efa\u6a21\u4efb\u52a1\u5173\u7cfb\uff1a</p> <pre><code># \u6a21\u578b\u914d\u7f6e\nmodel = PLE(\n    features=features,\n    expert_units=[256, 128],\n    num_experts=4,\n    num_layers=3,\n    num_shared_experts=2,\n    task_types=['binary', 'binary'])\n\n# \u8bad\u7ec3\u914d\u7f6e\ntrainer = MTLTrainer(\n    model=model,\n    optimizer_params={'lr': 0.001},\n    n_epochs=10)\n</code></pre>"},{"location":"zh/tutorials/multi-task/#_3","title":"\u4efb\u52a1\u6743\u91cd\u4f18\u5316","text":""},{"location":"zh/tutorials/multi-task/#gradnorm","title":"GradNorm","text":"<p>\u4f7f\u7528 GradNorm \u7b97\u6cd5\u52a8\u6001\u8c03\u6574\u4efb\u52a1\u6743\u91cd\uff1a</p> <pre><code># \u914d\u7f6e GradNorm\ntrainer = MTLTrainer(\n    model=model,\n    optimizer_params={'lr': 0.001},\n    task_weights_strategy='gradnorm',\n    gradnorm_alpha=1.5)\n</code></pre>"},{"location":"zh/tutorials/multi-task/#metabalance","title":"MetaBalance","text":"<p>\u4f7f\u7528 MetaBalance \u4f18\u5316\u5668\u5e73\u8861\u4efb\u52a1\u68af\u5ea6\uff1a</p> <pre><code>from rechub.utils import MetaBalance\n\n# \u914d\u7f6e MetaBalance \u4f18\u5316\u5668\noptimizer = MetaBalance(\n    model.parameters(),\n    relax_factor=0.7,\n    beta=0.9)\n\ntrainer = MTLTrainer(\n    model=model,\n    optimizer=optimizer)\n</code></pre>"},{"location":"zh/tutorials/multi-task/#_4","title":"\u6a21\u578b\u8bc4\u4f30","text":"<p>\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u4f7f\u7528\u76f8\u5e94\u7684\u8bc4\u4f30\u6307\u6807\uff1a</p> <pre><code># \u8bc4\u4f30\u6a21\u578b\nresults = evaluate_multi_task(model, test_dataloader)\nfor task, metrics in results.items():\n    print(f\"Task: {task}\")\n    print(f\"AUC: {metrics['auc']:.4f}\")\n    print(f\"LogLoss: {metrics['logloss']:.4f}\")\n</code></pre>"},{"location":"zh/tutorials/multi-task/#_5","title":"\u9ad8\u7ea7\u5e94\u7528","text":"<ol> <li> <p>\u81ea\u5b9a\u4e49\u4efb\u52a1\u635f\u5931\u6743\u91cd <pre><code>trainer = MTLTrainer(\n    model=model,\n    task_weights=[1.0, 0.5])  # \u8bbe\u7f6e\u56fa\u5b9a\u4efb\u52a1\u6743\u91cd\n</code></pre></p> </li> <li> <p>\u83b7\u53d6\u5171\u4eab\u5c42\u548c\u4efb\u52a1\u7279\u5b9a\u5c42 <pre><code>from rechub.utils import shared_task_layers\n\nshared_params, task_params = shared_task_layers(model)\n</code></pre></p> </li> <li> <p>\u4efb\u52a1\u7279\u5b9a\u7684\u5b66\u4e60\u7387 <pre><code>trainer = MTLTrainer(\n    model=model,\n    task_specific_lr={'click': 0.001, 'conversion': 0.0005})\n</code></pre></p> </li> </ol>"},{"location":"zh/tutorials/multi-task/#_6","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ol> <li>\u9009\u62e9\u5408\u9002\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u67b6\u6784</li> <li>\u6ce8\u610f\u4efb\u52a1\u4e4b\u95f4\u7684\u76f8\u5173\u6027</li> <li>\u5904\u7406\u4efb\u52a1\u95f4\u7684\u6570\u636e\u4e0d\u5e73\u8861</li> <li>\u5408\u7406\u8bbe\u7f6e\u4efb\u52a1\u6743\u91cd</li> <li>\u76d1\u63a7\u6bcf\u4e2a\u4efb\u52a1\u7684\u8bad\u7ec3\u8fdb\u5ea6</li> <li>\u9632\u6b62\u4efb\u52a1\u95f4\u7684\u8d1f\u8fc1\u79fb</li> <li>\u8003\u8651\u8ba1\u7b97\u8d44\u6e90\u7684\u9650\u5236</li> </ol>"},{"location":"zh/tutorials/ranking/","title":"\u6392\u5e8f\u6a21\u578b\u6559\u7a0b","text":"<p>\u672c\u6559\u7a0b\u5c06\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 Torch-RecHub \u4e2d\u7684\u5404\u79cd\u6392\u5e8f\u6a21\u578b\u3002\u6211\u4eec\u5c06\u4f7f\u7528 Criteo \u548c Avazu \u6570\u636e\u96c6\u4f5c\u4e3a\u793a\u4f8b\u3002</p>"},{"location":"zh/tutorials/ranking/#_2","title":"\u6570\u636e\u51c6\u5907","text":"<p>\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u51c6\u5907\u6570\u636e\u5e76\u8fdb\u884c\u7279\u5f81\u5904\u7406\uff1a</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom rechub.utils import DataGenerator\nfrom rechub.models import *\nfrom rechub.trainers import *\n\n# \u52a0\u8f7d\u6570\u636e\ndf = pd.read_csv(\"criteo_sample.csv\")\n\n# \u7279\u5f81\u5217\u5b9a\u4e49\nsparse_features = ['C1', 'C2', 'C3', ..., 'C26']\ndense_features = ['I1', 'I2', 'I3', ..., 'I13']\nfeatures = sparse_features + dense_features\n</code></pre>"},{"location":"zh/tutorials/ranking/#wide-deep","title":"Wide &amp; Deep \u6a21\u578b","text":"<p>Wide &amp; Deep \u6a21\u578b\u7ed3\u5408\u4e86\u8bb0\u5fc6\u548c\u6cdb\u5316\u80fd\u529b\uff1a</p> <pre><code># \u6a21\u578b\u914d\u7f6e\nmodel = WideDeep(\n    wide_features=sparse_features,\n    deep_features=features,\n    hidden_units=[256, 128, 64],\n    dropout_rates=[0.1, 0.1, 0.1])\n\n# \u8bad\u7ec3\u914d\u7f6e\ntrainer = Trainer(model=model,\n                 optimizer_params={'lr': 0.001},\n                 n_epochs=10,\n                 device='cuda:0')\n\n# \u8bad\u7ec3\u6a21\u578b\ntrainer.fit(train_dataloader, val_dataloader)\n</code></pre>"},{"location":"zh/tutorials/ranking/#deepfm","title":"DeepFM \u6a21\u578b","text":"<p>DeepFM \u6a21\u578b\u901a\u8fc7\u56e0\u5b50\u5206\u89e3\u673a\u548c\u6df1\u5ea6\u7f51\u7edc\u5efa\u6a21\u7279\u5f81\u4ea4\u4e92\uff1a</p> <pre><code># \u6a21\u578b\u914d\u7f6e\nmodel = DeepFM(\n    features=features,\n    hidden_units=[256, 128, 64],\n    dropout_rates=[0.1, 0.1, 0.1],\n    embedding_dim=16)\n\n# \u8bad\u7ec3\u914d\u7f6e\ntrainer = Trainer(model=model,\n                 optimizer_params={'lr': 0.001},\n                 n_epochs=10)\n</code></pre>"},{"location":"zh/tutorials/ranking/#din-deep-interest-network","title":"DIN (Deep Interest Network)","text":"<p>DIN \u6a21\u578b\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5efa\u6a21\u7528\u6237\u5174\u8da3\uff1a</p> <pre><code># \u751f\u6210\u884c\u4e3a\u5e8f\u5217\u7279\u5f81\nbehavior_features = ['item_id', 'category_id']\nseq_features = generate_seq_feature(df,\n                                  user_col='user_id',\n                                  item_col='item_id',\n                                  time_col='timestamp',\n                                  item_attribute_cols=['category_id'])\n\n# \u6a21\u578b\u914d\u7f6e\nmodel = DIN(\n    features=features,\n    behavior_features=behavior_features,\n    attention_units=[80, 40],\n    hidden_units=[256, 128, 64],\n    dropout_rate=0.1)\n\n# \u8bad\u7ec3\u914d\u7f6e\ntrainer = Trainer(model=model,\n                 optimizer_params={'lr': 0.001},\n                 n_epochs=10)\n</code></pre>"},{"location":"zh/tutorials/ranking/#dcn-v2","title":"DCN-V2 \u6a21\u578b","text":"<p>DCN-V2 \u901a\u8fc7\u4ea4\u53c9\u7f51\u7edc\u663e\u5f0f\u5efa\u6a21\u7279\u5f81\u4ea4\u4e92\uff1a</p> <pre><code># \u6a21\u578b\u914d\u7f6e\nmodel = DCNV2(\n    features=features,\n    cross_num=3,\n    hidden_units=[256, 128, 64],\n    dropout_rates=[0.1, 0.1, 0.1],\n    cross_parameterization='matrix')  # \u6216 'vector'\n\n# \u8bad\u7ec3\u914d\u7f6e\ntrainer = Trainer(model=model,\n                 optimizer_params={'lr': 0.001},\n                 n_epochs=10)\n</code></pre>"},{"location":"zh/tutorials/ranking/#_3","title":"\u6a21\u578b\u8bc4\u4f30","text":"<p>\u4f7f\u7528\u5e38\u89c1\u7684\u6392\u5e8f\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\uff1a</p> <pre><code># \u8bc4\u4f30\u6a21\u578b\nauc = evaluate_auc(model, test_dataloader)\nlog_loss = evaluate_logloss(model, test_dataloader)\nprint(f\"AUC: {auc:.4f}\")\nprint(f\"LogLoss: {log_loss:.4f}\")\n</code></pre>"},{"location":"zh/tutorials/ranking/#_4","title":"\u7279\u5f81\u5de5\u7a0b\u6280\u5de7","text":"<ol> <li> <p>\u7279\u5f81\u9884\u5904\u7406 <pre><code># \u7c7b\u522b\u7279\u5f81\u7f16\u7801\nfrom sklearn.preprocessing import LabelEncoder\nfor feat in sparse_features:\n    lbe = LabelEncoder()\n    df[feat] = lbe.fit_transform(df[feat])\n\n# \u6570\u503c\u7279\u5f81\u5f52\u4e00\u5316\nfrom sklearn.preprocessing import MinMaxScaler\nfor feat in dense_features:\n    scaler = MinMaxScaler()\n    df[feat] = scaler.fit_transform(df[feat].values.reshape(-1, 1))\n</code></pre></p> </li> <li> <p>\u7279\u5f81\u4ea4\u53c9 <pre><code># \u624b\u52a8\u7279\u5f81\u4ea4\u53c9\ndf['cross_feat'] = df['feat1'].astype(str) + '_' + df['feat2'].astype(str)\n</code></pre></p> </li> </ol>"},{"location":"zh/tutorials/ranking/#_5","title":"\u9ad8\u7ea7\u5e94\u7528","text":"<ol> <li> <p>\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570 <pre><code>class FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, pred, target):\n        # \u5b9e\u73b0 Focal Loss\n        pass\n\ntrainer = Trainer(model=model,\n                 loss_fn=FocalLoss(alpha=0.25, gamma=2))\n</code></pre></p> </li> <li> <p>\u5b66\u4e60\u7387\u8c03\u5ea6 <pre><code>from torch.optim.lr_scheduler import CosineAnnealingLR\n\ntrainer = Trainer(model=model,\n                 scheduler='cosine',  # \u4f7f\u7528\u4f59\u5f26\u9000\u706b\u8c03\u5ea6\n                 scheduler_params={'T_max': 10})\n</code></pre></p> </li> </ol>"},{"location":"zh/tutorials/ranking/#_6","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ol> <li>\u5408\u7406\u5904\u7406\u7f3a\u5931\u503c\u548c\u5f02\u5e38\u503c</li> <li>\u6ce8\u610f\u7279\u5f81\u5de5\u7a0b\u7684\u91cd\u8981\u6027</li> <li>\u9009\u62e9\u5408\u9002\u7684\u8bc4\u4f30\u6307\u6807</li> <li>\u5173\u6ce8\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027</li> <li>\u5e73\u8861\u6a21\u578b\u590d\u6742\u5ea6\u548c\u6548\u7387</li> <li>\u5904\u7406\u6837\u672c\u4e0d\u5e73\u8861\u95ee\u9898</li> </ol>"},{"location":"zh/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/","title":"\u53c2\u8003\u8d44\u6599","text":"<p>SIGIR 2020, SIGKDD 2020, RecSys 2020, CIKM 2020, AAAI 2021, WSDM 2021, WWW 2021, SIGIR 2021</p> SIGIR SIGKDD RecSys CIKM WSDM WWW ICDM AAAI IJCAI ICML/NIPS 2020 \u709c\u826f \u5b99\u946b \u5b99\u946b \u5b87\u5bb8 \u5947\u8fbe \u5947\u8fbe \u5b87\u5bb8 \u5947\u8fbe \u709c\u826f \u5b99\u946b 2021 2022 <p> SIGIR 2022 | \u63a8\u8350\u7cfb\u7edf\u76f8\u5173\u8bba\u6587\u5206\u7c7b\u6574\u7406 \u4f5c\u8005\uff1a @\u6797\u5b50\u6db5 \uff0c\u4e2d\u56fd\u4eba\u6c11\u5927\u5b66\u4fe1\u606f\u5b66\u9662\u7855\u58eb\u4e8c\u5e74\u7ea7\u5728\u8bfb\uff0c\u5bfc\u5e08\u4e3a\u8d75\u946b\u6559\u6388\uff0c\u7814\u7a76\u65b9\u5411\u4e3a\u63a8\u8350\u7cfb\u7edf \u5bfc\u8bfbACM SIGIR 2022\u662fCCF A\u7c7b\u4f1a\u8bae\uff0c\u4eba\u5de5\u667a\u80fd\u9886\u57df\u667a\u80fd\u4fe1\u606f\u68c0\u7d22\uff08 Information Retrieval\uff0cIR\uff09\u65b9\u5411\u6700\u6743\u5a01\u7684\u56fd\u9645\u4f1a\u8bae\u3002\u2026 https://zhuanlan.zhihu.com/p/500163155</p> <p> dblp: computer science bibliography The dblp computer science bibliography is the online reference for open bibliographic information on major computer science journals and proceedings. https://dblp.org/</p> <p> IJCAI2022\u6709\u54ea\u4e9b\u503c\u5f97\u5173\u6ce8\u7684\u8bba\u6587\uff1f - \u77e5\u4e4e IJCAI 2022\u5df2\u516c\u5e03\u5f55\u7528\u8bba\u6587:https://ijcai-22.org/main-track-accepted-papers/\u5bf9\u63a8\u8350\u7cfb\u7edf\u76f8\u5173\u8bba\u6587\u8fdb\u884c\u68b3\u2026 https://www.zhihu.com/question/528776724/answer/2515698081?utm_source=wechat_session\\&amp;utm_medium=social\\&amp;utm_oi=62371468935168\\&amp;utm_content=group3_Answer\\&amp;utm_campaign=shareopn</p> <p> Main Track Accepted papers  https://ijcai-21.org/program-main-track/</p>"},{"location":"","title":"Welcome to use Torch-RecHub","text":"Torch-RecHub <p>An easy-to-use, scalable, and high-performance recommendation system framework based on PyTorch</p>            Quick Start                     View GitHub"},{"location":"contributing/","title":"Contribution Guide","text":"<p>Thank you very much for your interest in the Torch-RecHub project and for considering contributing! Your help is crucial for the development of the project. This guide will detail how to participate in contributions.</p>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":"<p>You can contribute in the following ways:</p> <ol> <li>Report Bugs: If you find any errors or unexpected behavior while using Torch-RecHub, please submit an Issue detailing the problem, steps to reproduce it, and your environment information.</li> <li>Suggest Enhancements: If you have ideas for improving existing features or adding new ones, please create an Issue first to discuss.</li> <li>Submit Code Changes: Fix bugs, implement new features, or improve code quality.</li> <li>Improve Documentation: Enhance or correct documentation, write tutorials, or provide example use cases.</li> </ol>"},{"location":"contributing/#finding-contribution-points","title":"Finding Contribution Points","text":"<p>You can start from the following aspects:</p> <ol> <li>Check Issues: Browse the project Issues list, look for issues marked with <code>help wanted</code>.</li> <li>Improve Existing Features: If you find areas for optimization during use, feel free to propose suggestions or submit improvements directly.</li> <li>Implement New Features: If you have new ideas, it is recommended to create an Issue for discussion first to ensure it aligns with the project direction.</li> </ol>"},{"location":"contributing/#contribution-process-code-and-documentation","title":"Contribution Process (Code and Documentation)","text":"<p>We use the standard GitHub Fork &amp; Pull Request workflow to accept code and documentation contributions. (You can also perform the following operations on the GitHub website).</p> <ol> <li> <p>Fork the Repository     Visit the Torch-RecHub GitHub repository page, click the \"Fork\" button in the top right corner to copy the project to your own GitHub account.</p> </li> <li> <p>Clone Your Fork     Clone your forked repository locally:     <pre><code>git clone https://github.com/YOUR_USERNAME/torch-rechub.git\ncd torch-rechub\n</code></pre>     Please replace <code>YOUR_USERNAME</code> with your GitHub username.</p> </li> <li> <p>Set Upstream Remote (Optional but Recommended)     Add the original project repository as an upstream remote for easy synchronization of updates:     <pre><code>git remote add upstream https://github.com/datawhalechina/torch-rechub.git\n# Sync updates from upstream main branch if needed\n# git fetch upstream\n# git checkout main\n# git merge upstream/main\n</code></pre></p> </li> <li> <p>Make Changes     Write code, modify documentation, or make other improvements directly on the <code>main</code> branch.</p> </li> <li> <p>Ensure Code Quality</p> <ul> <li>Code Style: Please adhere to the project's existing code style.</li> <li>Documentation: For user-visible changes (like new features, API changes), please update relevant documentation (README, files under <code>docs/</code>, etc.).</li> </ul> </li> <li> <p>Commit Changes     Commit your changes with clear and meaningful commit messages. We recommend following the Conventional Commits specification.     <pre><code>git add .\ngit commit -m \"feat: Add support for XXX model\"\n# Or\ngit commit -m \"fix: Correct typo in README\"\n# Or\ngit commit -m \"docs: Update contribution guide\"\n</code></pre></p> </li> <li> <p>Push Branch     Push your local <code>main</code> branch to your forked GitHub repository:     <pre><code>git push origin main\n</code></pre></p> </li> <li> <p>Create a Pull Request (PR)     Return to your forked repository page on GitHub. You should see a prompt suggesting you create a Pull Request based on the recent pushes to <code>main</code>. Click that prompt or manually navigate to the \"Pull requests\" tab and click \"New pull request\".</p> <ul> <li>Choose Branches: Ensure the Base repository is <code>datawhalechina/torch-rechub</code>'s <code>main</code> branch, and the Head repository is your fork's <code>main</code> branch.</li> <li>Fill in PR Information:<ul> <li>Title: Concisely describe the purpose of the PR, often based on the commit message.</li> <li>Description: Detail the changes you've made, the problem solved (you can link related Issues, e.g., <code>Closes #123</code>), and any points reviewers should note. If the PR includes UI changes, please attach screenshots or screen recordings.</li> </ul> </li> <li>Allow Maintainer Edits (Optional): Checking \"Allow edits by maintainers\" often helps maintainers quickly fix minor issues.</li> <li>Submit PR: Click \"Create pull request\".</li> </ul> </li> </ol>"},{"location":"contributing/#pull-request-review","title":"Pull Request Review","text":"<ul> <li>After submitting the PR, the project's CI/CD workflow will automatically run tests and checks.</li> <li>Project maintainers will review your code and documentation and may suggest modifications.</li> <li>Please respond promptly to review comments and make necessary changes. Maintainers might make minor edits directly on your branch (if you allow it), or you might need to update the code yourself and push again.</li> <li>Once the PR is approved and passes all checks, maintainers will merge it into the main branch.</li> </ul> <p>Thank you again for your contribution to Torch-RecHub!</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>Common questions and troubleshooting guide for Torch-RecHub.</p> <ul> <li> <p>Will there be a TensorFlow version?</p> <ul> <li> <p>Not currently planned</p> </li> <li> <p>This project's core positioning is to provide easy-to-use model implementations for beginners, referencing industry-used models. PyTorch has a wider audience base.</p> </li> </ul> </li> <li> <p>Why is the AUC=0 when running the example?</p> <ul> <li>The example contains 100 sample data entries, which are meant to demonstrate data format and feature types, ensuring the code runs smoothly. It does not guarantee performance metrics.</li> <li>If you need to test performance, you can download the dataset using the links described in the readme, then refer to the parameter configuration file in the example for model training and evaluation.</li> </ul> </li> <li> <p>Installing annoy</p> <ul> <li> <p>Installing annoy on Windows</p> <ul> <li>Online installation</li> </ul> <pre><code>pip install annoy\n</code></pre> <p>If you don't have C++ related compilation environment on Windows, you might see the following error:</p> <pre><code>error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n---------------------------------------------------------------------\n</code></pre> <p>Error screenshot: </p> <p>In this case, you can use offline installation</p> <ul> <li>Offline installation</li> </ul> <p>Download annoy library from: https://www.lfd.uci.edu/~gohlke/pythonlibs/#_annoy</p> <pre><code>pip install annoy\u20111.17.0\u2011cp39\u2011cp39\u2011win_amd64.whl\n</code></pre> </li> <li> <p>Installing annoy on Linux/Mac OS</p> <ul> <li>Online installation</li> </ul> <pre><code>pip install annoy\n</code></pre> <p>Normally Mac can install successfully online. If online installation fails with nose-related errors at the bottom, proceed with offline compilation installation</p> <ul> <li> <p>Offline installation</p> </li> <li> <p>Download nose</p> <p>Download from: https://www.lfd.uci.edu/~gohlke/pythonlibs/#_annoy</p> <pre><code>pip install nose\u20111.3.7\u2011py3\u2011none\u2011any.whl\n</code></pre> </li> <li> <p>Download annoy</p> <p>Download from: https://files.pythonhosted.org/packages/a1/5b/1c22129f608b3f438713b91cd880dc681d747a860afe3e8e0af86e921942/annoy-1.17.0.tar.gz</p> <pre><code>tar -zxvf annoy-1.17.0.tar.gz\ncd annoy-1.17.0\npython setup.py install\n</code></pre> </li> </ul> </li> </ul> <p>After installing annoy, you can proceed to install torch-rechub</p> <pre><code>pip install --upgrade torch-rechub\n</code></pre> </li> </ul>"},{"location":"getting-started/","title":"Getting Started Tutorial","text":"<p>First, install Torch-RecHub:</p> <pre><code>pip install torch-rechub\n</code></pre> <p>Then use the following code to train recommender system models:</p>"},{"location":"getting-started/#ranking-ctr-prediction","title":"Ranking (CTR Prediction)","text":"<pre><code>from torch_rechub.models.ranking import DeepFM\nfrom torch_rechub.trainers import CTRTrainer\nfrom torch_rechub.utils.data import DataGenerator\n\ndg = DataGenerator(x, y)\ntrain_dl, val_dl, test_dl = dg.generate_dataloader(split_ratio=[0.7, 0.1], batch_size=256)\n\nmodel = DeepFM(deep_features=deep_features, fm_features=fm_features, \n               mlp_params={\"dims\": [256, 128], \"dropout\": 0.2, \"activation\": \"relu\"})\n\nctr_trainer = CTRTrainer(model)\nctr_trainer.fit(train_dl, val_dl)\nauc = ctr_trainer.evaluate(test_dl)\n</code></pre>"},{"location":"getting-started/#multi-task-learning","title":"Multi-Task Learning","text":"<pre><code>from torch_rechub.models.multi_task import SharedBottom, ESMM, MMOE, PLE, AITM\nfrom torch_rechub.trainers import MTLTrainer\n\ntask_types = [\"classification\", \"classification\"]\nmodel = MMOE(features, task_types, 8, \n            expert_params={\"dims\": [32,16]}, \n            tower_params_list=[{\"dims\": [32, 16]}, {\"dims\": [32, 16]}])\n\nmtl_trainer = MTLTrainer(model)\nmtl_trainer.fit(train_dl, val_dl)\n</code></pre>"},{"location":"getting-started/#matching-models","title":"Matching Models","text":"<pre><code>from torch_rechub.models.matching import DSSM\nfrom torch_rechub.trainers import MatchTrainer\nfrom torch_rechub.utils.data import MatchDataGenerator\n\ndg = MatchDataGenerator(x, y)\ntrain_dl, test_dl, item_dl = dg.generate_dataloader(test_user, all_item, batch_size=256)\n\nmodel = DSSM(user_features, item_features, temperature=0.02,\n             user_params={\"dims\": [256, 128, 64], \"activation\": 'prelu'},\n             item_params={\"dims\": [256, 128, 64], \"activation\": 'prelu'})\n\nmatch_trainer = MatchTrainer(model)\nmatch_trainer.fit(train_dl)\n</code></pre>"},{"location":"getting-started/#model-zoo","title":"Model Zoo","text":""},{"location":"getting-started/#model-list","title":"Model List","text":"Title Tag Development Status Developers Institution Meeting Year URL pdf DIN Rank,Sequence Completed \u8d56\u654f\u6750 Alibaba KDD 2018 https://arxiv.org/abs/1706.06978 1706.06978.pdf ESMM Rank Completed \u8d56\u654f\u6750 Alibaba SIGIR 2018 https://arxiv.org/abs/1804.07931 1804.07931.pdf Youtube-SBC Match Completed \u8d56\u654f\u6750 Google RecSys 2019 https://research.google/pubs/pub48840/ 6c8a86c981a62b0126a11896b7f6ae0dae4c3566.pdf DSSM Match Completed \u8d56\u654f\u6750 \u5fae\u8f6f CIKM 2013 https://posenhuang.github.io/papers/cikm2013_DSSM_fullversion.pdf cikm2013_DSSM_fullversion.pdf MetaBalance \u5176\u4ed6 Completed Facebook www 2022 https://arxiv.org/pdf/2203.06801v1.pdf 2203.06801v1-3.pdf Wide &amp; Deep Rank Completed \u8d56\u654f\u6750 Google DLRS 2016 https://arxiv.org/pdf/1606.07792.pdf 1606.07792.pdf DSSM-Facebook Match Completed \u8d56\u654f\u6750 Facebook KDD 2020 https://arxiv.org/abs/2006.11632 2006.11632.pdf DeepFM Rank Completed \u8d56\u654f\u6750 Huawei IJCAI 2017 https://arxiv.org/abs/1703.04247 1703.04247.pdf SasRec Match \u8fdb\u884c\u4e2d \u738b\u5b87\u5bb8 PLE Rank Completed \u8d56\u654f\u6750 Tencent RecSys 2020 https://dl.acm.org/doi/abs/10.1145/3383313.3412236?casa_token=4g_ErWbxWf8AAAAA%3APhbcdBa6b-SXHlpFtKh1Lybjtv48sYV2l1GsPeL43N5Lpih_GwarAwV5hzxOYUVZoWd8dimltm4czmI 2020 (Tencent) (Recsys) [PLE] Progressive Layered Extraction (PLE) - A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations.pdf AITM Rank Completed \u8d56\u654f\u6750 Meituan KDD 2021 https://arxiv.org/abs/2105.08489 2105.08489-2.pdf Shared-Bottom Rank Completed \u8d56\u654f\u6750 CMU ML 1997 https://link.springer.com/content/pdf/10.1023/A:1007379606734.pdf Caruana1997_Article_MultitaskLearning.pdf DCN Rank Completed \u8d56\u654f\u6750 Google,\u65af\u5766\u798f AKDD 2017 https://arxiv.org/abs/1708.05123 1708.05123.pdf Youtube-DNN Match Completed \u8d56\u654f\u6750 Google RecSys 2016 https://dl.acm.org/doi/10.1145/2959100.2959190 2959100.2959190.pdf MMOE Rank Completed \u8d56\u654f\u6750 Google KDD 2018 https://dl.acm.org/doi/pdf/10.1145/3219819.3220007 3219819.3220007.pdf GRU4Rec Match,Sequence Completed \u738b\u51ef Tencent KDD 2022 SASRec Match,Sequence Completed \u738b\u5b87\u5bb8 UC ICDM 2018 https://arxiv.org/pdf/1808.09781.pdf 1808.09781-3.pdf SINE Match Completed \u5eb7\u535a Alibaba WSDM 2021 https://arxiv.org/pdf/2102.09267.pdf 2102.09267.pdf (FAT-)DeepFFM Rank Completed \u5eb7\u535a Sina arXiv 2019 https://arxiv.org/pdf/1905.06336.pdf 1905.06336.pdf STAMP Match,Sequence Completed \u5eb7\u535a \u7535\u5b50\u79d1\u5927 KDD 2018 https://dl.acm.org/doi/10.1145/3219819.3219950 3219819.3219950.pdf NARM Match,Sequence Completed \u5eb7\u535a \u4eac\u4e1c,\u5c71\u4e1c\u5927\u5b66 CIKM 2017 https://arxiv.org/pdf/1711.04725.pdf 1711.00165.pdf DCN_v2 Rank Completed \u53f6\u5fd7\u96c4 Google www 2021 https://arxiv.org/abs/2008.13535 DCN V2 Improved Deep &amp; Cross Network and Practical Lessons.pdf EDCN Rank Completed \u53f6\u5fd7\u96c4 Huawei KDD 2021 https://dlp-kdd.github.io/assets/pdf/DLP-KDD_2021_paper_12.pdf FiBiNet Rank Completed \u53f6\u5fd7\u96c4 Sina RecSys 2019 https://dl.acm.org/doi/abs/10.1145/3298689.3347043 DIEN Rank,Sequence Completed \u8303\u6d9b Alibaba AAAI 2019 https://ojs.aaai.org/index.php/AAAI/article/view/4545 4545-Article Text-7584-1-10-20190706.pdf BST Rank,Sequence Completed \u8303\u6d9b Alibaba arXiv 2019 Behavior Sequence Transformer for E-commerce Recommendation in Alibaba pdf"},{"location":"installation/","title":"Installation Guide","text":"<p>This document provides detailed installation instructions for Torch-RecHub, including both stable and development versions.</p>"},{"location":"installation/#stable-release","title":"Stable Release","text":"<pre><code>pip install torch-rechub\n</code></pre>"},{"location":"installation/#latest-version-recommended","title":"Latest Version (Recommended)","text":"<pre><code>git clone https://github.com/datawhalechina/torch-rechub.git\ncd torch-rechub\npython setup.py install\n</code></pre>"},{"location":"introduction/","title":"Project Introduction","text":""},{"location":"introduction/#project-overview","title":"Project Overview","text":"<p>Torch-RecHub is a flexible and easily extensible recommendation system framework built using PyTorch. It aims to simplify the research and application of recommendation algorithms, providing common model implementations, data processing tools, and evaluation metrics.</p>"},{"location":"introduction/#features","title":"Features","text":"<ul> <li>Modular Design: Easy to add new models, datasets, and evaluation metrics.</li> <li>Based on PyTorch: Leverage PyTorch's dynamic graph and GPU acceleration capabilities.</li> <li>Rich Model Library: Includes various classic and cutting - edge recommendation algorithms (listed below).</li> <li>Standardized Process: Provide unified data loading, training, and evaluation processes.</li> <li>Easy to Configure: Easily adjust experimental settings through configuration files or command - line parameters.</li> <li>Reproducibility: Aims to ensure the reproducibility of experimental results.</li> <li>Easy to Extend: Decouple model training from model definition, without the concept of a base model.</li> <li>Native Functions: Use PyTorch's native classes and functions as much as possible without excessive customization.</li> <li>Concise Model Code: Facilitate beginners' learning while adhering to the ideas of academic papers.</li> <li>Other Features: For example, support negative sampling, multi - task learning, etc.</li> </ul>"},{"location":"introduction/#overall-architecture","title":"Overall Architecture","text":""},{"location":"introduction/#data-layer-design","title":"Data Layer Design","text":""},{"location":"introduction/#feature-classes","title":"Feature Classes","text":"<p>Numerical Features</p> <ul> <li>Such as age, salary, daily click - through rate, etc.</li> </ul> <p>Categorical Features</p> <ul> <li>Such as city, education level, gender, etc.</li> <li>Encode with LabelEncoder to obtain Embedding vectors.</li> </ul> <p>Sequence Features</p> <ul> <li>Ordered interest sequences: such as the item list clicked in the last week.</li> <li>Unordered tag features: such as movie genres (action | suspense | crime).</li> <li>Encode with LabelEncoder to obtain sequence Embedding vectors.</li> <li>Perform pooling to reduce dimensions.</li> <li>Preserve the sequence for model operations such as attention with other features.</li> <li>Share the Embedding Table with Sparse features.</li> </ul>"},{"location":"introduction/#data-classes","title":"Data Classes","text":"<ul> <li>Dataset</li> <li>Dataloader</li> </ul>"},{"location":"introduction/#tool-classes","title":"Tool Classes","text":"<ul> <li>Sequence feature generation</li> <li>Sample construction</li> <li>Negative sampling</li> <li>Vectorized retrieval</li> </ul>"},{"location":"introduction/#model-layer-design","title":"Model Layer Design","text":""},{"location":"introduction/#model-classes","title":"Model Classes","text":"<p>General Layers Shallow Feature Modeling</p> <ul> <li>LR: Logistic Regression</li> <li>MLP: Multi - Layer Perceptron, parameters such as dims can be set through a dictionary.</li> <li>EmbeddingLayer: A general Embedding layer that handles three types of features, maintains an EmbeddingTable in dictionary format, and outputs the input embeddings required by the model.</li> </ul> <p>Deep Feature Modeling</p> <ul> <li>FM, FFM, CIN</li> <li>self - attention, target - attention, transformer</li> </ul> <p>Custom Layers</p>"},{"location":"api-reference/basic/","title":"Basic Components API Reference","text":"<p>This document provides detailed documentation for basic components in Torch-RecHub, including feature processing, data transformation, and other fundamental functionalities.</p>"},{"location":"api-reference/basic/#feature-processing","title":"Feature Processing","text":""},{"location":"api-reference/basic/#feature-columns","title":"Feature Columns","text":""},{"location":"api-reference/basic/#densefeature","title":"DenseFeature","text":"<ul> <li>Introduction: Process continuous numerical features.</li> <li>Parameters:</li> <li><code>name</code> (str): Feature name</li> <li><code>dimension</code> (int): Feature dimension</li> <li><code>dtype</code> (str): Data type, default 'float32'</li> </ul>"},{"location":"api-reference/basic/#sparsefeature","title":"SparseFeature","text":"<ul> <li>Introduction: Process discrete categorical features.</li> <li>Parameters:</li> <li><code>name</code> (str): Feature name</li> <li><code>vocabulary_size</code> (int): Size of category vocabulary</li> <li><code>embedding_dim</code> (int): Embedding vector dimension</li> <li><code>dtype</code> (str): Data type, default 'int32'</li> <li><code>embedding_name</code> (str): Embedding layer name, default None</li> </ul>"},{"location":"api-reference/basic/#varlensparsefeature","title":"VarLenSparseFeature","text":"<ul> <li>Introduction: Process variable-length discrete features.</li> <li>Parameters:</li> <li><code>name</code> (str): Feature name</li> <li><code>vocabulary_size</code> (int): Size of category vocabulary</li> <li><code>embedding_dim</code> (int): Embedding vector dimension</li> <li><code>maxlen</code> (int): Maximum sequence length</li> <li><code>dtype</code> (str): Data type, default 'int32'</li> <li><code>embedding_name</code> (str): Embedding layer name, default None</li> <li><code>combiner</code> (str): Sequence pooling method, options: 'sum', 'mean', 'max', default 'mean'</li> </ul>"},{"location":"api-reference/basic/#data-transformation","title":"Data Transformation","text":""},{"location":"api-reference/basic/#data-preprocessing","title":"Data Preprocessing","text":""},{"location":"api-reference/basic/#minmaxscaler","title":"MinMaxScaler","text":"<ul> <li>Introduction: Normalize numerical features.</li> <li>Parameters:</li> <li><code>feature_range</code> (tuple): Normalization range, default (0, 1)</li> </ul>"},{"location":"api-reference/basic/#standardscaler","title":"StandardScaler","text":"<ul> <li>Introduction: Standardize numerical features.</li> <li>Parameters:</li> <li><code>with_mean</code> (bool): Whether to remove mean, default True</li> <li><code>with_std</code> (bool): Whether to scale by standard deviation, default True</li> </ul>"},{"location":"api-reference/basic/#labelencoder","title":"LabelEncoder","text":"<ul> <li>Introduction: Encode categorical features.</li> <li>Methods:</li> <li><code>fit(values)</code>: Fit the encoder</li> <li><code>transform(values)</code>: Transform data</li> <li><code>fit_transform(values)</code>: Fit and transform</li> </ul>"},{"location":"api-reference/basic/#data-format-conversion","title":"Data Format Conversion","text":""},{"location":"api-reference/basic/#pandas_to_torch","title":"pandas_to_torch","text":"<ul> <li>Introduction: Convert Pandas data to PyTorch tensors.</li> <li>Parameters:</li> <li><code>df</code> (pd.DataFrame): Input DataFrame</li> <li><code>dense_cols</code> (list): List of continuous feature column names</li> <li><code>sparse_cols</code> (list): List of discrete feature column names</li> <li><code>device</code> (str): Device type, 'cpu' or 'cuda'</li> </ul>"},{"location":"api-reference/basic/#numpy_to_torch","title":"numpy_to_torch","text":"<ul> <li>Introduction: Convert NumPy arrays to PyTorch tensors.</li> <li>Parameters:</li> <li><code>arrays</code> (list): List of NumPy arrays</li> <li><code>device</code> (str): Device type, 'cpu' or 'cuda'</li> </ul>"},{"location":"api-reference/basic/#model-components","title":"Model Components","text":""},{"location":"api-reference/basic/#activation-functions","title":"Activation Functions","text":""},{"location":"api-reference/basic/#dice","title":"Dice","text":"<ul> <li>Introduction: Dice activation function, proposed in Deep Interest Network (DIN).</li> <li>Parameters:</li> <li><code>epsilon</code> (float): Smoothing parameter, default 1e-3</li> <li><code>device</code> (str): Device type, default 'cpu'</li> </ul>"},{"location":"api-reference/basic/#attention-mechanisms","title":"Attention Mechanisms","text":""},{"location":"api-reference/basic/#scaleddotproductattention","title":"ScaledDotProductAttention","text":"<ul> <li>Introduction: Scaled dot-product attention mechanism.</li> <li>Parameters:</li> <li><code>temperature</code> (float): Temperature parameter for scaling</li> <li><code>attn_dropout</code> (float): Attention dropout rate</li> </ul>"},{"location":"api-reference/basic/#multiheadattention","title":"MultiHeadAttention","text":"<ul> <li>Introduction: Multi-head attention mechanism.</li> <li>Parameters:</li> <li><code>d_model</code> (int): Model dimension</li> <li><code>n_heads</code> (int): Number of attention heads</li> <li><code>d_k</code> (int): Key vector dimension</li> <li><code>d_v</code> (int): Value vector dimension</li> <li><code>dropout</code> (float): Dropout rate </li> </ul>"},{"location":"api-reference/models/","title":"Models API Reference","text":"<p>This section provides detailed API documentation for all models in Torch-RecHub.</p>"},{"location":"api-reference/models/#recall-models","title":"Recall Models","text":"<p>Recall models are primarily used in the recall stage for quick retrieval of relevant items from massive candidate sets. They typically adopt two-tower or sequential model structures to meet the efficiency requirements of the recall stage.</p>"},{"location":"api-reference/models/#two-tower-model-series","title":"Two-Tower Model Series","text":""},{"location":"api-reference/models/#dssm-deep-structured-semantic-model","title":"DSSM (Deep Structured Semantic Model)","text":"<ul> <li>Introduction: Originally proposed by Microsoft for semantic matching and later widely applied in recommender systems. Adopts classic two-tower structure that separately represents users and items, computing similarity through inner product. This structure allows pre-computation of item vectors during online serving, greatly improving service efficiency. The key lies in learning effective user and item representations.</li> <li>Parameters:</li> <li><code>user_features</code> (list): List of user features</li> <li><code>item_features</code> (list): List of item features</li> <li><code>hidden_units</code> (list): List of hidden layer units</li> <li><code>dropout_rates</code> (list): List of dropout rates</li> <li><code>embedding_dim</code> (int): Final representation vector dimension</li> </ul>"},{"location":"api-reference/models/#facebook-dssm","title":"Facebook DSSM","text":"<ul> <li>Introduction: Facebook's improved version of DSSM that incorporates multi-task learning framework. Besides the main recall task, it adds auxiliary tasks to help learn better feature representations. The model can simultaneously optimize multiple related objectives like clicks, favorites, purchases, etc., learning richer user and item representations.</li> <li>Parameters:</li> <li><code>user_features</code> (list): List of user features</li> <li><code>item_features</code> (list): List of item features</li> <li><code>hidden_units</code> (list): List of hidden layer units</li> <li><code>num_tasks</code> (int): Number of tasks</li> <li><code>task_types</code> (list): List of task types</li> </ul>"},{"location":"api-reference/models/#youtube-dnn","title":"YouTube DNN","text":"<ul> <li>Introduction: A deep recall model proposed by YouTube, designed for large-scale video recommendation scenarios. The model aggregates user viewing history through average pooling and combines it with other user features. Innovatively introduces negative sampling techniques and multi-task learning framework to improve training efficiency and effectiveness.</li> <li>Parameters:</li> <li><code>user_features</code> (list): List of user features</li> <li><code>item_features</code> (list): List of item features</li> <li><code>hidden_units</code> (list): List of hidden layer units</li> <li><code>embedding_dim</code> (int): Embedding dimension</li> <li><code>max_seq_len</code> (int): Maximum sequence length</li> </ul>"},{"location":"api-reference/models/#sequential-recommendation-series","title":"Sequential Recommendation Series","text":""},{"location":"api-reference/models/#gru4rec","title":"GRU4Rec","text":"<ul> <li>Introduction: A pioneering work that first applied GRU networks to session-based sequential recommendation. Through GRU network, it captures temporal dependencies in user behavior sequences, with hidden states at each time step containing information about historical behaviors. The model also introduces special mini-batch construction methods and loss function designs to adapt to the characteristics of sequential recommendation.</li> <li>Parameters:</li> <li><code>item_num</code> (int): Total number of items</li> <li><code>hidden_size</code> (int): Size of GRU hidden layer</li> <li><code>num_layers</code> (int): Number of GRU layers</li> <li><code>dropout_rate</code> (float): Dropout rate</li> <li><code>embedding_dim</code> (int): Item embedding dimension</li> </ul>"},{"location":"api-reference/models/#narm-neural-attentive-recommendation-machine","title":"NARM (Neural Attentive Recommendation Machine)","text":"<ul> <li>Introduction: A sequential recommendation model that introduces attention mechanism on top of GRU4Rec. Through attention mechanism, the model can dynamically focus on relevant behaviors in the sequence based on the current prediction target. It maintains both global and local sequence representations, comprehensively capturing user's short-term interests. This design enables better handling of user interest diversity and dynamics.</li> <li>Parameters:</li> <li><code>item_num</code> (int): Total number of items</li> <li><code>hidden_size</code> (int): Size of hidden layer</li> <li><code>attention_size</code> (int): Size of attention layer</li> <li><code>dropout_rate</code> (float): Dropout rate</li> <li><code>embedding_dim</code> (int): Item embedding dimension</li> </ul>"},{"location":"api-reference/models/#sasrec-self-attentive-sequential-recommendation","title":"SASRec (Self-Attentive Sequential Recommendation)","text":"<ul> <li>Introduction: A representative work that applies Transformer structure to sequential recommendation. Through self-attention mechanism, the model can directly compute and learn relationships between any two behaviors in the sequence, unrestricted by RNN's inherent sequential dependencies. Position encoding helps preserve temporal information of behaviors, while multi-layer structure allows the model to extract increasingly abstract behavior patterns layer by layer. Compared to RNN-based models, it offers better parallelism and scalability.</li> <li>Parameters:</li> <li><code>item_num</code> (int): Total number of items</li> <li><code>max_len</code> (int): Maximum sequence length</li> <li><code>num_heads</code> (int): Number of attention heads</li> <li><code>num_layers</code> (int): Number of Transformer layers</li> <li><code>hidden_size</code> (int): Hidden layer dimension</li> <li><code>dropout_rate</code> (float): Dropout rate</li> </ul>"},{"location":"api-reference/models/#mind-multi-interest-network-with-dynamic-routing","title":"MIND (Multi-Interest Network with Dynamic routing)","text":"<ul> <li>Introduction: A recall model designed for user's diverse interests. Through capsule network and dynamic routing mechanism, it extracts multiple interest vectors from user's behavior sequence. Each interest vector represents user preferences in different aspects, providing a more comprehensive characterization of user interest distribution.</li> <li>Parameters:</li> <li><code>item_num</code> (int): Total number of items</li> <li><code>num_interests</code> (int): Number of interest vectors</li> <li><code>routing_iterations</code> (int): Number of dynamic routing iterations</li> <li><code>hidden_size</code> (int): Hidden layer dimension</li> <li><code>embedding_dim</code> (int): Item embedding dimension</li> </ul>"},{"location":"api-reference/models/#ranking-models","title":"Ranking Models","text":"<p>Ranking models are primarily used in the fine-ranking stage to precisely rank candidate items. They learn complex interactions between users and items through deep learning methods to generate final ranking scores.</p>"},{"location":"api-reference/models/#wide-deep-series","title":"Wide &amp; Deep Series","text":""},{"location":"api-reference/models/#widedeep","title":"WideDeep","text":"<ul> <li>Introduction: A classic model proposed by Google in 2016 that combines the advantages of linear models and deep neural networks. The Wide part performs memorization through feature crosses, suitable for modeling direct, explicit feature correlations; the Deep part performs generalization through deep networks, capable of learning implicit, high-order feature relationships. This combination allows the model to both memorize historical patterns and generalize to new patterns.</li> <li>Parameters:</li> <li><code>wide_features</code> (list): List of features for the wide part, used in linear layer</li> <li><code>deep_features</code> (list): List of features for the deep part, used in deep network</li> <li><code>hidden_units</code> (list): List of hidden layer units for the deep network, e.g., [256, 128, 64]</li> <li><code>dropout_rates</code> (list): Dropout rates for each layer, used for preventing overfitting</li> </ul>"},{"location":"api-reference/models/#deepfm","title":"DeepFM","text":"<ul> <li>Introduction: A model that combines Factorization Machines (FM) feature interactions with deep learning models. The FM part efficiently models second-order feature interactions, while the Deep part learns high-order feature relationships. Compared to Wide&amp;Deep, DeepFM doesn't require manual feature engineering and can automatically learn feature crosses. The model consists of three parts: first-order features, FM's second-order interactions, and deep network's high-order interactions.</li> <li>Parameters:</li> <li><code>features</code> (list): List of features</li> <li><code>hidden_units</code> (list): Hidden layer units for DNN part</li> <li><code>dropout_rates</code> (list): List of dropout rates</li> <li><code>embedding_dim</code> (int): Feature embedding dimension</li> </ul>"},{"location":"api-reference/models/#dcn-dcn-v2","title":"DCN / DCN-V2","text":"<ul> <li>Introduction: Learns feature interactions explicitly through specially designed Cross Network layers. Each cross layer performs interactions between feature vectors and their original form, increasing the degree of feature crossing as the depth increases. DCN-V2 improves the cross network parameterization, offering both \"vector\" and \"matrix\" options, maintaining model expressiveness while improving efficiency.</li> <li>Parameters:</li> <li><code>features</code> (list): List of features</li> <li><code>cross_num</code> (int): Number of cross layers</li> <li><code>hidden_units</code> (list): Hidden layer units for DNN part</li> <li><code>cross_parameterization</code> (str, DCN-V2): Cross parameterization method, \"vector\" or \"matrix\"</li> </ul>"},{"location":"api-reference/models/#afm-attentional-factorization-machine","title":"AFM (Attentional Factorization Machine)","text":"<ul> <li>Introduction: Introduces attention mechanism to FM, assigning different importance weights to different feature interactions. Through the attention network, it adaptively learns the importance of feature interactions, identifying feature combinations that are more relevant to the prediction target.</li> <li>Parameters:</li> <li><code>features</code> (list): List of features</li> <li><code>attention_units</code> (list): Hidden layer units for attention network</li> <li><code>embedding_dim</code> (int): Feature embedding dimension</li> <li><code>dropout_rate</code> (float): Dropout rate for attention network</li> </ul>"},{"location":"api-reference/models/#fibinet-feature-importance-and-bilinear-feature-interaction-network","title":"FiBiNET (Feature Importance and Bilinear feature Interaction Network)","text":"<ul> <li>Introduction: Dynamically learns feature importance through SENET mechanism and uses bilinear layers for feature interaction. The SENET module helps identify important features, while bilinear interaction provides richer feature interaction methods than inner products.</li> <li>Parameters:</li> <li><code>features</code> (list): List of features</li> <li><code>bilinear_type</code> (str): Bilinear layer type, options: \"field_all\"/\"field_each\"/\"field_interaction\"</li> <li><code>hidden_units</code> (list): Hidden layer units for DNN part</li> <li><code>reduction_ratio</code> (int): Reduction ratio for SENET module</li> </ul>"},{"location":"api-reference/models/#attention-based-series","title":"Attention-based Series","text":""},{"location":"api-reference/models/#din-deep-interest-network","title":"DIN (Deep Interest Network)","text":"<ul> <li>Introduction: A model designed for user interest diversity, using attention mechanism for adaptive learning of user historical behaviors. The model dynamically calculates relevance weights of user historical behaviors based on the current candidate ad, thereby activating relevant user interests and capturing diverse user preferences. It innovatively introduced attention mechanism to recommender systems, pioneering a new paradigm for behavior sequence modeling.</li> <li>Parameters:</li> <li><code>features</code> (list): List of base features</li> <li><code>behavior_features</code> (list): List of behavior features for attention calculation</li> <li><code>attention_units</code> (list): Hidden layer units for attention network</li> <li><code>hidden_units</code> (list): Hidden layer units for DNN part</li> <li><code>activation</code> (str): Activation function type</li> </ul>"},{"location":"api-reference/models/#dien-deep-interest-evolution-network","title":"DIEN (Deep Interest Evolution Network)","text":"<ul> <li>Introduction: An advanced version of DIN that models the dynamic evolution of user interests through interest evolution layer. It uses GRU structure to capture interest evolution and innovatively designs AUGRU (GRU with Attentional Update Gate) to make the interest evolution process aware of target items. It also includes auxiliary loss to supervise the training of interest extraction layer. This design not only captures the dynamic changes of user interests but also models the temporal dependencies of interests.</li> <li>Parameters:</li> <li><code>features</code> (list): List of base features</li> <li><code>behavior_features</code> (list): List of behavior features</li> <li><code>interest_units</code> (list): Units for interest extraction layer</li> <li><code>gru_type</code> (str): GRU type, \"AUGRU\" or \"AIGRU\"</li> <li><code>hidden_units</code> (list): Hidden layer units for DNN part</li> </ul>"},{"location":"api-reference/models/#bst-behavior-sequence-transformer","title":"BST (Behavior Sequence Transformer)","text":"<ul> <li>Introduction: A pioneering work that introduces Transformer architecture to recommender systems for modeling user behavior sequences. Through self-attention mechanism, the model can directly compute relationships between any two behaviors in the sequence, overcoming the limitations of RNN models in processing long sequences. Position embedding helps the model perceive temporal information of behaviors, while multi-head attention allows the model to understand user behavior patterns from multiple perspectives.</li> <li>Parameters:</li> <li><code>features</code> (list): List of base features</li> <li><code>behavior_features</code> (list): List of behavior features</li> <li><code>num_heads</code> (int): Number of attention heads</li> <li><code>num_layers</code> (int): Number of Transformer layers</li> <li><code>hidden_size</code> (int): Hidden layer dimension</li> <li><code>dropout_rate</code> (float): Dropout rate</li> </ul>"},{"location":"api-reference/models/#edcn-enhancing-explicit-and-implicit-feature-interactions","title":"EDCN (Enhancing Explicit and Implicit Feature Interactions)","text":"<ul> <li>Introduction: A deep cross network that enhances both explicit and implicit feature interactions. Through a newly designed cross network structure, it considers both explicit and implicit feature interactions. Introduces gating mechanism to regulate the importance of different orders of feature interactions and uses residual connections to alleviate training issues in deep networks.</li> <li>Parameters:</li> <li><code>features</code> (list): List of features</li> <li><code>cross_num</code> (int): Number of cross layers</li> <li><code>hidden_units</code> (list): Hidden layer units for DNN part</li> <li><code>gate_type</code> (str): Gate type, \"FGU\" or \"BGU\"</li> </ul>"},{"location":"api-reference/models/#multi-task-models","title":"Multi-task Models","text":"<p>Multi-task models learn multiple related tasks jointly to achieve knowledge sharing and transfer, improving overall model performance.</p>"},{"location":"api-reference/models/#sharedbottom","title":"SharedBottom","text":"<ul> <li>Introduction: The most basic multi-task learning model that shares parameters in bottom network for extracting common feature representations. The shared layers learn common features across tasks, while task-specific layers learn individualized features for each task. This simple yet effective structure laid the foundation for multi-task learning.</li> <li>Parameters:</li> <li><code>features</code> (list): List of features</li> <li><code>hidden_units</code> (list): Hidden layer units for shared network</li> <li><code>task_hidden_units</code> (list): Hidden layer units for task-specific networks</li> <li><code>num_tasks</code> (int): Number of tasks</li> <li><code>task_types</code> (list): List of task types</li> </ul>"},{"location":"api-reference/models/#esmm-entire-space-multi-task-model","title":"ESMM (Entire Space Multi-Task Model)","text":"<ul> <li>Introduction: An innovative multi-task model proposed by Alibaba specifically designed to address sample selection bias in recommender systems. Through joint modeling of CVR and CTR tasks, it performs parameter learning in the entire space. The core innovation lies in introducing CTR as an auxiliary task and optimizing CVR estimation through task multiplication relationship. This design not only solves the sample selection bias in traditional CVR estimation but also provides unbiased CTR and CTCVR estimation.</li> <li>Parameters:</li> <li><code>features</code> (list): List of features</li> <li><code>hidden_units</code> (list): List of hidden layer units</li> <li><code>tower_units</code> (list): List of task tower layer units</li> <li><code>embedding_dim</code> (int): Feature embedding dimension</li> </ul>"},{"location":"api-reference/models/#mmoe-multi-gate-mixture-of-experts","title":"MMoE (Multi-gate Mixture-of-Experts)","text":"<ul> <li>Introduction: A multi-task learning model proposed by Google that achieves soft parameter sharing through expert mechanism and task-related gating networks. Each expert network can learn specific feature transformations, while gating networks dynamically allocate expert importance for each task. This design allows the model to flexibly combine expert knowledge based on task requirements, effectively handling task differences.</li> <li>Parameters:</li> <li><code>features</code> (list): List of features</li> <li><code>expert_units</code> (list): Hidden layer units for expert networks</li> <li><code>num_experts</code> (int): Number of experts</li> <li><code>num_tasks</code> (int): Number of tasks</li> <li><code>expert_activation</code> (str): Activation function for expert networks</li> <li><code>gate_activation</code> (str): Activation function for gate networks</li> </ul>"},{"location":"api-reference/models/#ple-progressive-layered-extraction","title":"PLE (Progressive Layered Extraction)","text":"<ul> <li>Introduction: An improved version of MMoE that better models task relationships through progressive layered extraction. Introduces the concept of task-specific experts and shared experts, implementing progressive feature extraction through multi-level expert networks. Each layer contains both task-specific experts and shared experts, allowing the model to learn both commonalities and individualities of tasks. This progressive design enhances the model's ability for knowledge extraction and transfer.</li> <li>Parameters:</li> <li><code>features</code> (list): List of features</li> <li><code>expert_units</code> (list): Units for expert networks</li> <li><code>num_experts</code> (int): Number of experts per layer</li> <li><code>num_layers</code> (int): Number of layers</li> <li><code>num_shared_experts</code> (int): Number of shared experts</li> <li><code>task_types</code> (list): List of task types</li> </ul>"},{"location":"api-reference/trainers/","title":"Trainers API Reference","text":"<p>This section provides detailed API documentation for all trainers in Torch-RecHub.</p>"},{"location":"api-reference/trainers/#ctrtrainer","title":"CTRTrainer","text":"<p>CTRTrainer is a general trainer for single task learning, primarily used for binary classification tasks such as Click-Through Rate (CTR) prediction.</p>"},{"location":"api-reference/trainers/#parameters","title":"Parameters","text":"<ul> <li><code>model</code> (nn.Module): Any single task learning model</li> <li><code>optimizer_fn</code> (torch.optim): PyTorch optimizer function, defaults to <code>torch.optim.Adam</code></li> <li><code>optimizer_params</code> (dict): Optimizer parameters, defaults to <code>{\"lr\": 1e-3, \"weight_decay\": 1e-5}</code></li> <li><code>scheduler_fn</code> (torch.optim.lr_scheduler): PyTorch learning rate scheduler, e.g., <code>torch.optim.lr_scheduler.StepLR</code></li> <li><code>scheduler_params</code> (dict): Learning rate scheduler parameters</li> <li><code>n_epoch</code> (int): Number of training epochs</li> <li><code>earlystop_patience</code> (int): Number of epochs to wait before early stopping when validation performance doesn't improve, defaults to 10</li> <li><code>device</code> (str): Device to use, either <code>\"cpu\"</code> or <code>\"cuda:0\"</code></li> <li><code>gpus</code> (list): List of GPU IDs, defaults to empty. If length &gt;=1, model will be wrapped by nn.DataParallel</li> <li><code>loss_mode</code> (bool): Training mode, defaults to True</li> <li><code>model_path</code> (str): Path to save the model, defaults to <code>\"./\"</code></li> </ul>"},{"location":"api-reference/trainers/#main-methods","title":"Main Methods","text":"<ul> <li><code>train_one_epoch(data_loader, log_interval=10)</code>: Train for one epoch</li> <li><code>fit(train_dataloader, val_dataloader=None)</code>: Train the model</li> <li><code>evaluate(model, data_loader)</code>: Evaluate the model</li> <li><code>predict(model, data_loader)</code>: Make predictions</li> </ul>"},{"location":"api-reference/trainers/#matchtrainer","title":"MatchTrainer","text":"<p>MatchTrainer is a trainer for matching/retrieval tasks, supporting multiple training modes.</p>"},{"location":"api-reference/trainers/#parameters_1","title":"Parameters","text":"<ul> <li><code>model</code> (nn.Module): Any matching model</li> <li><code>mode</code> (int): Training mode, options:</li> <li>0: point-wise</li> <li>1: pair-wise</li> <li>2: list-wise</li> <li><code>optimizer_fn</code> (torch.optim): Same as CTRTrainer</li> <li><code>optimizer_params</code> (dict): Same as CTRTrainer</li> <li><code>scheduler_fn</code> (torch.optim.lr_scheduler): Same as CTRTrainer</li> <li><code>scheduler_params</code> (dict): Same as CTRTrainer</li> <li><code>n_epoch</code> (int): Same as CTRTrainer</li> <li><code>earlystop_patience</code> (int): Same as CTRTrainer</li> <li><code>device</code> (str): Same as CTRTrainer</li> <li><code>gpus</code> (list): Same as CTRTrainer</li> <li><code>model_path</code> (str): Same as CTRTrainer</li> </ul>"},{"location":"api-reference/trainers/#main-methods_1","title":"Main Methods","text":"<ul> <li><code>train_one_epoch(data_loader, log_interval=10)</code>: Train for one epoch</li> <li><code>fit(train_dataloader, val_dataloader=None)</code>: Train the model</li> <li><code>evaluate(model, data_loader)</code>: Evaluate the model</li> <li><code>predict(model, data_loader)</code>: Make predictions</li> <li><code>inference_embedding(model, mode, data_loader, model_path)</code>: Infer embeddings</li> <li><code>mode</code>: Either \"user\" or \"item\"</li> </ul>"},{"location":"api-reference/trainers/#mtltrainer","title":"MTLTrainer","text":"<p>MTLTrainer is a trainer for multi-task learning, supporting various adaptive loss weighting methods.</p>"},{"location":"api-reference/trainers/#parameters_2","title":"Parameters","text":"<ul> <li><code>model</code> (nn.Module): Any multi-task learning model</li> <li><code>task_types</code> (list): List of task types, supports [\"classification\", \"regression\"]</li> <li><code>optimizer_fn</code> (torch.optim): Same as CTRTrainer</li> <li><code>optimizer_params</code> (dict): Same as CTRTrainer</li> <li><code>scheduler_fn</code> (torch.optim.lr_scheduler): Same as CTRTrainer</li> <li><code>scheduler_params</code> (dict): Same as CTRTrainer</li> <li><code>adaptive_params</code> (dict): Adaptive loss weighting method parameters, supports:</li> <li><code>{\"method\": \"uwl\"}</code>: Uncertainty Weighted Loss</li> <li><code>{\"method\": \"metabalance\"}</code>: MetaBalance method</li> <li><code>{\"method\": \"gradnorm\", \"alpha\": 0.16}</code>: GradNorm method</li> <li><code>n_epoch</code> (int): Same as CTRTrainer</li> <li><code>earlystop_taskid</code> (int): Task ID for early stopping, defaults to 0</li> <li><code>earlystop_patience</code> (int): Same as CTRTrainer</li> <li><code>device</code> (str): Same as CTRTrainer</li> <li><code>gpus</code> (list): Same as CTRTrainer</li> <li><code>model_path</code> (str): Same as CTRTrainer</li> </ul>"},{"location":"api-reference/trainers/#main-methods_2","title":"Main Methods","text":"<ul> <li><code>train_one_epoch(data_loader)</code>: Train for one epoch</li> <li><code>fit(train_dataloader, val_dataloader, mode='base', seed=0)</code>: Train the model</li> <li><code>evaluate(model, data_loader)</code>: Evaluate the model</li> <li><code>predict(model, data_loader)</code>: Make predictions</li> </ul>"},{"location":"api-reference/trainers/#special-features","title":"Special Features","text":"<ol> <li>Support for Multiple Adaptive Loss Weighting Methods:</li> <li>UWL (Uncertainty Weighted Loss)</li> <li>MetaBalance</li> <li> <p>GradNorm</p> </li> <li> <p>Multi-task Early Stopping:</p> </li> <li>Early stopping based on specified task performance</li> <li> <p>Saves best model weights based on validation performance</p> </li> <li> <p>Support for Multiple Task Type Combinations:</p> </li> <li>Classification tasks</li> <li> <p>Regression tasks</p> </li> <li> <p>Training Log Recording:</p> </li> <li>Records loss for each task</li> <li>Records loss weights (when using adaptive methods)</li> <li>Records performance metrics on validation set</li> </ol>"},{"location":"api-reference/utils/","title":"Utilities API Reference","text":"<p>This document provides detailed API documentation for utility classes and functions in Torch-RecHub.</p>"},{"location":"api-reference/utils/#data-processing-tools-datapy","title":"Data Processing Tools (data.py)","text":""},{"location":"api-reference/utils/#dataset-classes","title":"Dataset Classes","text":""},{"location":"api-reference/utils/#torchdataset","title":"TorchDataset","text":"<ul> <li>Introduction: Basic implementation of PyTorch dataset for handling features and labels.</li> <li>Parameters:</li> <li><code>x</code> (dict): Feature dictionary, keys are feature names, values are feature data</li> <li><code>y</code> (array): Label data</li> </ul>"},{"location":"api-reference/utils/#predictdataset","title":"PredictDataset","text":"<ul> <li>Introduction: Dataset class for prediction phase, containing only feature data.</li> <li>Parameters:</li> <li><code>x</code> (dict): Feature dictionary, keys are feature names, values are feature data</li> </ul>"},{"location":"api-reference/utils/#matchdatagenerator","title":"MatchDataGenerator","text":"<ul> <li>Introduction: Data generator for recall tasks, used to generate training and testing data loaders.</li> <li>Main Methods:</li> <li><code>generate_dataloader(x_test_user, x_all_item, batch_size, num_workers=8)</code>: Generate training, testing, and item data loaders</li> <li>Parameters:<ul> <li><code>x_test_user</code> (dict): Test user features</li> <li><code>x_all_item</code> (dict): All item features</li> <li><code>batch_size</code> (int): Batch size</li> <li><code>num_workers</code> (int): Number of worker processes for data loading</li> </ul> </li> </ul>"},{"location":"api-reference/utils/#datagenerator","title":"DataGenerator","text":"<ul> <li>Introduction: General data generator supporting dataset splitting and loading.</li> <li>Main Methods:</li> <li><code>generate_dataloader(x_val=None, y_val=None, x_test=None, y_test=None, split_ratio=None, batch_size=16, num_workers=0)</code>: Generate training, validation, and test data loaders</li> <li>Parameters:<ul> <li><code>x_val</code>, <code>y_val</code>: Validation set features and labels</li> <li><code>x_test</code>, <code>y_test</code>: Test set features and labels</li> <li><code>split_ratio</code> (list): Split ratios for train, validation, and test sets</li> <li><code>batch_size</code> (int): Batch size</li> <li><code>num_workers</code> (int): Number of worker processes for data loading</li> </ul> </li> </ul>"},{"location":"api-reference/utils/#utility-functions","title":"Utility Functions","text":""},{"location":"api-reference/utils/#get_auto_embedding_dim","title":"get_auto_embedding_dim","text":"<ul> <li>Introduction: Automatically calculate embedding vector dimension based on number of categories.</li> <li>Parameters:</li> <li><code>num_classes</code> (int): Number of categories</li> <li>Returns:</li> <li>int: Embedding vector dimension, formula: <code>[6 * (num_classes)^(1/4)]</code></li> </ul>"},{"location":"api-reference/utils/#get_loss_func","title":"get_loss_func","text":"<ul> <li>Introduction: Get loss function.</li> <li>Parameters:</li> <li><code>task_type</code> (str): Task type, \"classification\" or \"regression\"</li> <li>Returns:</li> <li>torch.nn.Module: Corresponding loss function</li> </ul>"},{"location":"api-reference/utils/#get_metric_func","title":"get_metric_func","text":"<ul> <li>Introduction: Get evaluation metric function.</li> <li>Parameters:</li> <li><code>task_type</code> (str): Task type, \"classification\" or \"regression\"</li> <li>Returns:</li> <li>function: Corresponding evaluation metric function</li> </ul>"},{"location":"api-reference/utils/#generate_seq_feature","title":"generate_seq_feature","text":"<ul> <li>Introduction: Generate sequence features and negative samples.</li> <li>Parameters:</li> <li><code>data</code> (pd.DataFrame): Raw data</li> <li><code>user_col</code> (str): User ID column name</li> <li><code>item_col</code> (str): Item ID column name</li> <li><code>time_col</code> (str): Timestamp column name</li> <li><code>item_attribute_cols</code> (list): Item attribute columns for sequence feature generation</li> <li><code>min_item</code> (int): Minimum number of items per user</li> <li><code>shuffle</code> (bool): Whether to shuffle data</li> <li><code>max_len</code> (int): Maximum sequence length</li> </ul>"},{"location":"api-reference/utils/#recall-tools-matchpy","title":"Recall Tools (match.py)","text":""},{"location":"api-reference/utils/#data-processing-functions","title":"Data Processing Functions","text":""},{"location":"api-reference/utils/#gen_model_input","title":"gen_model_input","text":"<ul> <li>Introduction: Merge user and item features, process sequence features.</li> <li>Parameters:</li> <li><code>df</code> (pd.DataFrame): Data with history sequence features</li> <li><code>user_profile</code> (pd.DataFrame): User feature data</li> <li><code>user_col</code> (str): User column name</li> <li><code>item_profile</code> (pd.DataFrame): Item feature data</li> <li><code>item_col</code> (str): Item column name</li> <li><code>seq_max_len</code> (int): Maximum sequence length</li> <li><code>padding</code> (str): Padding method, 'pre' or 'post'</li> <li><code>truncating</code> (str): Truncating method, 'pre' or 'post'</li> </ul>"},{"location":"api-reference/utils/#negative_sample","title":"negative_sample","text":"<ul> <li>Introduction: Negative sampling method for recall models.</li> <li>Parameters:</li> <li><code>items_cnt_order</code> (dict): Item count dictionary, sorted by count in descending order</li> <li><code>ratio</code> (int): Negative sample ratio</li> <li><code>method_id</code> (int): Sampling method ID<ul> <li>0: Random sampling</li> <li>1: Word2Vec-style popularity sampling</li> <li>2: Log popularity sampling</li> <li>3: Tencent RALM sampling</li> </ul> </li> </ul>"},{"location":"api-reference/utils/#vector-retrieval-classes","title":"Vector Retrieval Classes","text":""},{"location":"api-reference/utils/#annoy","title":"Annoy","text":"<ul> <li>Introduction: Vector recall tool based on Annoy.</li> <li>Parameters:</li> <li><code>metric</code> (str): Distance metric method</li> <li><code>n_trees</code> (int): Number of trees</li> <li><code>search_k</code> (int): Search parameter</li> <li>Main Methods:</li> <li><code>fit(X)</code>: Build index</li> <li><code>query(v, n)</code>: Query nearest neighbors</li> </ul>"},{"location":"api-reference/utils/#milvus","title":"Milvus","text":"<ul> <li>Introduction: Vector recall tool based on Milvus.</li> <li>Parameters:</li> <li><code>dim</code> (int): Vector dimension</li> <li><code>host</code> (str): Milvus server address</li> <li><code>port</code> (str): Milvus server port</li> <li>Main Methods:</li> <li><code>fit(X)</code>: Build index</li> <li><code>query(v, n)</code>: Query nearest neighbors</li> </ul>"},{"location":"api-reference/utils/#multi-task-learning-tools-mtlpy","title":"Multi-task Learning Tools (mtl.py)","text":""},{"location":"api-reference/utils/#utility-functions_1","title":"Utility Functions","text":""},{"location":"api-reference/utils/#shared_task_layers","title":"shared_task_layers","text":"<ul> <li>Introduction: Get shared layer and task-specific layer parameters in multi-task models.</li> <li>Parameters:</li> <li><code>model</code> (torch.nn.Module): Multi-task model, supports MMOE, SharedBottom, PLE, AITM</li> <li>Returns:</li> <li>list: Shared layer parameter list</li> <li>list: Task-specific layer parameter list</li> </ul>"},{"location":"api-reference/utils/#optimizer-classes","title":"Optimizer Classes","text":""},{"location":"api-reference/utils/#metabalance","title":"MetaBalance","text":"<ul> <li>Introduction: MetaBalance optimizer for balancing gradients in multi-task learning.</li> <li>Parameters:</li> <li><code>parameters</code> (list): Model parameters</li> <li><code>relax_factor</code> (float): Relaxation factor for gradient scaling, default 0.7</li> <li><code>beta</code> (float): Moving average coefficient, default 0.9</li> <li>Main Methods:</li> <li><code>step(losses)</code>: Execute optimization step, update parameters</li> </ul>"},{"location":"api-reference/utils/#gradient-processing-functions","title":"Gradient Processing Functions","text":""},{"location":"api-reference/utils/#gradnorm","title":"gradnorm","text":"<ul> <li>Introduction: Implement GradNorm algorithm for dynamically adjusting task weights in multi-task learning.</li> <li>Parameters:</li> <li><code>loss_list</code> (list): List of task losses</li> <li><code>loss_weight</code> (list): List of task weights</li> <li><code>share_layer</code> (torch.nn.Parameter): Shared layer parameters</li> <li><code>initial_task_loss</code> (list): List of initial task losses</li> <li><code>alpha</code> (float): GradNorm algorithm hyperparameter</li> </ul>"},{"location":"blog/match/","title":"Match Blog","text":""},{"location":"blog/match/#i-understanding-different-loss-functions-3-training-methods","title":"I. Understanding Different Loss Functions \u2014 3 Training Methods","text":"<p>In recall tasks, there are generally three training methods: point-wise, pair-wise, and list-wise. In RecHub, we use the mode parameter to specify the training method, with each method corresponding to a different loss function.</p>"},{"location":"blog/match/#11-point-wise-mode-0","title":"1.1 Point-wise (mode = 0)","text":"<p>\ud83e\udd70Core Idea: Treat recall as binary classification.</p> <p>For a recall model, the input is a tuple \\, and the output is \\(P(User, Item)\\), representing the user's interest level in the item. <p>Training objective: For positive samples, the output should be as close to 1 as possible; for negative samples, as close to 0 as possible.</p> <p>The most commonly used loss function is BCELoss (Binary Cross Entropy Loss).</p>"},{"location":"blog/match/#12-pair-wise-mode-1","title":"1.2 Pair-wise (mode = 1)","text":"<p>\ud83d\ude1dCore Idea: A user's interest in positive samples should be higher than in negative samples.</p> <p>For a recall model, the input is a triple \\&lt;User, ItemPositive, ItemNegative&gt;, outputting interest scores \\(P(User, ItemPositive)\\) and \\(P(User, ItemNegative)\\), representing the user's interest scores for positive and negative item samples.</p> <p>Training objective: The interest score for positive samples should be higher than that for negative samples.</p> <p>The framework uses BPRLoss (Bayes Personalized Ranking Loss). Here's the loss formula (for more details, see here - note that there are slight differences between the linked content and the formula below, but the core idea remains the same):</p> \\[ Loss=\\frac{1}{N}\\sum^N\\ _{i=1}-log(sigmoid(pos\\_score - neg\\_score)) \\]"},{"location":"blog/match/#13-list-wise-mode-2","title":"1.3 List-wise (mode = 2)","text":"<p>\ud83d\ude07Core Idea: A user's interest in positive samples should be higher than in negative samples.</p> <p>Wait, isn't this the same as Pair-wise?</p> <p>Yes! The core idea of List-wise training is the same as Pair-wise, but the implementation differs.</p> <p>For a recall model, the input is an N+2 tuple \\(&lt;User, ItemPositive, ItemNeg\\_1, ... , ItemNeg\\_N&gt;\\), outputting interest scores for 1 positive sample and N negative samples.</p> <p>Training objective: The interest score for the positive sample should be higher than all negative samples.</p> <p>The framework uses \\(torch.nn.CrossEntropyLoss\\), applying Softmax to the outputs.</p> <p>PS: This List-wise approach can be easily confused with List-wise in Ranking. Although they share the same name, List-wise in ranking considers the order relationship between samples. For example, ranking uses order-sensitive metrics like MAP and NDCG for evaluation, while List-wise in Matching doesn't consider order.</p>"},{"location":"blog/match/#ii-how-far-apart-are-two-vectors-3-similarity-metrics","title":"II. How Far Apart Are Two Vectors? \u2014 3 Similarity Metrics","text":"<p>\ud83e\udd14Given a user vector and an item vector, how do we measure their similarity?</p> <p>Let's first define user vector \\(user \\in \\mathcal R^D\\) and item vector \\(item\\in \\mathcal R^D\\), where D represents their dimension.</p>"},{"location":"blog/match/#21-cosine","title":"2.1 Cosine","text":"<p>From middle school math:</p> \\[ cos(a,b)=\\frac{&lt;a,b&gt;}{|a|*|b|} \\] <p>This represents the angle between two vectors, outputting a real number between [-1, 1]. We can use this as a similarity measure: the smaller the angle between vectors, the more similar they are.</p> <p>In all two-tower models in RecHub, cosine similarity is used during the training phase.</p>"},{"location":"blog/match/#22-dot-product","title":"2.2 Dot Product","text":"<p>This is the inner product of vectors, denoted as \\(&lt;a,b&gt;\\) for vectors a and b.</p> <p>A simple insight: If we L2 normalize vectors a and b, i.e., \\(\\tilde{a}=\\frac{a}{|a|}\\ ,\\tilde{b}=\\frac{b}{|b|}\\), then computing their dot product is equivalent to \\(cos(a,b)\\). (This is straightforward, so we'll skip the proof)</p> <p>In fact, this is exactly how all two-tower models in RecHub work: first computing User Embedding and Item Embedding, then applying L2 Norm to each, and finally computing their dot product to get cosine similarity. This approach improves model validation and inference speed.</p>"},{"location":"blog/match/#23-euclidean-distance","title":"2.3 Euclidean Distance","text":"<p>Euclidean distance is what we commonly understand as \"distance\" in everyday life.</p> <p>\ud83d\ude4bFor L2 normalized vectors a and b, maximizing their cosine similarity is equivalent to minimizing their Euclidean distance</p> <p>Why? See the formula below:</p> \\[ \\begin{align*}   EuclidianDistance(a,b)^2 &amp;= \\sum_{i=1}^N(a_i-b_i)^2 \\\\     &amp;= \\sum_{i=1}^Na_i^2+\\sum_{i=1}^Nb_i^2-\\sum_{i=1}^N2*a_i*b_i\\\\     &amp;= 2-2*\\sum_{i=1}^Na_i*b_i \\\\     &amp;= 2*(1-cos(a,b)) \\end{align*} \\] <p>Two points to explain:</p> <ol> <li>From second line to third line, \\(\\sum\\ _{i=1}^N a\\_i^2=1\\). Why? Because a is L2 normalized. Same for b.</li> <li>From third line to fourth line, \\(\\sum_{i=1}^Na_i*b_i\\) is the dot product of vectors a and b; since they're L2 normalized, this equals cos.</li> </ol> <p>In RecHub, we use Annoy's Euclidean distance during the validation phase.</p> <p>\ud83d\ude4bSummary: For L2 normalized vectors, maximizing dot product is equivalent to maximizing cosine similarity is equivalent to minimizing Euclidean distance</p>"},{"location":"blog/match/#iii-how-hot-is-the-temperature","title":"III. How Hot is the Temperature?","text":"<p>Before proceeding, please make sure you understand the operations in torch.nn.CrossEntropyLoss (LogSoftmax + NLLLoss). This is crucial for understanding the source code.</p> <p>Consider a scenario: Using List-wise training with 1 positive sample and 3 negative samples, with cosine similarity as the training metric.</p> <p>Suppose our model perfectly predicts a training sample, outputting logits (1, -1, -1, -1). Theoretically, the Loss should be 0, or at least very small. However, with CrossEntropyLoss, we get:</p> \\[ -log(exp(1)/(exp(1)+exp(-1)*3))=0.341 \\] <p>But if we divide the logits by a temperature coefficient \\(temperature=0.2\\), making them (5, -5, -5, -5), after CrossEntropyLoss, we get:</p> \\[ -log(exp(5)/(exp(5)+exp(-5)*3))=0.016 \\] <p>This gives us a negligibly small Loss.</p> <p>In other words, dividing logits by a temperature expands the upper and lower bounds of each element in the logits, bringing them back into the sensitive range of softmax operations.</p> <p>In practice, L2 Norm is commonly used together with temperature scaling.</p>"},{"location":"tutorials/matching/","title":"Recall Model Tutorial","text":"<p>This tutorial will introduce how to use various recall models in Torch-RecHub. We'll use the MovieLens dataset as an example.</p>"},{"location":"tutorials/matching/#data-preparation","title":"Data Preparation","text":"<p>First, we need to prepare the data. The MovieLens dataset contains user ratings for movies:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom rechub.utils import DataGenerator\nfrom rechub.models import *\nfrom rechub.trainers import *\n\n# Load data\ndf = pd.read_csv(\"movielens.csv\")\nuser_features = ['user_id', 'age', 'gender', 'occupation']\nitem_features = ['movie_id', 'genre', 'year']\n</code></pre>"},{"location":"tutorials/matching/#basic-two-tower-model-dssm","title":"Basic Two-Tower Model (DSSM)","text":"<p>DSSM is the most basic two-tower model, modeling users and items separately:</p> <pre><code># Model configuration\nmodel = DSSM(user_features=user_features,\n             item_features=item_features,\n             hidden_units=[64, 32, 16],\n             dropout_rates=[0.1, 0.1, 0.1])\n\n# Training configuration\ntrainer = MatchTrainer(model=model,\n                      mode=0,  # point-wise training\n                      optimizer_params={'lr': 0.001},\n                      n_epochs=10)\n\n# Train model\ntrainer.fit(train_dataloader, val_dataloader)\n</code></pre>"},{"location":"tutorials/matching/#sequential-recommendation-model-gru4rec","title":"Sequential Recommendation Model (GRU4Rec)","text":"<p>GRU4Rec models user behavior sequences through GRU networks:</p> <pre><code># Generate sequence features\nseq_features = generate_seq_feature(df,\n                                  user_col='user_id',\n                                  item_col='movie_id',\n                                  time_col='timestamp',\n                                  item_attribute_cols=['genre'])\n\n# Model configuration\nmodel = GRU4Rec(item_num=item_num,\n                hidden_size=64,\n                num_layers=2,\n                dropout_rate=0.1)\n\n# Training configuration\ntrainer = MatchTrainer(model=model,\n                      mode=1,  # pair-wise training\n                      optimizer_params={'lr': 0.001},\n                      n_epochs=10)\n</code></pre>"},{"location":"tutorials/matching/#multi-interest-model-mind","title":"Multi-Interest Model (MIND)","text":"<p>The MIND model can capture users' diverse interests:</p> <pre><code># Model configuration\nmodel = MIND(item_num=item_num,\n            num_interests=4,\n            hidden_size=64,\n            routing_iterations=3)\n\n# Training configuration\ntrainer = MatchTrainer(model=model,\n                      mode=2,  # list-wise training\n                      optimizer_params={'lr': 0.001},\n                      n_epochs=10)\n</code></pre>"},{"location":"tutorials/matching/#model-evaluation","title":"Model Evaluation","text":"<p>Use common recall metrics for evaluation:</p> <pre><code># Calculate recall rate and hit rate\nrecall_score = evaluate_recall(model, test_dataloader, k=10)\nhit_rate = evaluate_hit_rate(model, test_dataloader, k=10)\nprint(f\"Recall@10: {recall_score:.4f}\")\nprint(f\"HitRate@10: {hit_rate:.4f}\")\n</code></pre>"},{"location":"tutorials/matching/#vector-retrieval","title":"Vector Retrieval","text":"<p>The trained model can be used to generate vector representations of users and items for fast retrieval:</p> <pre><code># Use Annoy for vector retrieval\nfrom rechub.utils import Annoy\n\n# Build index\nitem_vectors = model.get_item_vectors()\nannoy = Annoy(metric='angular')\nannoy.fit(item_vectors)\n\n# Query similar items\nuser_vector = model.get_user_vector(user_id=1)\nsimilar_items = annoy.query(user_vector, n=10)\n</code></pre>"},{"location":"tutorials/matching/#advanced-techniques","title":"Advanced Techniques","text":"<ol> <li> <p>Temperature Coefficient Adjustment <pre><code>trainer = MatchTrainer(model=model,\n                      temperature=0.2,  # Add temperature coefficient\n                      mode=2)\n</code></pre></p> </li> <li> <p>Negative Sampling <pre><code>from rechub.utils import negative_sample\n\nneg_samples = negative_sample(items_cnt_order,\n                            ratio=5,\n                            method_id=1)  # Word2Vec-style sampling\n</code></pre></p> </li> <li> <p>Model Saving and Loading <pre><code># Save model\ntorch.save(model.state_dict(), 'model.pth')\n\n# Load model\nmodel.load_state_dict(torch.load('model.pth'))\n</code></pre></p> </li> </ol>"},{"location":"tutorials/matching/#important-notes","title":"Important Notes","text":"<ol> <li>Choose appropriate training mode (point-wise/pair-wise/list-wise)</li> <li>Pay attention to sequence feature length and padding method</li> <li>Adjust negative sample ratio based on actual scenarios</li> <li>Set appropriate batch_size and learning rate</li> <li>Use L2 regularization to prevent overfitting</li> </ol>"},{"location":"tutorials/multi-task/","title":"Multi-Task Learning Tutorial","text":"<p>This tutorial will introduce how to use multi-task learning models in Torch-RecHub. We'll use Alibaba's e-commerce dataset as an example.</p>"},{"location":"tutorials/multi-task/#data-preparation","title":"Data Preparation","text":"<p>First, we need to prepare data for multi-task learning:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom rechub.utils import DataGenerator\nfrom rechub.models import *\nfrom rechub.trainers import *\n\n# Load data\ndf = pd.read_csv(\"ali_ccp_data.csv\")\n\n# Feature definitions\nuser_features = ['user_id', 'age', 'gender', 'occupation']\nitem_features = ['item_id', 'category_id', 'shop_id', 'brand_id']\nfeatures = user_features + item_features\n\n# Multi-task labels\ntasks = ['click', 'conversion']  # CTR and CVR tasks\n</code></pre>"},{"location":"tutorials/multi-task/#sharedbottom-model","title":"SharedBottom Model","text":"<p>The most basic multi-task learning model with shared parameters in bottom layers:</p> <pre><code># Model configuration\nmodel = SharedBottom(\n    features=features,\n    hidden_units=[256, 128],\n    task_hidden_units=[64, 32],\n    num_tasks=2,\n    task_types=['binary', 'binary'])\n\n# Training configuration\ntrainer = MTLTrainer(\n    model=model,\n    optimizer_params={'lr': 0.001},\n    n_epochs=10)\n\n# Train model\ntrainer.fit(train_dataloader, val_dataloader)\n</code></pre>"},{"location":"tutorials/multi-task/#esmm-entire-space-multi-task-model","title":"ESMM (Entire Space Multi-Task Model)","text":"<p>A multi-task model that addresses sample selection bias:</p> <pre><code># Model configuration\nmodel = ESMM(\n    features=features,\n    hidden_units=[256, 128, 64],\n    tower_units=[32, 16],\n    embedding_dim=16)\n\n# Training configuration\ntrainer = MTLTrainer(\n    model=model,\n    optimizer_params={'lr': 0.001},\n    n_epochs=10)\n</code></pre>"},{"location":"tutorials/multi-task/#mmoe-multi-gate-mixture-of-experts","title":"MMoE (Multi-gate Mixture-of-Experts)","text":"<p>Implements soft parameter sharing between tasks through expert mechanism:</p> <pre><code># Model configuration\nmodel = MMoE(\n    features=features,\n    expert_units=[256, 128],\n    num_experts=8,\n    num_tasks=2,\n    expert_activation='relu',\n    gate_activation='softmax')\n\n# Training configuration\ntrainer = MTLTrainer(\n    model=model,\n    optimizer_params={'lr': 0.001},\n    n_epochs=10)\n</code></pre>"},{"location":"tutorials/multi-task/#ple-progressive-layered-extraction","title":"PLE (Progressive Layered Extraction)","text":"<p>Better models task relationships through layered extraction:</p> <pre><code># Model configuration\nmodel = PLE(\n    features=features,\n    expert_units=[256, 128],\n    num_experts=4,\n    num_layers=3,\n    num_shared_experts=2,\n    task_types=['binary', 'binary'])\n\n# Training configuration\ntrainer = MTLTrainer(\n    model=model,\n    optimizer_params={'lr': 0.001},\n    n_epochs=10)\n</code></pre>"},{"location":"tutorials/multi-task/#task-weight-optimization","title":"Task Weight Optimization","text":""},{"location":"tutorials/multi-task/#gradnorm","title":"GradNorm","text":"<p>Use GradNorm algorithm to dynamically adjust task weights:</p> <pre><code># Configure GradNorm\ntrainer = MTLTrainer(\n    model=model,\n    optimizer_params={'lr': 0.001},\n    task_weights_strategy='gradnorm',\n    gradnorm_alpha=1.5)\n</code></pre>"},{"location":"tutorials/multi-task/#metabalance","title":"MetaBalance","text":"<p>Use MetaBalance optimizer to balance task gradients:</p> <pre><code>from rechub.utils import MetaBalance\n\n# Configure MetaBalance optimizer\noptimizer = MetaBalance(\n    model.parameters(),\n    relax_factor=0.7,\n    beta=0.9)\n\ntrainer = MTLTrainer(\n    model=model,\n    optimizer=optimizer)\n</code></pre>"},{"location":"tutorials/multi-task/#model-evaluation","title":"Model Evaluation","text":"<p>Use appropriate evaluation metrics for different tasks:</p> <pre><code># Evaluate model\nresults = evaluate_multi_task(model, test_dataloader)\nfor task, metrics in results.items():\n    print(f\"Task: {task}\")\n    print(f\"AUC: {metrics['auc']:.4f}\")\n    print(f\"LogLoss: {metrics['logloss']:.4f}\")\n</code></pre>"},{"location":"tutorials/multi-task/#advanced-applications","title":"Advanced Applications","text":"<ol> <li> <p>Custom Task Loss Weights <pre><code>trainer = MTLTrainer(\n    model=model,\n    task_weights=[1.0, 0.5])  # Set fixed task weights\n</code></pre></p> </li> <li> <p>Get Shared and Task-Specific Layers <pre><code>from rechub.utils import shared_task_layers\n\nshared_params, task_params = shared_task_layers(model)\n</code></pre></p> </li> <li> <p>Task-Specific Learning Rates <pre><code>trainer = MTLTrainer(\n    model=model,\n    task_specific_lr={'click': 0.001, 'conversion': 0.0005})\n</code></pre></p> </li> </ol>"},{"location":"tutorials/multi-task/#important-notes","title":"Important Notes","text":"<ol> <li>Choose appropriate multi-task learning architecture</li> <li>Pay attention to task correlations</li> <li>Handle data imbalance between tasks</li> <li>Set reasonable task weights</li> <li>Monitor training progress for each task</li> <li>Prevent negative transfer between tasks</li> <li>Consider computational resource constraints</li> </ol>"},{"location":"tutorials/ranking/","title":"Ranking Model Tutorial","text":"<p>This tutorial will introduce how to use various ranking models in Torch-RecHub. We'll use the Criteo and Avazu datasets as examples.</p>"},{"location":"tutorials/ranking/#data-preparation","title":"Data Preparation","text":"<p>First, we need to prepare the data and process features:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom rechub.utils import DataGenerator\nfrom rechub.models import *\nfrom rechub.trainers import *\n\n# Load data\ndf = pd.read_csv(\"criteo_sample.csv\")\n\n# Feature column definitions\nsparse_features = ['C1', 'C2', 'C3', ..., 'C26']\ndense_features = ['I1', 'I2', 'I3', ..., 'I13']\nfeatures = sparse_features + dense_features\n</code></pre>"},{"location":"tutorials/ranking/#wide-deep-model","title":"Wide &amp; Deep Model","text":"<p>Wide &amp; Deep model combines memorization and generalization capabilities:</p> <pre><code># Model configuration\nmodel = WideDeep(\n    wide_features=sparse_features,\n    deep_features=features,\n    hidden_units=[256, 128, 64],\n    dropout_rates=[0.1, 0.1, 0.1])\n\n# Training configuration\ntrainer = Trainer(model=model,\n                 optimizer_params={'lr': 0.001},\n                 n_epochs=10,\n                 device='cuda:0')\n\n# Train model\ntrainer.fit(train_dataloader, val_dataloader)\n</code></pre>"},{"location":"tutorials/ranking/#deepfm-model","title":"DeepFM Model","text":"<p>DeepFM model uses factorization machines and deep networks to model feature interactions:</p> <pre><code># Model configuration\nmodel = DeepFM(\n    features=features,\n    hidden_units=[256, 128, 64],\n    dropout_rates=[0.1, 0.1, 0.1],\n    embedding_dim=16)\n\n# Training configuration\ntrainer = Trainer(model=model,\n                 optimizer_params={'lr': 0.001},\n                 n_epochs=10)\n</code></pre>"},{"location":"tutorials/ranking/#din-deep-interest-network","title":"DIN (Deep Interest Network)","text":"<p>DIN model uses attention mechanism to model user interests:</p> <pre><code># Generate behavior sequence features\nbehavior_features = ['item_id', 'category_id']\nseq_features = generate_seq_feature(df,\n                                  user_col='user_id',\n                                  item_col='item_id',\n                                  time_col='timestamp',\n                                  item_attribute_cols=['category_id'])\n\n# Model configuration\nmodel = DIN(\n    features=features,\n    behavior_features=behavior_features,\n    attention_units=[80, 40],\n    hidden_units=[256, 128, 64],\n    dropout_rate=0.1)\n\n# Training configuration\ntrainer = Trainer(model=model,\n                 optimizer_params={'lr': 0.001},\n                 n_epochs=10)\n</code></pre>"},{"location":"tutorials/ranking/#dcn-v2-model","title":"DCN-V2 Model","text":"<p>DCN-V2 explicitly models feature interactions through cross network:</p> <pre><code># Model configuration\nmodel = DCNV2(\n    features=features,\n    cross_num=3,\n    hidden_units=[256, 128, 64],\n    dropout_rates=[0.1, 0.1, 0.1],\n    cross_parameterization='matrix')  # or 'vector'\n\n# Training configuration\ntrainer = Trainer(model=model,\n                 optimizer_params={'lr': 0.001},\n                 n_epochs=10)\n</code></pre>"},{"location":"tutorials/ranking/#model-evaluation","title":"Model Evaluation","text":"<p>Use common ranking metrics for evaluation:</p> <pre><code># Evaluate model\nauc = evaluate_auc(model, test_dataloader)\nlog_loss = evaluate_logloss(model, test_dataloader)\nprint(f\"AUC: {auc:.4f}\")\nprint(f\"LogLoss: {log_loss:.4f}\")\n</code></pre>"},{"location":"tutorials/ranking/#feature-engineering-tips","title":"Feature Engineering Tips","text":"<ol> <li> <p>Feature Preprocessing <pre><code># Categorical feature encoding\nfrom sklearn.preprocessing import LabelEncoder\nfor feat in sparse_features:\n    lbe = LabelEncoder()\n    df[feat] = lbe.fit_transform(df[feat])\n\n# Numerical feature normalization\nfrom sklearn.preprocessing import MinMaxScaler\nfor feat in dense_features:\n    scaler = MinMaxScaler()\n    df[feat] = scaler.fit_transform(df[feat].values.reshape(-1, 1))\n</code></pre></p> </li> <li> <p>Feature Crossing <pre><code># Manual feature crossing\ndf['cross_feat'] = df['feat1'].astype(str) + '_' + df['feat2'].astype(str)\n</code></pre></p> </li> </ol>"},{"location":"tutorials/ranking/#advanced-applications","title":"Advanced Applications","text":"<ol> <li> <p>Custom Loss Function <pre><code>class FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, pred, target):\n        # Implement Focal Loss\n        pass\n\ntrainer = Trainer(model=model,\n                 loss_fn=FocalLoss(alpha=0.25, gamma=2))\n</code></pre></p> </li> <li> <p>Learning Rate Scheduling <pre><code>from torch.optim.lr_scheduler import CosineAnnealingLR\n\ntrainer = Trainer(model=model,\n                 scheduler='cosine',  # Use cosine annealing scheduler\n                 scheduler_params={'T_max': 10})\n</code></pre></p> </li> </ol>"},{"location":"tutorials/ranking/#important-notes","title":"Important Notes","text":"<ol> <li>Handle missing values and outliers appropriately</li> <li>Pay attention to feature engineering importance</li> <li>Choose appropriate evaluation metrics</li> <li>Focus on model interpretability</li> <li>Balance model complexity and efficiency</li> <li>Handle class imbalance issues</li> </ol>"}]}